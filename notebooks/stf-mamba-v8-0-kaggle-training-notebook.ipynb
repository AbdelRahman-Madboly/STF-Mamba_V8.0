{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T18:50:36.548420Z",
     "iopub.status.busy": "2026-02-21T18:50:36.547437Z",
     "iopub.status.idle": "2026-02-21T18:50:44.618333Z",
     "shell.execute_reply": "2026-02-21T18:50:44.617344Z",
     "shell.execute_reply.started": "2026-02-21T18:50:36.548387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Installs all of the libraries present in the 'offline-pytorch-2-1-2'\n",
    "!pip install \\\n",
    "   --requirement /kaggle/input/offline-pytorch-2-1-2/requirements.txt \\\n",
    "   --no-index \\\n",
    "   --find-links file:///kaggle/input/offline-pytorch-2-1-2/wheels  \\\n",
    "--q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T18:50:58.309319Z",
     "iopub.status.busy": "2026-02-21T18:50:58.309016Z",
     "iopub.status.idle": "2026-02-21T18:50:58.318188Z",
     "shell.execute_reply": "2026-02-21T18:50:58.317379Z",
     "shell.execute_reply.started": "2026-02-21T18:50:58.309298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "def check_suitability():\n",
    "    print(\"=\"*60)\n",
    "    print(\"  STF-Mamba V8.0 — Compatibility Test Report\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. PyTorch Version Check\n",
    "    torch_version = torch.__version__.split('+')[0]\n",
    "    print(f\"1. PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Ideal versions are 2.4.0 or 2.1.0 for pre-compiled wheels\n",
    "    if torch_version in [\"2.4.0\", \"2.1.0\"]:\n",
    "        print(\"   ✓ EXCELLENT: This version has pre-compiled Mamba kernels available.\")\n",
    "    elif torch_version > \"2.4.0\":\n",
    "        print(\"   ⚠ WARNING: Version too new. You will likely have to compile from source (30-60 min).\")\n",
    "    else:\n",
    "        print(\"   ✗ ERROR: Version too old. Mamba 2.0+ requires PyTorch 2.1.0 minimum.\")\n",
    "\n",
    "    # 2. CUDA & GPU Check\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"\\n2. CUDA Available: {cuda_available}\")\n",
    "    if cuda_available:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"   ✓ GPU Device: {gpu_name}\")\n",
    "        # Mamba requires Ampere (A100/3060) or newer for best speed, \n",
    "        # but T4 (Kaggle) is supported via specific kernels.\n",
    "    else:\n",
    "        print(\"   ✗ ERROR: No GPU detected. Mamba cannot run on CPU.\")\n",
    "\n",
    "    # 3. Python Version\n",
    "    python_version = platform.python_version()\n",
    "    print(f\"\\n3. Python Version: {python_version}\")\n",
    "    if python_version.startswith(\"3.10\"):\n",
    "        print(\"   ✓ MATCH: Python 3.10 is the standard for Mamba wheels.\")\n",
    "    else:\n",
    "        print(f\"   ⚠ NOTE: Wheels are usually built for 3.10. You are on {python_version}.\")\n",
    "\n",
    "    # 4. Summary Verdict\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    if torch_version == \"2.4.0\" and cuda_available:\n",
    "        print(\"  VERDICT: PERFECT SUITABILITY. Use the direct .whl links.\")\n",
    "    elif cuda_available:\n",
    "        print(\"  VERDICT: SEMI-SUITABLE. You can run it, but expect long install times.\")\n",
    "    else:\n",
    "        print(\"  VERDICT: NOT SUITABLE. Switch to a GPU-enabled notebook.\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "check_suitability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T18:52:45.642153Z",
     "iopub.status.busy": "2026-02-21T18:52:45.641374Z",
     "iopub.status.idle": "2026-02-21T19:00:26.166932Z",
     "shell.execute_reply": "2026-02-21T19:00:26.166018Z",
     "shell.execute_reply.started": "2026-02-21T18:52:45.642123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 1: High-Speed Setup (PyTorch 2.1.2 + cu118)\n",
    "# ============================================================================\n",
    "import torch, os, sys\n",
    "print(f\"Verified Active Version: {torch.__version__}\")\n",
    "\n",
    "# Direct links for Python 3.10 + CUDA 11.8 + Torch 2.1\n",
    "# These wheels bypass the 33-minute CPU compilation process.\n",
    "print(\"Installing native Mamba kernels...\")\n",
    "!pip install https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.1.3/causal_conv1d-1.1.3+cu118torch2.1cxx11abiFalse-cp310-cp310-linux_x86_64.whl -q\n",
    "!pip install https://github.com/state-spaces/mamba/releases/download/v1.1.1/mamba_ssm-1.1.1+cu118torch2.1cxx11abiFalse-cp310-cp310-linux_x86_64.whl -q\n",
    "\n",
    "# Install required vision and utility libraries\n",
    "print(\"Installing vision utilities...\")\n",
    "!pip install dlib imutils albumentations einops -q\n",
    "\n",
    "try:\n",
    "    from mamba_ssm import Mamba\n",
    "    from causal_conv1d import causal_conv1d_fn\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"✓ SUCCESS: Mamba Kernels Ready\")\n",
    "    print(\"=\"*40)\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Error: {e}\")\n",
    "    print(\"If failure persists, please ensure your GPU is set to T4.\")\n",
    "\n",
    "print(\"\\n✓ Section 1 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T19:02:15.092780Z",
     "iopub.status.busy": "2026-02-21T19:02:15.091701Z",
     "iopub.status.idle": "2026-02-21T19:02:17.632954Z",
     "shell.execute_reply": "2026-02-21T19:02:17.632105Z",
     "shell.execute_reply.started": "2026-02-21T19:02:15.092710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 2: Clone Repo + Path Configuration\n",
    "# ============================================================================\n",
    "REPO_URL = \"https://github.com/AbdelRahman-Madboly/STF-Mamba_V8.0.git\"\n",
    "REPO_DIR = \"/kaggle/working/STF-Mamba_V8.0\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    print(f\"Repo already cloned at {REPO_DIR}\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "from stf_mamba import STFMambaV8, STFMambaLoss, is_mamba_available\n",
    "from data import SBIVideoDataset, get_train_transforms, get_val_transforms, load_all_splits\n",
    "from training import Trainer, build_optimizer, build_scheduler\n",
    "\n",
    "print(f\"\\nRepo: {REPO_DIR}\")\n",
    "print(f\"Mamba SSM: {'native' if is_mamba_available() else 'Conv1d fallback'}\")\n",
    "print(\"✓ Section 2 complete — all imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T19:02:54.349928Z",
     "iopub.status.busy": "2026-02-21T19:02:54.349356Z",
     "iopub.status.idle": "2026-02-21T19:03:54.459078Z",
     "shell.execute_reply": "2026-02-21T19:03:54.458333Z",
     "shell.execute_reply.started": "2026-02-21T19:02:54.349899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 3: Dataset Paths — Load Pre-Built Cache\n",
    "# ============================================================================\n",
    "from pathlib import Path\n",
    "\n",
    "KAGGLE_INPUT = Path(\"/kaggle/input\")\n",
    "\n",
    "# --- Find pre-built cache dataset ---\n",
    "CACHE_DATASET = None\n",
    "for p in sorted(KAGGLE_INPUT.rglob(\"crops\")):\n",
    "    if p.is_dir() and any(f.name.endswith(\"_crops.npz\") for f in p.iterdir()):\n",
    "        CACHE_DATASET = str(p.parent)  # parent of crops/\n",
    "        break\n",
    "\n",
    "if CACHE_DATASET is None:\n",
    "    # Fallback: search by dataset name\n",
    "    for name in [\"stf-mamba-v8-cache\", \"stf-cache\", \"stf_cache\"]:\n",
    "        for p in KAGGLE_INPUT.rglob(name):\n",
    "            if p.is_dir():\n",
    "                CACHE_DATASET = str(p)\n",
    "                break\n",
    "\n",
    "if CACHE_DATASET is None:\n",
    "    raise RuntimeError(\n",
    "        \"Pre-built cache not found!\\n\"\n",
    "        \"Run the preprocessing notebook first and attach 'stf-mamba-v8-cache' dataset.\"\n",
    "    )\n",
    "\n",
    "# Verify cache contents\n",
    "n_crops = len([f for f in os.listdir(os.path.join(CACHE_DATASET, \"crops\")) if f.endswith(\".npz\")])\n",
    "sbi_dir = os.path.join(CACHE_DATASET, \"sbi_seed42\")\n",
    "n_sbi = len([f for f in os.listdir(sbi_dir) if f.endswith(\".npz\")]) if os.path.isdir(sbi_dir) else 0\n",
    "print(f\"✓ Cache found: {CACHE_DATASET}\")\n",
    "print(f\"  crops/     : {n_crops} files\")\n",
    "print(f\"  sbi_seed42/: {n_sbi} files\")\n",
    "\n",
    "# Cache is read-only on Kaggle input, so we symlink or copy to working dir\n",
    "CACHE_DIR = \"/kaggle/working/cache\"\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.symlink(CACHE_DATASET, CACHE_DIR)\n",
    "    print(f\"  Symlinked to: {CACHE_DIR}\")\n",
    "\n",
    "# FF++ video dir (needed only if SBI cache needs regeneration — shouldn't happen)\n",
    "FF_VIDEO_DIR = \"/kaggle/input/datasets/xdxd003/ff-c23/FaceForensics++_C23/original\"\n",
    "\n",
    "# Celeb-DF\n",
    "CELEB_DF_ROOT = None\n",
    "for p in KAGGLE_INPUT.rglob(\"*\"):\n",
    "    if p.is_dir() and \"celeb\" in p.name.lower():\n",
    "        children = [c.name for c in p.iterdir() if c.is_dir()]\n",
    "        if any(\"synth\" in c.lower() for c in children):\n",
    "            CELEB_DF_ROOT = str(p)\n",
    "            break\n",
    "print(f\"  Celeb-DF: {CELEB_DF_ROOT or 'not found (cross-dataset eval skipped)'}\")\n",
    "\n",
    "# dlib predictor (for Celeb-DF eval only)\n",
    "PREDICTOR_PATH = \"/kaggle/working/shape_predictor_81_face_landmarks.dat\"\n",
    "if not os.path.exists(PREDICTOR_PATH):\n",
    "    for p in KAGGLE_INPUT.rglob(\"shape_predictor_81_face_landmarks.dat\"):\n",
    "        os.system(f\"cp '{p}' '{PREDICTOR_PATH}'\")\n",
    "        break\n",
    "\n",
    "# Splits + checkpoint dirs\n",
    "SPLITS_DIR = os.path.join(REPO_DIR, \"splits\")\n",
    "CHECKPOINT_DIR = \"/kaggle/working/checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "splits = load_all_splits(SPLITS_DIR)\n",
    "print(f\"  Splits: train={len(splits['train'])}, val={len(splits['val'])}, test={len(splits['test'])}\")\n",
    "print(\"\\n✓ Section 3 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T19:03:54.460591Z",
     "iopub.status.busy": "2026-02-21T19:03:54.460336Z",
     "iopub.status.idle": "2026-02-21T19:03:54.803124Z",
     "shell.execute_reply": "2026-02-21T19:03:54.802239Z",
     "shell.execute_reply.started": "2026-02-21T19:03:54.460567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 4: Load Cached Datasets (instant — no face extraction!)\n",
    "# ============================================================================\n",
    "NUM_FRAMES = 32\n",
    "IMG_SIZE = 224\n",
    "\n",
    "train_tf = get_train_transforms(IMG_SIZE)\n",
    "val_tf = get_val_transforms(IMG_SIZE)\n",
    "\n",
    "train_ds = SBIVideoDataset(\n",
    "    split_path=os.path.join(SPLITS_DIR, \"Dataset_Split_train.json\"),\n",
    "    video_dir=FF_VIDEO_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    phase=\"train\",\n",
    "    num_frames=NUM_FRAMES,\n",
    "    img_size=IMG_SIZE,\n",
    "    transform=train_tf,\n",
    "    sbi_seed=42,\n",
    "    predictor_path=PREDICTOR_PATH if os.path.exists(PREDICTOR_PATH) else None,\n",
    ")\n",
    "\n",
    "val_ds = SBIVideoDataset(\n",
    "    split_path=os.path.join(SPLITS_DIR, \"Dataset_Split_val.json\"),\n",
    "    video_dir=FF_VIDEO_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    phase=\"val\",\n",
    "    num_frames=NUM_FRAMES,\n",
    "    img_size=IMG_SIZE,\n",
    "    transform=val_tf,\n",
    "    sbi_seed=42,\n",
    "    predictor_path=PREDICTOR_PATH if os.path.exists(PREDICTOR_PATH) else None,\n",
    ")\n",
    "\n",
    "# Quick sanity check\n",
    "sample = train_ds[0]\n",
    "print(f\"Sanity check:\")\n",
    "print(f\"  frames: {sample['frames'].shape}\")\n",
    "print(f\"  label:  {sample['label']}\")\n",
    "print(f\"  id:     {sample['video_id']}\")\n",
    "print(f\"\\nDataset sizes: train={len(train_ds)}, val={len(val_ds)}\")\n",
    "print(\"\\n✓ Section 4 complete — loaded from cache (no preprocessing needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T19:30:44.141412Z",
     "iopub.status.busy": "2026-02-21T19:30:44.140654Z",
     "iopub.status.idle": "2026-02-21T19:30:46.980830Z",
     "shell.execute_reply": "2026-02-21T19:30:46.980060Z",
     "shell.execute_reply.started": "2026-02-21T19:30:44.141383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 5: Model Init + Param Count + Forward Pass Verify\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from stf_mamba import STFMambaV8\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Build model — Force d_conv=4 for compatibility with Mamba CUDA kernels\n",
    "print(\"\\nLoading DINOv2-ViT-B/14 + Hydra-Mamba...\")\n",
    "# d_conv MUST be 2, 3, or 4 to avoid 'causal_conv1d' RuntimeError\n",
    "model = STFMambaV8(pretrained_backbone=True, d_conv=4)\n",
    "\n",
    "# Param count\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Total:     {total / 1e6:.1f}M\")\n",
    "print(f\"  Trainable: {trainable / 1e6:.1f}M\")\n",
    "print(f\"  Frozen:    { (total - trainable) / 1e6:.1f}M\")\n",
    "\n",
    "# Forward pass verify\n",
    "print(f\"\\nForward pass test...\")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# Batch format: [Batch, Frames, Channels, Height, Width]\n",
    "x_test = torch.randn(1, 32, 3, 224, 224).to(device)\n",
    "with torch.no_grad():\n",
    "    out = model(x_test)\n",
    "\n",
    "print(f\"  Logits:   {out['logits'].shape}  ← expected (1, 2)\")\n",
    "print(f\"  Variance: {out['variance'].shape} ← expected (1, 1)\")\n",
    "\n",
    "# Verification asserts\n",
    "assert out['logits'].shape == (1, 2), f\"Logits shape mismatch: {out['logits'].shape}\"\n",
    "assert out['variance'].shape == (1, 1), f\"Variance shape mismatch: {out['variance'].shape}\"\n",
    "\n",
    "# Cleanup to preserve T4 memory for training\n",
    "del x_test, out\n",
    "torch.cuda.empty_cache()\n",
    "model = model.cpu()\n",
    "print(\"\\n✓ Section 5 complete — Forward pass successful with d_conv=4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T22:03:16.044589Z",
     "iopub.status.busy": "2026-02-21T22:03:16.044060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 6: Training Loop — all fixes applied:\n",
    "#   1. Clear module cache → loads patched consistency_head.py\n",
    "#   2. lambda_var=1.0, lr_backbone=1e-5\n",
    "#   3. Warmup sets LR from base values (not compound-multiplies)\n",
    "#   4. num_workers=2 + fork (faster loading)\n",
    "#   5. var_gap tracked in history\n",
    "# ============================================================================\n",
    "import sys, gc, time, random, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ── Clear module cache so patched consistency_head.py is loaded ──────────────\n",
    "for key in list(sys.modules.keys()):\n",
    "    if \"stf_mamba\" in key:\n",
    "        del sys.modules[key]\n",
    "\n",
    "from stf_mamba import STFMambaV8, STFMambaLoss\n",
    "print(\"✓ Loaded patched stf_mamba\")\n",
    "\n",
    "# ── Reproducibility ───────────────────────────────────────────────────────────\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ── Config ────────────────────────────────────────────────────────────────────\n",
    "config = {\n",
    "    \"epochs\":        25,\n",
    "    \"batch_size\":    4,\n",
    "    \"lr_backbone\":   1e-5,   # was 5e-6\n",
    "    \"lr_temporal\":   1e-4,\n",
    "    \"lr_head\":       1e-4,\n",
    "    \"weight_decay\":  1e-4,\n",
    "    \"warmup_epochs\": 3,\n",
    "    \"grad_clip\":     1.0,\n",
    "    \"lambda_var\":    1.0,    # was 0.1 — variance was dead\n",
    "}\n",
    "\n",
    "# ── Clear GPU ─────────────────────────────────────────────────────────────────\n",
    "try: del model, trainer\n",
    "except: pass\n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# ── DataLoaders ───────────────────────────────────────────────────────────────\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=config[\"batch_size\"], shuffle=True,\n",
    "    num_workers=2, multiprocessing_context=\"fork\",\n",
    "    pin_memory=True, drop_last=True,\n",
    "    collate_fn=SBIVideoDataset.collate_fn,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds, batch_size=config[\"batch_size\"], shuffle=False,\n",
    "    num_workers=2, multiprocessing_context=\"fork\",\n",
    "    pin_memory=True, drop_last=False,\n",
    "    collate_fn=SBIVideoDataset.collate_fn,\n",
    ")\n",
    "\n",
    "# ── Model ─────────────────────────────────────────────────────────────────────\n",
    "device = torch.device(\"cuda:0\")\n",
    "model  = STFMambaV8(pretrained_backbone=True, d_conv=4).to(device)\n",
    "\n",
    "# ── Criterion ─────────────────────────────────────────────────────────────────\n",
    "criterion = STFMambaLoss(lambda_var=config[\"lambda_var\"])\n",
    "\n",
    "# ── Optimizer: differential LR ────────────────────────────────────────────────\n",
    "backbone_params, temporal_params, head_params = [], [], []\n",
    "for name, p in model.named_parameters():\n",
    "    if not p.requires_grad: continue\n",
    "    if \"backbone\" in name or \"dinov2\" in name: backbone_params.append(p)\n",
    "    elif \"mamba\" in name or \"temporal\" in name: temporal_params.append(p)\n",
    "    else: head_params.append(p)\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {\"params\": backbone_params, \"lr\": config[\"lr_backbone\"]},\n",
    "    {\"params\": temporal_params, \"lr\": config[\"lr_temporal\"]},\n",
    "    {\"params\": head_params,     \"lr\": config[\"lr_head\"]},\n",
    "], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=config[\"epochs\"], eta_min=1e-7)\n",
    "\n",
    "# ── History ───────────────────────────────────────────────────────────────────\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [],\n",
    "           \"val_auc\": [], \"val_acc\": [], \"var_gap\": []}\n",
    "best_auc   = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"  STF-Mamba V8.0 — Training ({config['epochs']} epochs, batch={config['batch_size']})\")\n",
    "print(f\"  Train: {len(train_ds)} | Val: {len(val_ds)} | Device: {device}\")\n",
    "print(f\"  lambda_var={config['lambda_var']} | lr_backbone={config['lr_backbone']}\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ── Warmup: set LR from base values each epoch (not compound-multiply) ────\n",
    "    if epoch <= config[\"warmup_epochs\"]:\n",
    "        wf = epoch / config[\"warmup_epochs\"]\n",
    "        optimizer.param_groups[0][\"lr\"] = config[\"lr_backbone\"] * wf\n",
    "        optimizer.param_groups[1][\"lr\"] = config[\"lr_temporal\"] * wf\n",
    "        optimizer.param_groups[2][\"lr\"] = config[\"lr_head\"]     * wf\n",
    "\n",
    "    # ── TRAIN ─────────────────────────────────────────────────────────────────\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Ep {epoch:02d}/{config['epochs']} [Train]\",\n",
    "                leave=True, ncols=120)\n",
    "    for batch in pbar:\n",
    "        frames = batch[\"frames\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        out       = model(frames)\n",
    "        loss_dict = criterion(out[\"logits\"], labels, out[\"variance\"])\n",
    "        loss      = loss_dict[\"total\"]\n",
    "        loss.backward()\n",
    "\n",
    "        if config[\"grad_clip\"] > 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_clip\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = labels.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        preds         = out[\"logits\"].argmax(dim=1)\n",
    "        correct      += (preds == labels).sum().item()\n",
    "        total        += bs\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss_dict['total'].item():.4f}\",\n",
    "            \"ce\":   f\"{loss_dict['ce'].item():.4f}\",\n",
    "            \"var\":  f\"{loss_dict['var'].item():.4f}\",\n",
    "            \"acc\":  f\"{correct/total:.3f}\",\n",
    "            \"lr\":   f\"{optimizer.param_groups[1]['lr']:.1e}\",\n",
    "        })\n",
    "    train_loss = running_loss / total\n",
    "    train_acc  = correct / total\n",
    "    pbar.close()\n",
    "\n",
    "    # ── VALIDATE ──────────────────────────────────────────────────────────────\n",
    "    model.eval()\n",
    "    val_loss_sum = val_correct = val_total = 0\n",
    "    val_var_real, val_var_fake = [], []\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vbar = tqdm(val_loader, desc=f\"Ep {epoch:02d}/{config['epochs']} [ Val ]\",\n",
    "                    leave=True, ncols=120)\n",
    "        for batch in vbar:\n",
    "            frames = batch[\"frames\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"label\"].to(device, non_blocking=True)\n",
    "\n",
    "            out       = model(frames)\n",
    "            loss_dict = criterion(out[\"logits\"], labels, out[\"variance\"])\n",
    "            loss      = loss_dict[\"total\"]\n",
    "\n",
    "            bs = labels.size(0)\n",
    "            val_loss_sum += loss.item() * bs\n",
    "            probs  = torch.softmax(out[\"logits\"], dim=1)[:, 1]\n",
    "            preds  = out[\"logits\"].argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total   += bs\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            for v, lbl in zip(out[\"variance\"].cpu().numpy().flatten(), labels.cpu().numpy()):\n",
    "                (val_var_real if lbl == 0 else val_var_fake).append(float(v))\n",
    "            vbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        vbar.close()\n",
    "\n",
    "    val_loss = val_loss_sum / val_total\n",
    "    val_acc  = val_correct / val_total\n",
    "    try:    val_auc = roc_auc_score(all_labels, all_probs)\n",
    "    except: val_auc = 0.5\n",
    "\n",
    "    _vr     = float(np.mean(val_var_real)) if val_var_real else 0.0\n",
    "    _vf     = float(np.mean(val_var_fake)) if val_var_fake else 0.0\n",
    "    var_gap = _vf - _vr\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # ── Checkpoint ────────────────────────────────────────────────────────────\n",
    "    if val_auc > best_auc:\n",
    "        best_auc   = val_auc\n",
    "        best_epoch = epoch\n",
    "        torch.save({\n",
    "            \"epoch\":                epoch,\n",
    "            \"model_state_dict\":     model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_auc\":              val_auc,\n",
    "        }, os.path.join(CHECKPOINT_DIR, \"best.pth\"))\n",
    "\n",
    "    # ── History ───────────────────────────────────────────────────────────────\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_auc\"].append(val_auc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"var_gap\"].append(var_gap)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"  Ep {epoch:02d} | TrLoss {train_loss:.4f} | TrAcc {train_acc:.3f} \"\n",
    "          f\"| VaLoss {val_loss:.4f} | VaAUC {val_auc:.4f} | VaAcc {val_acc:.3f} \"\n",
    "          f\"| VarGap {var_gap:+.4f} | {elapsed:.0f}s {'★ best' if epoch == best_epoch else ''}\")\n",
    "\n",
    "print(f\"\\n✓ Done! Best Val AUC: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_DIR}/best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-21T20:17:43.426823Z",
     "iopub.status.idle": "2026-02-21T20:17:43.427117Z",
     "shell.execute_reply": "2026-02-21T20:17:43.426991Z",
     "shell.execute_reply.started": "2026-02-21T20:17:43.426978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 6b + 7 + 8: Curves, Evaluation, Variance Visualization\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from stf_mamba import STFMambaV8\n",
    "\n",
    "# ── Training Curves ───────────────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "ep = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "axes[0, 0].plot(ep, history[\"train_loss\"], \"b-\", label=\"Train\")\n",
    "axes[0, 0].plot(ep, history[\"val_loss\"],   \"r-\", label=\"Val\")\n",
    "axes[0, 0].set_title(\"Loss\"); axes[0, 0].legend(); axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(ep, history[\"train_acc\"], \"b-\", label=\"Train\")\n",
    "axes[0, 1].plot(ep, history[\"val_acc\"],   \"r-\", label=\"Val\")\n",
    "axes[0, 1].set_title(\"Accuracy\"); axes[0, 1].legend(); axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(ep, history[\"val_auc\"], \"g-\", marker=\"o\", markersize=3)\n",
    "axes[1, 0].axhline(y=0.75, color=\"orange\", linestyle=\"--\", alpha=0.7, label=\"Stage 5 (0.75)\")\n",
    "axes[1, 0].axhline(y=0.90, color=\"red\",    linestyle=\"--\", alpha=0.7, label=\"Paper (0.90)\")\n",
    "axes[1, 0].set_title(\"Val AUC\"); axes[1, 0].set_ylim([0.4, 1.0])\n",
    "axes[1, 0].legend(); axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(ep, history[\"var_gap\"], \"m-\", marker=\"o\", markersize=3)\n",
    "axes[1, 1].axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1, 1].set_title(\"Variance Gap (fake − real)\"); axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"STF-Mamba V8.0 — Kaggle Training (25 epochs)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, \"training_curves.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"✓ Training curves saved\")\n",
    "\n",
    "# ── Section 7a: FF++ Val AUC (reload best checkpoint) ────────────────────────\n",
    "best_path = os.path.join(CHECKPOINT_DIR, \"best.pth\")\n",
    "print(f\"\\nLoading best checkpoint: {best_path}\")\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "\n",
    "# d_conv=4 must match what was used during training\n",
    "eval_model = STFMambaV8(pretrained_backbone=True, d_conv=4).to(device)\n",
    "eval_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "eval_model.eval()\n",
    "\n",
    "all_probs, all_labels, all_variances = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        frames = batch[\"frames\"].to(device)\n",
    "        labels = batch[\"label\"]\n",
    "        out = eval_model(frames)\n",
    "        probs = torch.softmax(out[\"logits\"], dim=1)[:, 1].cpu().numpy()\n",
    "        var   = out[\"variance\"].cpu().numpy().flatten()\n",
    "        all_probs.extend(probs)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_variances.extend(var)\n",
    "\n",
    "ff_val_auc = roc_auc_score(all_labels, all_probs)\n",
    "ff_val_acc = accuracy_score(all_labels, [1 if p > 0.5 else 0 for p in all_probs])\n",
    "all_labels_np = np.array(all_labels)\n",
    "all_var_np    = np.array(all_variances)\n",
    "var_real = all_var_np[all_labels_np == 0].mean()\n",
    "var_fake = all_var_np[all_labels_np == 1].mean()\n",
    "\n",
    "print(f\"  FF++ Val AUC : {ff_val_auc:.4f}\")\n",
    "print(f\"  FF++ Val Acc : {ff_val_acc:.4f}\")\n",
    "print(f\"  Variance gap : {var_fake - var_real:+.6f}\")\n",
    "\n",
    "# ── Section 7b: Celeb-DF Cross-Dataset ───────────────────────────────────────\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from data.preprocessing import FacePreprocessor\n",
    "from data.augmentation import apply_transform_to_clip\n",
    "\n",
    "cdf_auc = None\n",
    "if CELEB_DF_ROOT and os.path.isdir(CELEB_DF_ROOT):\n",
    "    print(\"\\n--- Celeb-DF v2 Cross-Dataset ---\")\n",
    "\n",
    "    cdf_real_dirs = [os.path.join(CELEB_DF_ROOT, d) for d in [\"Celeb-real\", \"YouTube-real\", \"celeb_real\"]]\n",
    "    cdf_fake_dirs = [os.path.join(CELEB_DF_ROOT, d) for d in [\"Celeb-synthesis\", \"celeb_synthesis\"]]\n",
    "\n",
    "    def find_videos(dir_list):\n",
    "        videos = []\n",
    "        for d in dir_list:\n",
    "            if os.path.isdir(d):\n",
    "                for f in sorted(os.listdir(d)):\n",
    "                    if f.lower().endswith((\".mp4\", \".avi\")):\n",
    "                        videos.append(os.path.join(d, f))\n",
    "        return videos\n",
    "\n",
    "    cdf_real = find_videos(cdf_real_dirs)\n",
    "    cdf_fake = find_videos(cdf_fake_dirs)\n",
    "    print(f\"  Real: {len(cdf_real)}, Fake: {len(cdf_fake)}\")\n",
    "\n",
    "    if cdf_real and cdf_fake:\n",
    "        cdf_preprocessor = FacePreprocessor(\n",
    "            video_dir=CELEB_DF_ROOT,\n",
    "            cache_dir=\"/kaggle/working/cdf_cache\",\n",
    "            num_frames=NUM_FRAMES, img_size=IMG_SIZE,\n",
    "            predictor_path=PREDICTOR_PATH if os.path.exists(PREDICTOR_PATH) else None,\n",
    "        )\n",
    "\n",
    "        def eval_videos(video_paths, label):\n",
    "            probs = []\n",
    "            for vpath in tqdm(video_paths[:200], desc=f\"label={label}\"):\n",
    "                vid_id = os.path.splitext(os.path.basename(vpath))[0]\n",
    "                try:\n",
    "                    crops, _ = cdf_preprocessor.get_video(vid_id)\n",
    "                    frames_t = apply_transform_to_clip(crops[:NUM_FRAMES], val_tf)\n",
    "                    frames_t = frames_t.unsqueeze(0).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        out = eval_model(frames_t)\n",
    "                        prob = torch.softmax(out[\"logits\"], dim=1)[0, 1].item()\n",
    "                    probs.append(prob)\n",
    "                except Exception:\n",
    "                    continue\n",
    "            return probs\n",
    "\n",
    "        real_probs = eval_videos(cdf_real, 0)\n",
    "        fake_probs = eval_videos(cdf_fake, 1)\n",
    "\n",
    "        if real_probs and fake_probs:\n",
    "            cdf_labels = [0] * len(real_probs) + [1] * len(fake_probs)\n",
    "            cdf_probs  = real_probs + fake_probs\n",
    "            cdf_auc    = roc_auc_score(cdf_labels, cdf_probs)\n",
    "            print(f\"  Celeb-DF AUC: {cdf_auc:.4f}\")\n",
    "else:\n",
    "    print(\"\\nCeleb-DF not found — skipping cross-dataset eval\")\n",
    "\n",
    "# ── Section 7c: Results Summary ───────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  RESULTS SUMMARY — STF-Mamba V8.0 (Kaggle 25 epochs)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<30} {'Value':>10}\")\n",
    "print(\"-\" * 42)\n",
    "print(f\"{'FF++ Val AUC (SBI)':<30} {ff_val_auc:>10.4f}\")\n",
    "print(f\"{'FF++ Val Acc':<30} {ff_val_acc:>10.4f}\")\n",
    "if cdf_auc:\n",
    "    print(f\"{'Celeb-DF AUC':<30} {cdf_auc:>10.4f}\")\n",
    "print(f\"{'Variance gap (fake-real)':<30} {var_fake - var_real:>+10.6f}\")\n",
    "print(f\"{'Best epoch':<30} {ckpt['epoch']:>10d}\")\n",
    "\n",
    "print(f\"\\n--- Comparison to Baselines ---\")\n",
    "print(f\"{'Model':<35} {'FF++ Val':>10} {'CDF':>10}\")\n",
    "print(\"-\" * 57)\n",
    "print(f\"{'B0 frame-level (Step 3)':<35} {'0.6850':>10} {'0.6135':>10}\")\n",
    "print(f\"{'B0 + GRU temporal (Step 4)':<35} {'0.5954':>10} {'0.5524':>10}\")\n",
    "print(f\"{'SBI reference (EffNet-B4)':<35} {'—':>10} {'0.9382':>10}\")\n",
    "cdf_str = f\"{cdf_auc:.4f}\" if cdf_auc else \"—\"\n",
    "print(f\"{'V8.0 (this run)':<35} {ff_val_auc:>10.4f} {cdf_str:>10}\")\n",
    "\n",
    "print(f\"\\n--- Stage 5 Exit Criteria ---\")\n",
    "if cdf_auc and cdf_auc > 0.75:\n",
    "    print(f\"  ✓ CDF AUC {cdf_auc:.4f} > 0.75\")\n",
    "elif cdf_auc:\n",
    "    print(f\"  ✗ CDF AUC {cdf_auc:.4f} < 0.75\")\n",
    "else:\n",
    "    print(f\"  ? CDF AUC not measured\")\n",
    "\n",
    "if var_fake - var_real > 0:\n",
    "    print(f\"  ✓ Variance gap positive ({var_fake - var_real:+.6f})\")\n",
    "else:\n",
    "    print(f\"  ✗ Variance gap not positive\")\n",
    "\n",
    "last5 = history[\"val_loss\"][-5:]\n",
    "if len(last5) >= 5 and last5[-1] <= last5[0] * 1.2:\n",
    "    print(f\"  ✓ No overfitting (val loss stable)\")\n",
    "\n",
    "print(\"\\n✓ Section 7 complete\")\n",
    "\n",
    "# ── Section 8: Variance / Similarity Visualization ───────────────────────────\n",
    "n_samples = min(10, len(val_ds) // 2)\n",
    "real_sims, fake_sims = [], []\n",
    "real_vars, fake_vars = [], []\n",
    "\n",
    "eval_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(min(n_samples * 2, len(val_ds))):\n",
    "        sample = val_ds[i]\n",
    "        frames = sample[\"frames\"].unsqueeze(0).to(device)\n",
    "        out = eval_model(frames)\n",
    "        sims = out[\"similarities\"][0].cpu().numpy()\n",
    "        var  = out[\"variance\"][0].item()\n",
    "        if sample[\"label\"] == 0:\n",
    "            real_sims.append(sims); real_vars.append(var)\n",
    "        else:\n",
    "            fake_sims.append(sims); fake_vars.append(var)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "for i, s in enumerate(real_sims[:5]):\n",
    "    axes[0].plot(s, alpha=0.6, label=f\"Real {i}\")\n",
    "axes[0].set_title(f\"Real — Mean σ²={np.mean(real_vars):.6f}\")\n",
    "axes[0].set_ylim([0.5, 1.05]); axes[0].legend(fontsize=8); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "for i, s in enumerate(fake_sims[:5]):\n",
    "    axes[1].plot(s, alpha=0.6, label=f\"Fake {i}\")\n",
    "axes[1].set_title(f\"Fake — Mean σ²={np.mean(fake_vars):.6f}\")\n",
    "axes[1].set_ylim([0.5, 1.05]); axes[1].legend(fontsize=8); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"STF-Mamba V8.0 — Identity Consistency Signal\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, \"similarity_traces.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVariance Statistics:\")\n",
    "print(f\"  Real: {np.mean(real_vars):.6f} ± {np.std(real_vars):.6f}\")\n",
    "print(f\"  Fake: {np.mean(fake_vars):.6f} ± {np.std(fake_vars):.6f}\")\n",
    "print(f\"  Gap:  {np.mean(fake_vars) - np.mean(real_vars):+.6f}\")\n",
    "print(\"\\n✓ Section 8 complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  STF-Mamba V8.0 — Training Complete\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Best FF++ Val AUC : {ff_val_auc:.4f}\")\n",
    "if cdf_auc:\n",
    "    print(f\"  Celeb-DF AUC      : {cdf_auc:.4f}\")\n",
    "print(f\"  Next: Stage 6 — RunPod A100 full 50-epoch training\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 5454468,
     "datasetId": 3120670,
     "sourceId": 5380830,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10408999,
     "datasetId": 6248577,
     "sourceId": 10125851,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 7513477,
     "datasetId": 4313880,
     "sourceId": 7422059,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 1353100,
     "datasetId": 765805,
     "sourceId": 1320834,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 15767467,
     "datasetId": 9535469,
     "sourceId": 14902571,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
