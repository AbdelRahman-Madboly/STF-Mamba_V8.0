{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa112516-2137-4256-99d5-2763e13d89d6",
   "metadata": {},
   "source": [
    "# Step 4: Temporal Consistency Verification Module\n",
    "## Ablation B â€” Does Temporal Modeling Help?\n",
    "\n",
    "**Step 3 result (honest baseline):** EfficientNet-B0, frame-level only â†’ Celeb-DF AUC = 0.6135\n",
    "\n",
    "**This notebook adds the temporal module on top:**\n",
    "- Same EfficientNet-B0 backbone (frozen after initial training)\n",
    "- Input: 8 frames per video â†’ sequence of per-frame embeddings â†’ temporal model\n",
    "- Temporal model: lightweight GRU (fits T4 VRAM, fast to train)\n",
    "- Video-level prediction from temporal sequence\n",
    "\n",
    "**Why GRU before Mamba?**  \n",
    "GRU is fast, well-understood, and a proven temporal baseline. If GRU improves over  \n",
    "frame-level by +3%, Mamba will do better. If GRU shows no improvement, we know  \n",
    "temporal modeling needs rethinking before investing in Mamba.\n",
    "\n",
    "**Ablation question:** Does modeling temporal consistency across frames add signal  \n",
    "beyond what a single frame already provides?\n",
    "\n",
    "**Expected:** +5-10% Celeb-DF AUC (0.6135 â†’ 0.65-0.72)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f314d81-8f7f-4c68-b103-55f7762b5480",
   "metadata": {},
   "source": [
    "## Section 1 â€” Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbd84f-b797-453f-8e8e-0c1feaef27c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T10:47:06.682556Z",
     "iopub.status.busy": "2026-02-19T10:47:06.682336Z",
     "iopub.status.idle": "2026-02-19T10:47:17.672388Z",
     "shell.execute_reply": "2026-02-19T10:47:17.671672Z",
     "shell.execute_reply.started": "2026-02-19T10:47:06.682535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, json, random, time, warnings, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device : {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU    : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM   : {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n",
    "\n",
    "OUTPUT_DIR = Path('/kaggle/working/step4')\n",
    "CKPT_DIR   = OUTPUT_DIR / 'checkpoints'\n",
    "PLOTS_DIR  = OUTPUT_DIR / 'plots'\n",
    "for d in [OUTPUT_DIR, CKPT_DIR, PLOTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Outputs â†’ {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c805929-df89-4f95-bc83-a733d2bb054f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T10:47:17.674203Z",
     "iopub.status.busy": "2026-02-19T10:47:17.673794Z",
     "iopub.status.idle": "2026-02-19T10:47:17.680107Z",
     "shell.execute_reply": "2026-02-19T10:47:17.679471Z",
     "shell.execute_reply.started": "2026-02-19T10:47:17.674181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    # Data\n",
    "    'img_size':        224,\n",
    "    'n_frames':        8,       # â†‘ was 4 â€” more frames = better temporal signal\n",
    "    'n_train_real':    600,\n",
    "    'n_train_fake':    600,     # 150 per method Ã— 4 methods\n",
    "    'n_val_each':      50,\n",
    "\n",
    "    # Model\n",
    "    'embed_dim':       1280,    # EfficientNet-B0 output dim\n",
    "    'temporal_hidden': 512,     # GRU hidden size\n",
    "    'temporal_layers': 2,       # GRU layers\n",
    "    'dropout':         0.3,\n",
    "\n",
    "    # Training â€” two phases\n",
    "    # Phase 1: train temporal head only (backbone frozen) â€” fast convergence\n",
    "    # Phase 2: fine-tune everything together â€” refine\n",
    "    'phase1_epochs':   10,\n",
    "    'phase2_epochs':   10,\n",
    "    'lr_head':         1e-3,    # high LR for fresh temporal head\n",
    "    'lr_backbone':     1e-5,    # low LR for pretrained backbone\n",
    "    'weight_decay':    1e-4,\n",
    "    'label_smoothing': 0.0,\n",
    "    'batch_size':      16,      # lower batch â€” videos not frames now\n",
    "}\n",
    "\n",
    "TRAIN_METHODS = ['Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures']\n",
    "\n",
    "print(\"Config:\")\n",
    "for k, v in CFG.items():\n",
    "    print(f\"  {k:22s}: {v}\")\n",
    "print(f\"Training methods: {TRAIN_METHODS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e77cc-02a0-4c8e-ac75-0810cab9911d",
   "metadata": {},
   "source": [
    "## Section 2 â€” Dataset Paths & ID-Based Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0b9ab-06ef-47ae-8f19-ae47db3f6efb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T10:47:17.681056Z",
     "iopub.status.busy": "2026-02-19T10:47:17.680846Z",
     "iopub.status.idle": "2026-02-19T10:47:36.745547Z",
     "shell.execute_reply": "2026-02-19T10:47:36.744800Z",
     "shell.execute_reply.started": "2026-02-19T10:47:17.681038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "KAGGLE_INPUT = Path('/kaggle/input')\n",
    "\n",
    "def locate_ff_root(base):\n",
    "    known = base / 'datasets' / 'xdxd003' / 'ff-c23' / 'FaceForensics++_C23'\n",
    "    if known.exists(): return known\n",
    "    for d in sorted(base.rglob('*')):\n",
    "        if d.is_dir():\n",
    "            if sum(1 for m in ['Deepfakes','Face2Face','FaceSwap'] if (d/m).exists()) >= 2:\n",
    "                return d\n",
    "    return None\n",
    "\n",
    "def locate_celeb_root(base):\n",
    "    known = base / 'datasets' / 'reubensuju' / 'celeb-df-v2'\n",
    "    if known.exists(): return known\n",
    "    for d in sorted(base.rglob('*')):\n",
    "        if d.is_dir() and (d/'Celeb-real').exists(): return d\n",
    "    return None\n",
    "\n",
    "FF_ROOT    = locate_ff_root(KAGGLE_INPUT)\n",
    "CELEB_ROOT = locate_celeb_root(KAGGLE_INPUT)\n",
    "print(f\"FF++    : {FF_ROOT}\")\n",
    "print(f\"Celeb-DF: {CELEB_ROOT}\")\n",
    "\n",
    "FF_REAL = sorted(FF_ROOT.rglob('original*/*.mp4')) if FF_ROOT else []\n",
    "if not FF_REAL and FF_ROOT:\n",
    "    FF_REAL = sorted(p for p in FF_ROOT.rglob('*.mp4') if 'original' in str(p).lower())\n",
    "\n",
    "FF_FAKE_BY_METHOD = {}\n",
    "for method in TRAIN_METHODS:\n",
    "    paths = sorted((FF_ROOT/method).glob('*.mp4')) if FF_ROOT and (FF_ROOT/method).exists() else []\n",
    "    FF_FAKE_BY_METHOD[method] = paths\n",
    "    print(f\"  FF++/{method:20s}: {len(paths)} videos\")\n",
    "print(f\"  FF++/{'real':20s}: {len(FF_REAL)} videos\")\n",
    "\n",
    "CDF_REAL, CDF_FAKE = [], []\n",
    "if CELEB_ROOT:\n",
    "    CDF_REAL = (sorted((CELEB_ROOT/'Celeb-real').glob('*.mp4')) +\n",
    "                sorted((CELEB_ROOT/'YouTube-real').glob('*.mp4')))\n",
    "    CDF_FAKE = sorted((CELEB_ROOT/'Celeb-synthesis').glob('*.mp4'))\n",
    "    print(f\"  Celeb-DF real: {len(CDF_REAL)} | fake: {len(CDF_FAKE)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040505a1-6e8c-46e1-af90-fcff804cc426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T10:47:36.747285Z",
     "iopub.status.busy": "2026-02-19T10:47:36.747042Z",
     "iopub.status.idle": "2026-02-19T10:47:36.854765Z",
     "shell.execute_reply": "2026-02-19T10:47:36.853969Z",
     "shell.execute_reply.started": "2026-02-19T10:47:36.747264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ ID-based split (no leakage) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_video_id(path):\n",
    "    return Path(path).stem.split('_')[0]\n",
    "\n",
    "rng = random.Random(SEED)\n",
    "all_ids = sorted(set(get_video_id(p) for p in FF_REAL))\n",
    "rng.shuffle(all_ids)\n",
    "n_train_ids = int(len(all_ids) * 0.75)\n",
    "train_ids   = set(all_ids[:n_train_ids])\n",
    "val_ids     = set(all_ids[n_train_ids:])\n",
    "print(f\"Video IDs â€” train: {len(train_ids)}, val: {len(val_ids)} (no overlap)\")\n",
    "\n",
    "n_per_method = CFG['n_train_fake'] // len(TRAIN_METHODS)\n",
    "\n",
    "# Training set\n",
    "train_real = rng.sample([p for p in FF_REAL if get_video_id(p) in train_ids],\n",
    "                         min(CFG['n_train_real'], len([p for p in FF_REAL if get_video_id(p) in train_ids])))\n",
    "TRAIN_DATA = [(p, 0) for p in train_real]\n",
    "for method in TRAIN_METHODS:\n",
    "    pool   = [p for p in FF_FAKE_BY_METHOD[method] if get_video_id(p) in train_ids]\n",
    "    picked = rng.sample(pool, min(n_per_method, len(pool)))\n",
    "    TRAIN_DATA += [(p, 1) for p in picked]\n",
    "rng.shuffle(TRAIN_DATA)\n",
    "\n",
    "# Validation set\n",
    "val_real = rng.sample([p for p in FF_REAL if get_video_id(p) in val_ids],\n",
    "                       min(CFG['n_val_each'], len([p for p in FF_REAL if get_video_id(p) in val_ids])))\n",
    "VAL_DATA = [(p, 0) for p in val_real]\n",
    "for method in TRAIN_METHODS:\n",
    "    pool   = [p for p in FF_FAKE_BY_METHOD[method] if get_video_id(p) in val_ids]\n",
    "    picked = rng.sample(pool, min(CFG['n_val_each']//len(TRAIN_METHODS), len(pool)))\n",
    "    VAL_DATA += [(p, 1) for p in picked]\n",
    "rng.shuffle(VAL_DATA)\n",
    "\n",
    "# Celeb-DF\n",
    "n_cdf    = min(200, len(CDF_REAL), len(CDF_FAKE))\n",
    "CDF_TEST = ([(p,0) for p in rng.sample(CDF_REAL, n_cdf)] +\n",
    "            [(p,1) for p in rng.sample(CDF_FAKE,  n_cdf)])\n",
    "\n",
    "print(f\"Train: {sum(1 for _,l in TRAIN_DATA if l==0)} real + \"\n",
    "      f\"{sum(1 for _,l in TRAIN_DATA if l==1)} fake = {len(TRAIN_DATA)}\")\n",
    "print(f\"Val  : {sum(1 for _,l in VAL_DATA   if l==0)} real + \"\n",
    "      f\"{sum(1 for _,l in VAL_DATA   if l==1)} fake = {len(VAL_DATA)}\")\n",
    "print(f\"CDF  : {n_cdf} real + {n_cdf} fake = {len(CDF_TEST)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f498856-9152-4fe9-b065-163c79b3b055",
   "metadata": {},
   "source": [
    "## Section 3 â€” Video Dataset\n",
    "\n",
    "**Key difference from Steps 2-3:** each sample is now a full video clip (8 frames),  \n",
    "not an individual frame. The model receives a sequence and predicts once per video.  \n",
    "This is what enables temporal modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051bd41-4e8f-42c1-af95-e1887c655f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T10:47:36.855924Z",
     "iopub.status.busy": "2026-02-19T10:47:36.855644Z",
     "iopub.status.idle": "2026-02-19T11:22:35.971726Z",
     "shell.execute_reply": "2026-02-19T11:22:35.970814Z",
     "shell.execute_reply.started": "2026-02-19T10:47:36.855886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "frame_tf = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "aug_tf = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "def load_video_clip(video_path, n_frames, img_size):\n",
    "    \"\"\"Load n evenly-spaced frames. Returns (n_frames, H, W, 3) uint8 or None.\"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened(): return None\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total < 1:\n",
    "        cap.release(); return None\n",
    "    positions = np.linspace(0, total-1, n_frames, dtype=int)\n",
    "    frames = []\n",
    "    for pos in positions:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(pos))\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: continue\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w = frame.shape[:2]\n",
    "        frame = frame[int(h*0.05):int(h*0.95), int(w*0.10):int(w*0.90)]\n",
    "        frame = cv2.resize(frame, (img_size, img_size))\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    if not frames: return None\n",
    "    while len(frames) < n_frames: frames.append(frames[-1])\n",
    "    return frames[:n_frames]\n",
    "\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item is a full video clip: (n_frames, 3, H, W) tensor.\n",
    "    Pre-extracts all clips at construction â€” no video I/O in DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, video_label_pairs, n_frames, img_size, augment=False):\n",
    "        self.augment   = augment\n",
    "        self.transform = aug_tf if augment else frame_tf\n",
    "        self.clips     = []   # list of (frames_list, label)\n",
    "        failed = 0\n",
    "        for path, label in tqdm(video_label_pairs, ncols=80, desc='Loading clips'):\n",
    "            frames = load_video_clip(str(path), n_frames, img_size)\n",
    "            if frames is None:\n",
    "                failed += 1\n",
    "                continue\n",
    "            self.clips.append((frames, label))\n",
    "        print(f\"  {len(self.clips)} clips ready ({failed} failed)\")\n",
    "\n",
    "    def __len__(self): return len(self.clips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames, label = self.clips[idx]\n",
    "        # Apply same random transform consistently to all frames in clip\n",
    "        tensors = torch.stack([self.transform(f) for f in frames])  # (T, 3, H, W)\n",
    "        return tensors, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "print(\"Pre-extracting video clips (~8 min for 1200 videos Ã— 8 frames)...\")\n",
    "t0 = time.time()\n",
    "train_ds = VideoDataset(TRAIN_DATA, CFG['n_frames'], CFG['img_size'], augment=True)\n",
    "val_ds   = VideoDataset(VAL_DATA,   CFG['n_frames'], CFG['img_size'], augment=False)\n",
    "cdf_ds   = VideoDataset(CDF_TEST,   CFG['n_frames'], CFG['img_size'], augment=False)\n",
    "print(f\"Done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG['batch_size'],\n",
    "                          shuffle=True,  num_workers=0, pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=CFG['batch_size'],\n",
    "                          shuffle=False, num_workers=0, pin_memory=False)\n",
    "cdf_loader   = DataLoader(cdf_ds,   batch_size=CFG['batch_size'],\n",
    "                          shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(f\"Train clips: {len(train_ds)} | Val: {len(val_ds)} | CDF: {len(cdf_ds)}\")\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"Batch: x={x.shape} (B, T, C, H, W), labels={y.unique().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f5d66d-fa0f-4d1c-9b6e-435544f906a1",
   "metadata": {},
   "source": [
    "## Section 4 â€” Model: EfficientNet-B0 + Temporal GRU\n",
    "\n",
    "**Architecture:**\n",
    "1. EfficientNet-B0 extracts per-frame embeddings independently â†’ (B, T, 1280)\n",
    "2. GRU processes the temporal sequence â†’ captures frame-to-frame consistency\n",
    "3. Final hidden state â†’ classification head\n",
    "\n",
    "**Two-phase training:**\n",
    "- Phase 1 (epochs 1-10): backbone FROZEN, only train GRU + head\n",
    "- Phase 2 (epochs 11-20): unfreeze backbone with very low LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df6770-0241-47c6-8167-59cd0dcb7f07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T11:22:35.973432Z",
     "iopub.status.busy": "2026-02-19T11:22:35.973078Z",
     "iopub.status.idle": "2026-02-19T11:22:37.900921Z",
     "shell.execute_reply": "2026-02-19T11:22:37.900311Z",
     "shell.execute_reply.started": "2026-02-19T11:22:35.973408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TemporalDeepfakeDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet-B0 spatial backbone + bidirectional GRU temporal model.\n",
    "    \n",
    "    The GRU processes the sequence of per-frame embeddings and detects\n",
    "    inconsistencies in face identity across time â€” the core temporal signal.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # â”€â”€ Spatial backbone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        effnet = models.efficientnet_b0(\n",
    "            weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        # Remove the classifier â€” keep feature extractor only\n",
    "        self.backbone = effnet.features   # outputs (B, 1280, 7, 7)\n",
    "        self.pool     = nn.AdaptiveAvgPool2d(1)   # â†’ (B, 1280)\n",
    "\n",
    "        # â”€â”€ Temporal module â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # Project to smaller dim before GRU to save memory\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(CFG['embed_dim'], CFG['temporal_hidden']),\n",
    "            nn.LayerNorm(CFG['temporal_hidden']),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Bidirectional GRU â€” forward pass sees future, backward sees past\n",
    "        # This lets the model ask: \"is frame 8 consistent with frame 1?\"\n",
    "        self.gru = nn.GRU(\n",
    "            input_size  = CFG['temporal_hidden'],\n",
    "            hidden_size = CFG['temporal_hidden'],\n",
    "            num_layers  = CFG['temporal_layers'],\n",
    "            batch_first = True,\n",
    "            bidirectional = True,\n",
    "            dropout = CFG['dropout'] if CFG['temporal_layers'] > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # â”€â”€ Classification head â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        gru_out_dim = CFG['temporal_hidden'] * 2   # bidirectional\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(CFG['dropout']),\n",
    "            nn.Linear(gru_out_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(CFG['dropout'] * 0.5),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "\n",
    "        # Orthogonal init for GRU (prevents dead-branch bug from V7.3)\n",
    "        for name, param in self.gru.named_parameters():\n",
    "            if 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def extract_frame_embeddings(self, x_video):\n",
    "        \"\"\"\n",
    "        x_video: (B, T, C, H, W)\n",
    "        returns: (B, T, embed_dim)\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x_video.shape\n",
    "        # Flatten batch and time dims â†’ process all frames at once\n",
    "        x_flat = x_video.view(B * T, C, H, W)\n",
    "        feats  = self.backbone(x_flat)    # (B*T, 1280, 7, 7)\n",
    "        feats  = self.pool(feats)         # (B*T, 1280, 1, 1)\n",
    "        feats  = feats.view(B*T, -1)      # (B*T, 1280)\n",
    "        feats  = feats.view(B, T, -1)     # (B, T, 1280)\n",
    "        return feats\n",
    "\n",
    "    def forward(self, x_video):\n",
    "        \"\"\"\n",
    "        x_video: (B, T, C, H, W)\n",
    "        returns: logits (B, 2)\n",
    "        \"\"\"\n",
    "        # Step 1: per-frame embeddings\n",
    "        embeds = self.extract_frame_embeddings(x_video)  # (B, T, 1280)\n",
    "\n",
    "        # Step 2: project to temporal hidden dim\n",
    "        proj   = self.proj(embeds)   # (B, T, 512)\n",
    "\n",
    "        # Step 3: temporal GRU â€” models consistency across frames\n",
    "        out, _ = self.gru(proj)      # (B, T, 1024) bidirectional\n",
    "\n",
    "        # Step 4: use mean pooling over time (captures overall temporal pattern)\n",
    "        # + last hidden state (captures final state after seeing all frames)\n",
    "        temporal_feat = out.mean(dim=1)  # (B, 1024)\n",
    "\n",
    "        # Step 5: classify\n",
    "        return self.head(temporal_feat)  # (B, 2)\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "        print(\"Backbone FROZEN\")\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = True\n",
    "        print(\"Backbone UNFROZEN\")\n",
    "\n",
    "    def get_param_groups(self, phase):\n",
    "        if phase == 1:\n",
    "            # Phase 1: only temporal module + head\n",
    "            return [{'params': list(self.proj.parameters()) +\n",
    "                               list(self.gru.parameters()) +\n",
    "                               list(self.head.parameters()),\n",
    "                     'lr': CFG['lr_head']}]\n",
    "        else:\n",
    "            # Phase 2: backbone (low LR) + temporal (lower LR for stability)\n",
    "            return [\n",
    "                {'params': self.backbone.parameters(), 'lr': CFG['lr_backbone']},\n",
    "                {'params': list(self.proj.parameters()) +\n",
    "                           list(self.gru.parameters()) +\n",
    "                           list(self.head.parameters()),\n",
    "                 'lr': CFG['lr_head'] / 10},\n",
    "            ]\n",
    "\n",
    "\n",
    "model = TemporalDeepfakeDetector().to(DEVICE)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "backbone_p = sum(p.numel() for p in model.backbone.parameters())\n",
    "temporal_p = sum(p.numel() for p in list(model.proj.parameters()) +\n",
    "                              list(model.gru.parameters()) +\n",
    "                              list(model.head.parameters()))\n",
    "print(f\"Total params   : {total/1e6:.2f}M\")\n",
    "print(f\"Backbone params: {backbone_p/1e6:.2f}M\")\n",
    "print(f\"Temporal params: {temporal_p/1e6:.2f}M\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_in  = torch.randn(2, CFG['n_frames'], 3, 224, 224).to(DEVICE)\n",
    "    test_out = model(test_in)\n",
    "    print(f\"Forward: (2, {CFG['n_frames']}, 3, 224, 224) â†’ {test_out.shape} âœ“\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb76047-123a-40fc-8039-fd9a3ee90a9f",
   "metadata": {},
   "source": [
    "## Section 5 â€” Two-Phase Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673deb2-696c-43ae-8b3f-3d794135d817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T11:22:37.902046Z",
     "iopub.status.busy": "2026-02-19T11:22:37.901771Z",
     "iopub.status.idle": "2026-02-19T11:22:37.916085Z",
     "shell.execute_reply": "2026-02-19T11:22:37.915540Z",
     "shell.execute_reply.started": "2026-02-19T11:22:37.902023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=CFG['label_smoothing'])\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss   = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct    += (logits.detach().argmax(1) == y).sum().item()\n",
    "        total      += y.size(0)\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_labels, all_probs = [], []\n",
    "    total_loss, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y   = x.to(DEVICE), y.to(DEVICE)\n",
    "            logits = model(x)\n",
    "            total_loss += criterion(logits, y).item()\n",
    "            probs  = F.softmax(logits, dim=1)[:, 1]\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            n += 1\n",
    "    labels = np.array(all_labels)\n",
    "    probs  = np.array(all_probs)\n",
    "    auc    = roc_auc_score(labels, probs) if len(np.unique(labels)) > 1 else 0.5\n",
    "    acc    = ((probs > 0.5).astype(int) == labels).mean()\n",
    "    return {'auc': auc, 'acc': acc, 'loss': total_loss/max(n,1),\n",
    "            'labels': labels, 'probs': probs}\n",
    "\n",
    "\n",
    "def run_phase(phase, epochs, model, loader_tr, loader_val):\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.get_param_groups(phase), weight_decay=CFG['weight_decay'])\n",
    "\n",
    "    def lr_lambda(ep):\n",
    "        warmup = 2\n",
    "        if ep < warmup: return (ep+1)/warmup\n",
    "        progress = (ep-warmup) / max(1, epochs-warmup)\n",
    "        return 0.5*(1+np.cos(np.pi*progress))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    history = {'train_loss':[], 'train_acc':[], 'val_auc':[], 'val_loss':[]}\n",
    "    best_auc, best_epoch = 0.0, 0\n",
    "\n",
    "    print(f\"\\n{'='*68}\")\n",
    "    print(f\"PHASE {phase} â€” {'Temporal head only (backbone frozen)' if phase==1 else 'Full fine-tune'}\")\n",
    "    print(f\"{'='*68}\")\n",
    "    print(f\"{'Ep':>3} {'TrLoss':>8} {'TrAcc':>7} {'VaLoss':>8} \"\n",
    "          f\"{'VaAUC':>7} {'VaAcc':>7} {'t':>5}\")\n",
    "    print(f\"{'-'*68}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_acc = train_epoch(model, loader_tr, optimizer)\n",
    "        val_m           = evaluate(model, loader_val)\n",
    "        scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['train_acc'].append(tr_acc)\n",
    "        history['val_auc'].append(val_m['auc'])\n",
    "        history['val_loss'].append(val_m['loss'])\n",
    "\n",
    "        flag = ' âœ“' if val_m['auc'] > best_auc else ''\n",
    "        print(f\"{epoch+1:>3} {tr_loss:>8.4f} {tr_acc:>7.3f} {val_m['loss']:>8.4f} \"\n",
    "              f\"{val_m['auc']:>7.4f} {val_m['acc']:>7.3f} \"\n",
    "              f\"{time.time()-t0:>4.0f}s{flag}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if val_m['auc'] > best_auc:\n",
    "            best_auc   = val_m['auc']\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save({'epoch': epoch, 'model_state': model.state_dict(),\n",
    "                        'val_auc': best_auc, 'phase': phase},\n",
    "                       CKPT_DIR / f'best_phase{phase}.pth')\n",
    "\n",
    "    print(f\"Phase {phase} best: AUC={best_auc:.4f} at epoch {best_epoch}\")\n",
    "    return history, best_auc\n",
    "\n",
    "print(\"âœ… Training functions ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f759d-0e69-454e-bf89-88afa2c6530e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T11:22:37.917407Z",
     "iopub.status.busy": "2026-02-19T11:22:37.916965Z",
     "iopub.status.idle": "2026-02-19T11:53:48.123518Z",
     "shell.execute_reply": "2026-02-19T11:53:48.122587Z",
     "shell.execute_reply.started": "2026-02-19T11:22:37.917387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# â”€â”€ Phase 1: Train temporal head only â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model.freeze_backbone()\n",
    "hist1, best_auc_p1 = run_phase(1, CFG['phase1_epochs'], model, train_loader, val_loader)\n",
    "\n",
    "# â”€â”€ Phase 2: Fine-tune everything â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model.unfreeze_backbone()\n",
    "hist2, best_auc_p2 = run_phase(2, CFG['phase2_epochs'], model, train_loader, val_loader)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal training time: {total_time/60:.1f} min\")\n",
    "print(f\"Best phase 1 AUC: {best_auc_p1:.4f}\")\n",
    "print(f\"Best phase 2 AUC: {best_auc_p2:.4f}\")\n",
    "\n",
    "# Load the best overall model\n",
    "best_phase = 2 if best_auc_p2 >= best_auc_p1 else 1\n",
    "ckpt = torch.load(CKPT_DIR / f'best_phase{best_phase}.pth',\n",
    "                  map_location=DEVICE, weights_only=False)\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "print(f\"Loaded best model from phase {best_phase}, epoch {ckpt['epoch']+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f615add-6e0a-4c5a-bea4-f383734952d6",
   "metadata": {},
   "source": [
    "## Section 6 â€” Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd607018-84ef-426f-ab34-3093db104a8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T11:53:48.124795Z",
     "iopub.status.busy": "2026-02-19T11:53:48.124504Z",
     "iopub.status.idle": "2026-02-19T11:53:57.287898Z",
     "shell.execute_reply": "2026-02-19T11:53:57.287149Z",
     "shell.execute_reply.started": "2026-02-19T11:53:48.124766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ff_m  = evaluate(model, val_loader)\n",
    "cdf_m = evaluate(model, cdf_loader)\n",
    "\n",
    "# Baselines from previous steps\n",
    "STEP3 = {'ff_auc': 0.6850, 'cdf_auc': 0.6135}\n",
    "\n",
    "print(\"\\n\" + \"=\"*62)\n",
    "print(\"ABLATION RESULTS â€” Temporal vs Frame-Level\")\n",
    "print(\"=\"*62)\n",
    "print(f\"{'Metric':<35} {'Step 3 (frame)':>14} {'Step 4 (temporal)':>14}\")\n",
    "print(\"-\"*62)\n",
    "print(f\"{'FF++ Val AUC':<35} {STEP3['ff_auc']:>14.4f} {ff_m['auc']:>14.4f}\")\n",
    "print(f\"{'Celeb-DF AUC (cross-dataset)':<35} {STEP3['cdf_auc']:>14.4f} {cdf_m['auc']:>14.4f}\")\n",
    "delta = cdf_m['auc'] - STEP3['cdf_auc']\n",
    "print(f\"{'Improvement from temporal module':<35} {'':>14} {delta:>+14.4f}\")\n",
    "print(\"=\"*62)\n",
    "\n",
    "if delta >= 0.05:\n",
    "    verdict = \"ðŸŸ¢ TEMPORAL HELPS â€” +5%+ improvement. Mamba will do better.\"\n",
    "elif delta >= 0.02:\n",
    "    verdict = \"ðŸŸ¡ MODEST IMPROVEMENT â€” Temporal adds signal. Continue to Step 5 (B4).\"\n",
    "elif delta >= 0.0:\n",
    "    verdict = \"ðŸŸ¡ MARGINAL â€” Temporal module barely helps with B0. B4 backbone needed first.\"\n",
    "else:\n",
    "    verdict = \"ðŸ”´ NO IMPROVEMENT â€” Investigate: temporal head may need more epochs or data.\"\n",
    "print(f\"\\n{verdict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46b0d0a-9e91-47d2-a8e2-31f740089463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T11:53:57.290698Z",
     "iopub.status.busy": "2026-02-19T11:53:57.290293Z",
     "iopub.status.idle": "2026-02-19T11:53:58.322361Z",
     "shell.execute_reply": "2026-02-19T11:53:58.321635Z",
     "shell.execute_reply.started": "2026-02-19T11:53:57.290673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Plots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Step 4: Temporal GRU â€” Training Curves & Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Combined loss curves (phase 1 + phase 2)\n",
    "all_tr_loss = hist1['train_loss'] + hist2['train_loss']\n",
    "all_va_loss = hist1['val_loss']   + hist2['val_loss']\n",
    "all_va_auc  = hist1['val_auc']    + hist2['val_auc']\n",
    "x = range(1, len(all_tr_loss)+1)\n",
    "split = CFG['phase1_epochs']\n",
    "\n",
    "axes[0].plot(x, all_tr_loss, color='#3498db', linewidth=2, label='Train loss')\n",
    "axes[0].plot(x, all_va_loss, color='#e74c3c', linewidth=2, label='Val loss')\n",
    "axes[0].axvline(split+0.5, color='gray', linestyle='--', alpha=0.7, label='Phase boundary')\n",
    "axes[0].set_title('Loss (P1: frozen | P2: fine-tune)')\n",
    "axes[0].set_xlabel('Epoch'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(x, all_va_auc, color='#2ecc71', linewidth=2.5, label='Val AUC')\n",
    "axes[1].axvline(split+0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "axes[1].axhline(max(all_va_auc), color='#2ecc71', linestyle='--', alpha=0.5,\n",
    "                label=f'Best={max(all_va_auc):.4f}')\n",
    "axes[1].axhline(STEP3['ff_auc'], color='gray', linestyle=':', alpha=0.6,\n",
    "                label=f'Step3={STEP3[\"ff_auc\"]:.4f}')\n",
    "axes[1].axhline(cdf_m['auc'], color='#e74c3c', linestyle='--', alpha=0.7,\n",
    "                label=f'CDF={cdf_m[\"auc\"]:.4f}')\n",
    "axes[1].axhline(STEP3['cdf_auc'], color='#e74c3c', linestyle=':', alpha=0.5,\n",
    "                label=f'Step3 CDF={STEP3[\"cdf_auc\"]:.4f}')\n",
    "axes[1].set_title('Val AUC'); axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylim(0.40, 1.0); axes[1].legend(fontsize=8); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# ROC curves\n",
    "for color, m, label in [\n",
    "    ('#3498db', ff_m,  f\"FF++ Val (AUC={ff_m['auc']:.4f})\"),\n",
    "    ('#e74c3c', cdf_m, f\"Celeb-DF (AUC={cdf_m['auc']:.4f})\"),\n",
    "]:\n",
    "    fpr, tpr, _ = roc_curve(m['labels'], m['probs'])\n",
    "    axes[2].plot(fpr, tpr, color=color, linewidth=2, label=label)\n",
    "axes[2].plot([0,1],[0,1],'k--', alpha=0.4, label='Random')\n",
    "axes[2].set_title('ROC Curves'); axes[2].set_xlabel('FPR'); axes[2].set_ylabel('TPR')\n",
    "axes[2].legend(fontsize=9); axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'step4_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… step4_results.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d0d14-2190-4254-8a00-9fb7ce12ed64",
   "metadata": {},
   "source": [
    "## Section 7 â€” Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95043a31-0fb9-4ab3-8845-30b41650d853",
   "metadata": {
    "execution": {
     "execution_failed": "2026-02-19T12:20:10.761Z",
     "iopub.execute_input": "2026-02-19T11:53:58.325740Z",
     "iopub.status.busy": "2026-02-19T11:53:58.325443Z",
     "iopub.status.idle": "2026-02-19T11:53:58.332819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    'model':           'EfficientNet-B0 + Bidirectional GRU (2 layers)',\n",
    "    'n_frames':        CFG['n_frames'],\n",
    "    'train_methods':   TRAIN_METHODS,\n",
    "    'phase1_best_auc': round(best_auc_p1, 4),\n",
    "    'phase2_best_auc': round(best_auc_p2, 4),\n",
    "    'ff_val':          {'auc': round(ff_m['auc'],  4), 'acc': round(ff_m['acc'],  4)},\n",
    "    'celeb_df':        {'auc': round(cdf_m['auc'], 4), 'acc': round(cdf_m['acc'], 4)},\n",
    "    'step3_cdf_auc':   STEP3['cdf_auc'],\n",
    "    'temporal_improvement': round(cdf_m['auc'] - STEP3['cdf_auc'], 4),\n",
    "    'training_minutes': round(total_time/60, 1),\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'step4_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 4 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Frame-level baseline (Step 3): CDF AUC = {STEP3['cdf_auc']:.4f}\")\n",
    "print(f\"  + Temporal GRU    (Step 4): CDF AUC = {cdf_m['auc']:.4f}\")\n",
    "print(f\"  Temporal contribution       : {results['temporal_improvement']:+.4f}\")\n",
    "print()\n",
    "print(\"Next: Step 5 â€” upgrade backbone to EfficientNet-B4\")\n",
    "print(f\"âœ… Results â†’ {OUTPUT_DIR / 'step4_results.json'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 5454468,
     "datasetId": 3120670,
     "sourceId": 5380830,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10408999,
     "datasetId": 6248577,
     "sourceId": 10125851,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
