{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STF-Mamba V8.0 — Kaggle Training Notebook\n",
    "\n",
    "**Semantic Temporal Forensics via Hydra-Mamba and DINOv2**\n",
    "\n",
    "Target: CVPR/ICCV 2026 | Platform: Kaggle T4 x2 | 25 epochs\n",
    "\n",
    "---\n",
    "\n",
    "| Section | Contents |\n",
    "|---------|----------|\n",
    "| 1 | Setup + GPU check + install dependencies |\n",
    "| 2 | Clone repo + path configuration |\n",
    "| 3 | Dataset paths (FF++ and Celeb-DF) |\n",
    "| 4 | Preprocessing + SBI cache build |\n",
    "| 5 | Model init + param count + forward pass verify |\n",
    "| 6 | Training loop (25 epochs, batch 8) |\n",
    "| 7 | Evaluation — FF++ val + Celeb-DF AUC |\n",
    "| 8 | Variance visualization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup + GPU Check + Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 1: Setup + GPU Check + Install Dependencies\n",
    "# ============================================================================\n",
    "import subprocess, sys, os\n",
    "\n",
    "# GPU check\n",
    "print(\"=\" * 60)\n",
    "print(\"  STF-Mamba V8.0 — Kaggle Environment Check\")\n",
    "print(\"=\" * 60)\n",
    "!nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)} ({torch.cuda.get_device_properties(i).total_mem / 1e9:.1f} GB)\")\n",
    "\n",
    "# Install mamba_ssm (may fail — Conv1d fallback is built in)\n",
    "print(\"\\n--- Installing mamba_ssm ---\")\n",
    "try:\n",
    "    import mamba_ssm\n",
    "    print(f\"mamba_ssm already installed: {mamba_ssm.__version__}\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        !pip install mamba_ssm -q\n",
    "        import mamba_ssm\n",
    "        print(f\"mamba_ssm installed: {mamba_ssm.__version__}\")\n",
    "    except Exception as e:\n",
    "        print(f\"mamba_ssm install failed (expected on some Kaggle kernels): {e}\")\n",
    "        print(\"Using Conv1d fallback — this is fine for training.\")\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install albumentations einops imutils -q\n",
    "\n",
    "print(\"\\n✓ Section 1 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Clone Repo + Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 2: Clone Repo + Path Configuration\n",
    "# ============================================================================\n",
    "REPO_URL = \"https://github.com/AbdelRahman-Madboly/STF-Mamba_V8.0.git\"\n",
    "REPO_DIR = \"/kaggle/working/STF-Mamba_V8.0\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "else:\n",
    "    print(f\"Repo already cloned at {REPO_DIR}\")\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "# Add repo to Python path\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "os.chdir(REPO_DIR)\n",
    "\n",
    "# Verify imports\n",
    "from stf_mamba import STFMambaV8, STFMambaLoss, is_mamba_available\n",
    "from data import SBIVideoDataset, get_train_transforms, get_val_transforms, load_all_splits\n",
    "from training import Trainer, build_optimizer, build_scheduler\n",
    "\n",
    "print(f\"\\nRepo: {REPO_DIR}\")\n",
    "print(f\"Mamba SSM: {'native' if is_mamba_available() else 'Conv1d fallback'}\")\n",
    "print(\"✓ Section 2 complete — all imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Dataset Paths\n",
    "\n",
    "**Required Kaggle datasets (attach before running):**\n",
    "- FaceForensics++ (original sequences, raw videos)\n",
    "- Celeb-DF v2 (for cross-dataset evaluation)\n",
    "\n",
    "Update the paths below to match your attached dataset names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 3: Dataset Paths\n",
    "# ============================================================================\n",
    "# UPDATE THESE to match your attached Kaggle dataset names:\n",
    "FF_ROOT = \"/kaggle/input/faceforensics\"          # FF++ root\n",
    "CELEB_DF_ROOT = \"/kaggle/input/celeb-df-v2\"      # Celeb-DF root\n",
    "\n",
    "# Auto-detect FF++ video directory\n",
    "FF_VIDEO_CANDIDATES = [\n",
    "    os.path.join(FF_ROOT, \"original_sequences/youtube/raw/videos\"),\n",
    "    os.path.join(FF_ROOT, \"original_sequences/youtube/c23/videos\"),\n",
    "    os.path.join(FF_ROOT, \"youtube/raw/videos\"),\n",
    "    os.path.join(FF_ROOT, \"videos\"),\n",
    "    FF_ROOT,\n",
    "]\n",
    "\n",
    "FF_VIDEO_DIR = None\n",
    "for candidate in FF_VIDEO_CANDIDATES:\n",
    "    if os.path.isdir(candidate):\n",
    "        mp4s = [f for f in os.listdir(candidate) if f.endswith('.mp4')]\n",
    "        if mp4s:\n",
    "            FF_VIDEO_DIR = candidate\n",
    "            print(f\"FF++ videos found: {candidate} ({len(mp4s)} videos)\")\n",
    "            break\n",
    "\n",
    "if FF_VIDEO_DIR is None:\n",
    "    print(\"WARNING: FF++ video directory not found!\")\n",
    "    print(\"Available at FF_ROOT:\")\n",
    "    if os.path.exists(FF_ROOT):\n",
    "        for root, dirs, files in os.walk(FF_ROOT):\n",
    "            depth = root.replace(FF_ROOT, '').count(os.sep)\n",
    "            if depth < 4:\n",
    "                indent = ' ' * 2 * depth\n",
    "                print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    else:\n",
    "        print(f\"  {FF_ROOT} does not exist — attach the FF++ dataset!\")\n",
    "\n",
    "# Working directories\n",
    "SPLITS_DIR = os.path.join(REPO_DIR, \"splits\")\n",
    "CACHE_DIR = \"/kaggle/working/cache\"\n",
    "CHECKPOINT_DIR = \"/kaggle/working/checkpoints\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# dlib predictor path (download if needed)\n",
    "PREDICTOR_PATH = \"/kaggle/working/shape_predictor_81_face_landmarks.dat\"\n",
    "if not os.path.exists(PREDICTOR_PATH):\n",
    "    print(\"\\nDownloading dlib 81-point landmark predictor...\")\n",
    "    # Check if bundled with a dataset\n",
    "    dlib_candidates = [\n",
    "        \"/kaggle/input/dlib-shape-predictor/shape_predictor_81_face_landmarks.dat\",\n",
    "        \"/kaggle/input/shape-predictor/shape_predictor_81_face_landmarks.dat\",\n",
    "    ]\n",
    "    found = False\n",
    "    for c in dlib_candidates:\n",
    "        if os.path.exists(c):\n",
    "            !cp {c} {PREDICTOR_PATH}\n",
    "            found = True\n",
    "            print(f\"  Copied from: {c}\")\n",
    "            break\n",
    "    if not found:\n",
    "        print(\"  NOTE: 81-point predictor not found.\")\n",
    "        print(\"  Attach 'dlib-shape-predictor' dataset, or download manually.\")\n",
    "        print(\"  Source: https://github.com/codeniko/shape_predictor_81_face_landmarks\")\n",
    "\n",
    "# Verify splits\n",
    "splits = load_all_splits(SPLITS_DIR)\n",
    "print(f\"\\nSplits: train={len(splits['train'])}, val={len(splits['val'])}, test={len(splits['test'])}\")\n",
    "\n",
    "print(\"\\n✓ Section 3 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Preprocessing + SBI Cache Build\n",
    "\n",
    "First run: ~15 min to extract face crops from all videos and build SBI cache.  \n",
    "Subsequent runs: loads instantly from NPZ cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 4: Preprocessing + SBI Cache Build\n",
    "# ============================================================================\n",
    "import time\n",
    "from data.preprocessing import FacePreprocessor\n",
    "from data.splits import get_video_ids\n",
    "\n",
    "# --- 4a: Pre-extract face crops for all videos in train + val splits ---\n",
    "print(\"=\" * 60)\n",
    "print(\"  Phase 4a: Face Crop Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "NUM_FRAMES = 32\n",
    "IMG_SIZE = 224\n",
    "\n",
    "preprocessor = FacePreprocessor(\n",
    "    video_dir=FF_VIDEO_DIR,\n",
    "    cache_dir=os.path.join(CACHE_DIR, \"crops\"),\n",
    "    num_frames=NUM_FRAMES,\n",
    "    img_size=IMG_SIZE,\n",
    "    predictor_path=PREDICTOR_PATH if os.path.exists(PREDICTOR_PATH) else None,\n",
    ")\n",
    "\n",
    "# Get all video IDs needed for train + val\n",
    "train_ids = get_video_ids(splits['train'])\n",
    "val_ids = get_video_ids(splits['val'])\n",
    "all_ids = sorted(set(train_ids + val_ids))\n",
    "print(f\"Videos to preprocess: {len(all_ids)} (train: {len(train_ids)}, val: {len(val_ids)})\")\n",
    "\n",
    "t0 = time.time()\n",
    "preprocessor.preprocess_all(all_ids, show_progress=True)\n",
    "print(f\"Face crops done in {time.time()-t0:.0f}s\")\n",
    "\n",
    "# --- 4b: Build SBI fake cache ---\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"  Phase 4b: SBI Fake Generation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_tf = get_train_transforms(IMG_SIZE)\n",
    "val_tf = get_val_transforms(IMG_SIZE)\n",
    "\n",
    "train_ds = SBIVideoDataset(\n",
    "    split_path=os.path.join(SPLITS_DIR, \"Dataset_Split_train.json\"),\n",
    "    video_dir=FF_VIDEO_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    phase=\"train\",\n",
    "    num_frames=NUM_FRAMES,\n",
    "    img_size=IMG_SIZE,\n",
    "    transform=train_tf,\n",
    "    sbi_seed=42,\n",
    "    predictor_path=PREDICTOR_PATH if os.path.exists(PREDICTOR_PATH) else None,\n",
    ")\n",
    "\n",
    "val_ds = SBIVideoDataset(\n",
    "    split_path=os.path.join(SPLITS_DIR, \"Dataset_Split_val.json\"),\n",
    "    video_dir=FF_VIDEO_DIR,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    phase=\"val\",\n",
    "    num_frames=NUM_FRAMES,\n",
    "    img_size=IMG_SIZE,\n",
    "    transform=val_tf,\n",
    "    sbi_seed=42,\n",
    "    predictor_path=PREDICTOR_PATH if os.path.exists(PREDICTOR_PATH) else None,\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "train_ds.build_cache(show_progress=True)\n",
    "val_ds.build_cache(show_progress=True)\n",
    "print(f\"SBI cache done in {time.time()-t0:.0f}s\")\n",
    "\n",
    "# Quick sanity check\n",
    "sample = train_ds[0]\n",
    "print(f\"\\nSanity check:\")\n",
    "print(f\"  frames: {sample['frames'].shape}\")\n",
    "print(f\"  label:  {sample['label']}\")\n",
    "print(f\"  id:     {sample['video_id']}\")\n",
    "print(f\"\\nDataset sizes: train={len(train_ds)}, val={len(val_ds)}\")\n",
    "\n",
    "print(\"\\n✓ Section 4 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Model Init + Param Count + Forward Pass Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 5: Model Init + Param Count + Forward Pass Verify\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Build model\n",
    "print(\"\\nLoading DINOv2-ViT-B/14...\")\n",
    "model = STFMambaV8(pretrained_backbone=True)\n",
    "\n",
    "# Param count\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nParameter Count:\")\n",
    "print(f\"  Total:     {total_params / 1e6:.1f}M\")\n",
    "print(f\"  Trainable: {trainable_params / 1e6:.1f}M\")\n",
    "print(f\"  Frozen:    {frozen_params / 1e6:.1f}M\")\n",
    "\n",
    "# Forward pass verify\n",
    "print(f\"\\nForward pass test...\")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "x_test = torch.randn(1, NUM_FRAMES, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(x_test)\n",
    "\n",
    "print(f\"  Input:    {x_test.shape}\")\n",
    "print(f\"  Logits:   {out['logits'].shape}  ← expected (1, 2)\")\n",
    "print(f\"  Variance: {out['variance'].shape} ← expected (1, 1)\")\n",
    "assert out['logits'].shape == (1, 2), \"Logits shape mismatch!\"\n",
    "assert out['variance'].shape == (1, 1), \"Variance shape mismatch!\"\n",
    "\n",
    "# Clean up test tensor\n",
    "del x_test, out\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Move model back to CPU before Trainer wraps it\n",
    "model = model.cpu()\n",
    "\n",
    "print(\"\\n✓ Section 5 complete — forward pass verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Training (25 Epochs)\n",
    "\n",
    "Config: `configs/v8_kaggle.yaml`\n",
    "- Batch size: 8 (T4 VRAM constraint)\n",
    "- Frames: 32 per clip\n",
    "- LR: backbone=5e-6, temporal+head=1e-4\n",
    "- Loss: CE + 0.1 * variance auxiliary\n",
    "- DataParallel on T4 x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 6: Training Loop\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Kaggle config\n",
    "config = {\n",
    "    \"epochs\": 25,\n",
    "    \"batch_size\": 8,\n",
    "    \"lr_backbone\": 5e-6,\n",
    "    \"lr_temporal\": 1e-4,\n",
    "    \"lr_head\": 1e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"warmup_epochs\": 3,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"lambda_var\": 0.1,\n",
    "}\n",
    "\n",
    "# DataLoaders — num_workers=0 on Kaggle (CRITICAL: prevents deadlock)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=SBIVideoDataset.collate_fn,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=SBIVideoDataset.collate_fn,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_ds)} clips → {len(train_loader)} batches (bs={config['batch_size']})\")\n",
    "print(f\"Val:   {len(val_ds)} clips → {len(val_loader)} batches\")\n",
    "\n",
    "# Loss (label_smoothing=0.0 ALWAYS — Bug #1)\n",
    "criterion = STFMambaLoss(lambda_var=config[\"lambda_var\"])\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    save_dir=CHECKPOINT_DIR,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Train!\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  Starting training: {config['epochs']} epochs on {device}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "history = trainer.train(num_epochs=config[\"epochs\"])\n",
    "\n",
    "print(f\"\\nBest val AUC: {trainer.best_val_auc:.4f}\")\n",
    "print(\"\\n✓ Section 6 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6b: Training Curves ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train')\n",
    "axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val')\n",
    "axes[0, 0].set_title('Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Train')\n",
    "axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Val')\n",
    "axes[0, 1].set_title('Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Val AUC\n",
    "axes[1, 0].plot(epochs, history['val_auc'], 'g-', marker='o', markersize=3)\n",
    "axes[1, 0].axhline(y=0.75, color='orange', linestyle='--', alpha=0.7, label='Stage 5 target (0.75)')\n",
    "axes[1, 0].axhline(y=0.90, color='red', linestyle='--', alpha=0.7, label='Paper target (0.90)')\n",
    "axes[1, 0].set_title('Val AUC (FF++)')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylim([0.4, 1.0])\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Variance Gap\n",
    "axes[1, 1].plot(epochs, history['var_gap'], 'm-', marker='o', markersize=3)\n",
    "axes[1, 1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_title('Variance Gap (fake - real)')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].legend(['Var gap'])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('STF-Mamba V8.0 — Kaggle Training (25 epochs)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Training curves saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Evaluation — FF++ Val + Celeb-DF AUC\n",
    "\n",
    "Load best checkpoint, evaluate on:\n",
    "1. FF++ validation split (SBI)\n",
    "2. Celeb-DF v2 (cross-dataset — the main paper number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 7: Evaluation\n",
    "# ============================================================================\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "# Load best checkpoint\n",
    "best_path = os.path.join(CHECKPOINT_DIR, \"best.pth\")\n",
    "print(f\"Loading best checkpoint: {best_path}\")\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "print(f\"  Saved at epoch {ckpt['epoch']}, val AUC: {ckpt['val_metrics']['auc']:.4f}\")\n",
    "\n",
    "eval_model = STFMambaV8(pretrained_backbone=True).to(device)\n",
    "eval_model.load_state_dict(ckpt['model_state_dict'])\n",
    "eval_model.eval()\n",
    "\n",
    "# ---- 7a: FF++ Val AUC (SBI) ----\n",
    "print(\"\\n--- 7a: FF++ Validation (SBI) ---\")\n",
    "all_probs, all_labels = [], []\n",
    "all_variances = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        frames = batch['frames'].to(device)\n",
    "        labels = batch['label']\n",
    "        out = eval_model(frames)\n",
    "        probs = torch.softmax(out['logits'], dim=1)[:, 1].cpu().numpy()\n",
    "        var = out['variance'].cpu().numpy().flatten()\n",
    "        all_probs.extend(probs)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_variances.extend(var)\n",
    "\n",
    "ff_val_auc = roc_auc_score(all_labels, all_probs)\n",
    "ff_val_acc = accuracy_score(all_labels, [1 if p > 0.5 else 0 for p in all_probs])\n",
    "\n",
    "all_labels_np = np.array(all_labels)\n",
    "all_var_np = np.array(all_variances)\n",
    "var_real = all_var_np[all_labels_np == 0].mean()\n",
    "var_fake = all_var_np[all_labels_np == 1].mean()\n",
    "\n",
    "print(f\"  FF++ Val AUC:  {ff_val_auc:.4f}\")\n",
    "print(f\"  FF++ Val Acc:  {ff_val_acc:.4f}\")\n",
    "print(f\"  Variance real: {var_real:.6f}\")\n",
    "print(f\"  Variance fake: {var_fake:.6f}\")\n",
    "print(f\"  Variance gap:  {var_fake - var_real:+.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 7b: Celeb-DF Cross-Dataset Evaluation ----\n",
    "print(\"\\n--- 7b: Celeb-DF v2 Cross-Dataset ---\")\n",
    "\n",
    "# Celeb-DF evaluation: load real + fake videos, run model\n",
    "import cv2\n",
    "from data.preprocessing import FacePreprocessor\n",
    "from data.augmentation import apply_transform_to_clip\n",
    "\n",
    "# Find Celeb-DF videos\n",
    "cdf_real_dirs = [\n",
    "    os.path.join(CELEB_DF_ROOT, \"Celeb-real\"),\n",
    "    os.path.join(CELEB_DF_ROOT, \"YouTube-real\"),\n",
    "    os.path.join(CELEB_DF_ROOT, \"celeb_real\"),\n",
    "]\n",
    "cdf_fake_dirs = [\n",
    "    os.path.join(CELEB_DF_ROOT, \"Celeb-synthesis\"),\n",
    "    os.path.join(CELEB_DF_ROOT, \"celeb_synthesis\"),\n",
    "]\n",
    "\n",
    "def find_videos(dir_list, extensions=('.mp4', '.avi')):\n",
    "    \"\"\"Find all video files in candidate directories.\"\"\"\n",
    "    videos = []\n",
    "    for d in dir_list:\n",
    "        if os.path.isdir(d):\n",
    "            for root, _, files in os.walk(d):\n",
    "                for f in files:\n",
    "                    if f.lower().endswith(extensions):\n",
    "                        videos.append(os.path.join(root, f))\n",
    "    return sorted(videos)\n",
    "\n",
    "cdf_real_videos = find_videos(cdf_real_dirs)\n",
    "cdf_fake_videos = find_videos(cdf_fake_dirs)\n",
    "print(f\"  Celeb-DF real:  {len(cdf_real_videos)} videos\")\n",
    "print(f\"  Celeb-DF fake:  {len(cdf_fake_videos)} videos\")\n",
    "\n",
    "if len(cdf_real_videos) > 0 and len(cdf_fake_videos) > 0:\n",
    "    # Process Celeb-DF videos\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    cdf_preprocessor = FacePreprocessor(\n",
    "        video_dir=CELEB_DF_ROOT,\n",
    "        cache_dir=os.path.join(CACHE_DIR, \"cdf_crops\"),\n",
    "        num_frames=NUM_FRAMES,\n",
    "        img_size=IMG_SIZE,\n",
    "        predictor_path=PREDICTOR_PATH if os.path.exists(PREDICTOR_PATH) else None,\n",
    "    )\n",
    "\n",
    "    def evaluate_video_list(video_paths, label, model, transform, preprocessor):\n",
    "        \"\"\"Evaluate a list of videos and return probs.\"\"\"\n",
    "        probs_list = []\n",
    "        for vpath in tqdm(video_paths, desc=f\"Eval label={label}\"):\n",
    "            vid_id = os.path.splitext(os.path.basename(vpath))[0]\n",
    "            try:\n",
    "                crops, _ = preprocessor.get_video(vid_id)\n",
    "                frames_t = apply_transform_to_clip(crops[:NUM_FRAMES], transform)\n",
    "                frames_t = frames_t.unsqueeze(0).to(device)  # (1, T, 3, H, W)\n",
    "                with torch.no_grad():\n",
    "                    out = model(frames_t)\n",
    "                    prob = torch.softmax(out['logits'], dim=1)[0, 1].item()\n",
    "                probs_list.append(prob)\n",
    "            except Exception as e:\n",
    "                continue  # Skip failed videos\n",
    "        return probs_list\n",
    "\n",
    "    cdf_real_probs = evaluate_video_list(cdf_real_videos, 0, eval_model, val_tf, cdf_preprocessor)\n",
    "    cdf_fake_probs = evaluate_video_list(cdf_fake_videos, 1, eval_model, val_tf, cdf_preprocessor)\n",
    "\n",
    "    cdf_labels = [0] * len(cdf_real_probs) + [1] * len(cdf_fake_probs)\n",
    "    cdf_probs = cdf_real_probs + cdf_fake_probs\n",
    "\n",
    "    if len(set(cdf_labels)) > 1:\n",
    "        cdf_auc = roc_auc_score(cdf_labels, cdf_probs)\n",
    "        cdf_acc = accuracy_score(cdf_labels, [1 if p > 0.5 else 0 for p in cdf_probs])\n",
    "        print(f\"\\n  Celeb-DF AUC:  {cdf_auc:.4f}\")\n",
    "        print(f\"  Celeb-DF Acc:  {cdf_acc:.4f}\")\n",
    "        print(f\"  Videos evaluated: {len(cdf_real_probs)} real + {len(cdf_fake_probs)} fake\")\n",
    "    else:\n",
    "        cdf_auc = None\n",
    "        print(\"  Could not compute AUC — need both classes\")\n",
    "else:\n",
    "    cdf_auc = None\n",
    "    print(\"  Celeb-DF videos not found — skip cross-dataset eval\")\n",
    "    print(f\"  Looked in: {cdf_real_dirs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 7c: Results Summary Table ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  RESULTS SUMMARY — STF-Mamba V8.0 (Kaggle 25 epochs)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Value':>10}\")\n",
    "print(\"-\" * 42)\n",
    "print(f\"{'FF++ Val AUC (SBI)':<30} {ff_val_auc:>10.4f}\")\n",
    "print(f\"{'FF++ Val Acc':<30} {ff_val_acc:>10.4f}\")\n",
    "if cdf_auc is not None:\n",
    "    print(f\"{'Celeb-DF AUC':<30} {cdf_auc:>10.4f}\")\n",
    "print(f\"{'Variance gap (fake-real)':<30} {var_fake - var_real:>+10.6f}\")\n",
    "print(f\"{'Best epoch':<30} {ckpt['epoch']:>10d}\")\n",
    "\n",
    "print(f\"\\n--- Comparison to Baselines ---\")\n",
    "print(f\"{'Model':<35} {'FF++ Val':>10} {'CDF':>10}\")\n",
    "print(\"-\" * 57)\n",
    "print(f\"{'B0 frame-level (Step 3)':<35} {'0.6850':>10} {'0.6135':>10}\")\n",
    "print(f\"{'B0 + GRU temporal (Step 4)':<35} {'0.5954':>10} {'0.5524':>10}\")\n",
    "print(f\"{'SBI reference (EffNet-B4)':<35} {'—':>10} {'0.9382':>10}\")\n",
    "cdf_str = f\"{cdf_auc:.4f}\" if cdf_auc else \"—\"\n",
    "print(f\"{'V8.0 (this run)':<35} {ff_val_auc:>10.4f} {cdf_str:>10}\")\n",
    "\n",
    "# Stage 5 exit criteria check\n",
    "print(f\"\\n--- Stage 5 Exit Criteria ---\")\n",
    "checks = []\n",
    "if cdf_auc and cdf_auc > 0.75:\n",
    "    checks.append(f\"  ✓ CDF AUC {cdf_auc:.4f} > 0.75\")\n",
    "elif cdf_auc:\n",
    "    checks.append(f\"  ✗ CDF AUC {cdf_auc:.4f} < 0.75 — needs improvement\")\n",
    "else:\n",
    "    checks.append(f\"  ? CDF AUC not measured — attach Celeb-DF dataset\")\n",
    "\n",
    "if var_fake - var_real > 0:\n",
    "    checks.append(f\"  ✓ Variance gap positive ({var_fake - var_real:+.6f})\")\n",
    "else:\n",
    "    checks.append(f\"  ✗ Variance gap not positive — check Section 5.2 troubleshooting\")\n",
    "\n",
    "# Check overfitting\n",
    "last5_val = history['val_loss'][-5:]\n",
    "if len(last5_val) >= 5 and last5_val[-1] <= last5_val[0] * 1.2:\n",
    "    checks.append(f\"  ✓ No overfitting (last 5 val loss stable)\")\n",
    "else:\n",
    "    checks.append(f\"  ? Check val loss trend in last 5 epochs\")\n",
    "\n",
    "for c in checks:\n",
    "    print(c)\n",
    "\n",
    "print(\"\\n✓ Section 7 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Variance Visualization\n",
    "\n",
    "Show per-frame identity variance for real vs fake clips.\n",
    "**Key hypothesis:** Fake clips should show higher temporal variance in identity embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Section 8: Variance Visualization\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Collect per-frame similarities for several real and fake clips\n",
    "n_samples = min(10, len(val_ds) // 2)\n",
    "\n",
    "real_similarities = []\n",
    "fake_similarities = []\n",
    "real_variances = []\n",
    "fake_variances = []\n",
    "\n",
    "eval_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, min(n_samples * 2, len(val_ds)), 2):\n",
    "        # Real clip (even index)\n",
    "        real_sample = val_ds[i]\n",
    "        real_frames = real_sample['frames'].unsqueeze(0).to(device)\n",
    "        real_out = eval_model(real_frames)\n",
    "        real_sims = real_out['similarities'][0].cpu().numpy()\n",
    "        real_similarities.append(real_sims)\n",
    "        real_variances.append(real_out['variance'][0].item())\n",
    "\n",
    "        # Fake clip (odd index)\n",
    "        if i + 1 < len(val_ds):\n",
    "            fake_sample = val_ds[i + 1]\n",
    "            fake_frames = fake_sample['frames'].unsqueeze(0).to(device)\n",
    "            fake_out = eval_model(fake_frames)\n",
    "            fake_sims = fake_out['similarities'][0].cpu().numpy()\n",
    "            fake_similarities.append(fake_sims)\n",
    "            fake_variances.append(fake_out['variance'][0].item())\n",
    "\n",
    "# --- Plot 1: Per-frame similarity traces ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Real clips\n",
    "for i, sims in enumerate(real_similarities[:5]):\n",
    "    axes[0].plot(range(len(sims)), sims, alpha=0.6, label=f'Real {i}')\n",
    "axes[0].set_title(f'Real Clips — Per-Frame Cosine Similarity\\nMean σ² = {np.mean(real_variances):.6f}')\n",
    "axes[0].set_xlabel('Frame')\n",
    "axes[0].set_ylabel('cos(h_t, mean(H))')\n",
    "axes[0].set_ylim([0.5, 1.05])\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Fake clips\n",
    "for i, sims in enumerate(fake_similarities[:5]):\n",
    "    axes[1].plot(range(len(sims)), sims, alpha=0.6, label=f'Fake {i}')\n",
    "axes[1].set_title(f'Fake Clips — Per-Frame Cosine Similarity\\nMean σ² = {np.mean(fake_variances):.6f}')\n",
    "axes[1].set_xlabel('Frame')\n",
    "axes[1].set_ylabel('cos(h_t, mean(H))')\n",
    "axes[1].set_ylim([0.5, 1.05])\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('STF-Mamba V8.0 — Identity Consistency Signal', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, 'similarity_traces.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Variance distribution ---\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bins = np.linspace(\n",
    "    min(min(real_variances, default=0), min(fake_variances, default=0)),\n",
    "    max(max(real_variances, default=1), max(fake_variances, default=1)),\n",
    "    30\n",
    ")\n",
    "ax.hist(real_variances, bins=bins, alpha=0.6, label=f'Real (n={len(real_variances)})', color='green')\n",
    "ax.hist(fake_variances, bins=bins, alpha=0.6, label=f'Fake (n={len(fake_variances)})', color='red')\n",
    "ax.set_title('Temporal Variance Distribution: Real vs Fake')\n",
    "ax.set_xlabel('Identity Variance (σ²)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, 'variance_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVariance Statistics:\")\n",
    "print(f\"  Real mean σ²: {np.mean(real_variances):.6f} ± {np.std(real_variances):.6f}\")\n",
    "print(f\"  Fake mean σ²: {np.mean(fake_variances):.6f} ± {np.std(fake_variances):.6f}\")\n",
    "print(f\"  Gap:           {np.mean(fake_variances) - np.mean(real_variances):+.6f}\")\n",
    "\n",
    "print(\"\\n✓ Section 8 complete\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  STF-Mamba V8.0 — Kaggle Notebook Complete\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Best FF++ Val AUC: {ff_val_auc:.4f}\")\n",
    "if cdf_auc:\n",
    "    print(f\"  Celeb-DF AUC:      {cdf_auc:.4f}\")\n",
    "print(f\"  Checkpoint:        {best_path}\")\n",
    "print(f\"  Next: Stage 6 — RunPod A100 full 50-epoch training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
