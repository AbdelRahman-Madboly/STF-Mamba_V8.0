{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-md-title",
   "metadata": {},
   "source": [
    "# Step 2: EfficientNet-B0 Semantic Baseline\n",
    "## Proving the Pretrained Backbone Hypothesis on Kaggle T4\n",
    "\n",
    "**Concept proof conclusion:** No handcrafted feature separates real from fake on FF++ c23.  \n",
    "The signal is semantic â€” only accessible through ImageNet-pretrained features.\n",
    "\n",
    "**This notebook:** Train EfficientNet-B0 (pretrained ImageNet) on FF++ and measure  \n",
    "cross-dataset AUC on Celeb-DF. This gives us the floor V8.0 must beat.\n",
    "\n",
    "### Key Fixes in This Version\n",
    "- âœ… **Parallel frame extraction** via `ThreadPoolExecutor` (8 threads) â€” ~10x faster than single-threaded sequential scan\n",
    "- âœ… **Keyframe-anchored seek** â€” one `cap.set()` per video to first target, then linear read â€” avoids full-file buffering on network FS\n",
    "- âœ… **All frames cached to RAM** before training â€” zero filesystem I/O in DataLoader\n",
    "- âœ… **Batch-level tqdm** inside train/eval loops â€” training never silently hangs\n",
    "- âœ… **Video validation** step before training to catch corrupt files early\n",
    "- âœ… **Flush after every epoch print** so output appears immediately in Kaggle\n",
    "- â„¹ï¸  **GPU is idle during extraction** â€” that is correct and expected; GPU activates in Section 5\n",
    "\n",
    "### Expected Timeline on Kaggle T4\n",
    "| Phase | Time |\n",
    "|-------|------|\n",
    "| Frame extraction (8 threads) | ~3-5 min |\n",
    "| Training 20 epochs | ~50-70 min |\n",
    "| Evaluation on FF++ val | ~3 min |\n",
    "| Evaluation on Celeb-DF | ~3 min |\n",
    "| **Total** | **~60-80 min** |\n",
    "\n",
    "### Success Criteria\n",
    "- **> 75% Celeb-DF AUC** â†’ semantic signal is strong â†’ V8.0 temporal module will push higher  \n",
    "- **60-75%** â†’ harder problem â†’ need multi-method training  \n",
    "- **< 60%** â†’ something wrong with training setup â†’ debug first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-s1",
   "metadata": {},
   "source": [
    "## Section 1 â€” Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T00:58:02.354887Z",
     "iopub.status.busy": "2026-02-19T00:58:02.354567Z",
     "iopub.status.idle": "2026-02-19T00:58:22.963335Z",
     "shell.execute_reply": "2026-02-19T00:58:22.962532Z",
     "shell.execute_reply.started": "2026-02-19T00:58:02.354858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, sys, json, random, time, warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# â”€â”€ CRITICAL: Disable OpenCV threading BEFORE any cv2 import â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Must happen before cv2 is imported to prevent deadlocks with PyTorch DataLoader\n",
    "import cv2\n",
    "cv2.setNumThreads(0)\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "# â”€â”€ Reproducibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# â”€â”€ Device â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "OUTPUT_DIR = Path('/kaggle/working/baseline')\n",
    "CKPT_DIR   = OUTPUT_DIR / 'checkpoints'\n",
    "PLOTS_DIR  = OUTPUT_DIR / 'plots'\n",
    "for d in [OUTPUT_DIR, CKPT_DIR, PLOTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Outputs â†’ {OUTPUT_DIR}')\n",
    "print(f'tqdm version: {tqdm.__module__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T00:58:22.965330Z",
     "iopub.status.busy": "2026-02-19T00:58:22.964881Z",
     "iopub.status.idle": "2026-02-19T00:58:22.971621Z",
     "shell.execute_reply": "2026-02-19T00:58:22.970753Z",
     "shell.execute_reply.started": "2026-02-19T00:58:22.965302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Training config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CFG = {\n",
    "    # Data\n",
    "    'img_size':        224,\n",
    "    'n_frames':        4,       # frames extracted per video\n",
    "    'n_train_real':    300,\n",
    "    'n_train_fake':    300,\n",
    "    'n_val':           100,\n",
    "\n",
    "    # Training\n",
    "    'epochs':          20,\n",
    "    'batch_size':      32,\n",
    "    'lr':              1e-4,\n",
    "    'weight_decay':    1e-4,\n",
    "    'warmup_epochs':   2,\n",
    "\n",
    "    # Model\n",
    "    'dropout':         0.3,\n",
    "    'label_smoothing': 0.0,\n",
    "\n",
    "    # DataLoader â€” keep 0 workers; cv2+fork = deadlock\n",
    "    'num_workers':     0,\n",
    "    'pin_memory':      False,\n",
    "\n",
    "    # Frame extraction: read sequentially instead of random seek\n",
    "    # This is the main fix for Kaggle /kaggle/input network-mounted FS\n",
    "    'use_sequential_read': True,\n",
    "}\n",
    "\n",
    "print('Config:')\n",
    "for k, v in CFG.items():\n",
    "    print(f'  {k:25s}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-s2",
   "metadata": {},
   "source": [
    "## Section 2 â€” Dataset Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-paths",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T00:58:32.252766Z",
     "iopub.status.busy": "2026-02-19T00:58:32.252445Z",
     "iopub.status.idle": "2026-02-19T00:59:00.150442Z",
     "shell.execute_reply": "2026-02-19T00:59:00.149605Z",
     "shell.execute_reply.started": "2026-02-19T00:58:32.252739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "KAGGLE_INPUT = Path('/kaggle/input')\n",
    "\n",
    "def locate_ff_root(base):\n",
    "    known = base / 'datasets' / 'xdxd003' / 'ff-c23' / 'FaceForensics++_C23'\n",
    "    if known.exists(): return known\n",
    "    for d in sorted(base.rglob('*')):\n",
    "        if d.is_dir():\n",
    "            if sum(1 for m in ['Deepfakes', 'Face2Face', 'FaceSwap']\n",
    "                   if (d / m).exists()) >= 2:\n",
    "                return d\n",
    "    return None\n",
    "\n",
    "def locate_celeb_root(base):\n",
    "    known = base / 'datasets' / 'reubensuju' / 'celeb-df-v2'\n",
    "    if known.exists(): return known\n",
    "    for d in sorted(base.rglob('*')):\n",
    "        if d.is_dir() and (d / 'Celeb-real').exists():\n",
    "            return d\n",
    "    return None\n",
    "\n",
    "FF_ROOT    = locate_ff_root(KAGGLE_INPUT)\n",
    "CELEB_ROOT = locate_celeb_root(KAGGLE_INPUT)\n",
    "print(f'FF++    : {FF_ROOT}')\n",
    "print(f'Celeb-DF: {CELEB_ROOT}')\n",
    "\n",
    "assert FF_ROOT is not None,    'ERROR: FF++ root not found â€” check dataset mount'\n",
    "assert CELEB_ROOT is not None, 'ERROR: Celeb-DF root not found â€” check dataset mount'\n",
    "\n",
    "# â”€â”€ Collect video paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_ff_videos(root):\n",
    "    real = sorted(root.rglob('original*/*.mp4'))\n",
    "    if not real:\n",
    "        real = sorted(p for p in root.rglob('*.mp4') if 'original' in str(p).lower())\n",
    "    fakes = sorted((root / 'Deepfakes').glob('*.mp4')) if (root / 'Deepfakes').exists() else []\n",
    "    return real, fakes\n",
    "\n",
    "def get_celeb_videos(root):\n",
    "    real = (sorted((root / 'Celeb-real').glob('*.mp4')) +\n",
    "            sorted((root / 'YouTube-real').glob('*.mp4')))\n",
    "    fake = sorted((root / 'Celeb-synthesis').glob('*.mp4'))\n",
    "    return real, fake\n",
    "\n",
    "FF_REAL, FF_FAKE   = get_ff_videos(FF_ROOT)\n",
    "CDF_REAL, CDF_FAKE = get_celeb_videos(CELEB_ROOT)\n",
    "\n",
    "print(f'FF++ real: {len(FF_REAL)}, fake(Deepfakes): {len(FF_FAKE)}')\n",
    "print(f'Celeb-DF real: {len(CDF_REAL)}, fake: {len(CDF_FAKE)}')\n",
    "assert len(FF_REAL) > 0 and len(FF_FAKE) > 0,   'ERROR: No FF++ videos found'\n",
    "assert len(CDF_REAL) > 0 and len(CDF_FAKE) > 0, 'ERROR: No Celeb-DF videos found'\n",
    "\n",
    "# â”€â”€ Splits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "rng = random.Random(SEED)\n",
    "\n",
    "def make_split(real_list, fake_list, n_real, n_fake, n_val_each=50):\n",
    "    real_s = rng.sample(real_list, min(n_real + n_val_each, len(real_list)))\n",
    "    fake_s = rng.sample(fake_list, min(n_fake + n_val_each, len(fake_list)))\n",
    "    train = ([(p, 0) for p in real_s[:n_real]] +\n",
    "             [(p, 1) for p in fake_s[:n_fake]])\n",
    "    val   = ([(p, 0) for p in real_s[n_real:n_real + n_val_each]] +\n",
    "             [(p, 1) for p in fake_s[n_fake:n_fake + n_val_each]])\n",
    "    rng.shuffle(train)\n",
    "    rng.shuffle(val)\n",
    "    return train, val\n",
    "\n",
    "TRAIN_DATA, VAL_DATA = make_split(\n",
    "    FF_REAL, FF_FAKE,\n",
    "    CFG['n_train_real'], CFG['n_train_fake'],\n",
    "    n_val_each=50\n",
    ")\n",
    "\n",
    "n_cdf = min(200, len(CDF_REAL), len(CDF_FAKE))\n",
    "CDF_TEST = ([(p, 0) for p in rng.sample(CDF_REAL, n_cdf)] +\n",
    "            [(p, 1) for p in rng.sample(CDF_FAKE, n_cdf)])\n",
    "\n",
    "print(f'\\nSplits:')\n",
    "print(f'  Train : {len(TRAIN_DATA)} videos')\n",
    "print(f'  Val   : {len(VAL_DATA)} videos')\n",
    "print(f'  CDF   : {len(CDF_TEST)} videos (cross-dataset test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-s2b",
   "metadata": {},
   "source": [
    "## Section 2b â€” Video Validation\n",
    "\n",
    "Probe the first N videos to catch corrupt files or mount issues **before** training begins.  \n",
    "This also confirms that frame seeking actually works on the Kaggle filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-validate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T00:59:01.893475Z",
     "iopub.status.busy": "2026-02-19T00:59:01.893175Z",
     "iopub.status.idle": "2026-02-19T00:59:02.994386Z",
     "shell.execute_reply": "2026-02-19T00:59:02.993383Z",
     "shell.execute_reply.started": "2026-02-19T00:59:01.893452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Video validation â€” runs quickly, catches problems before training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def probe_video(path, n_frames=CFG['n_frames']):\n",
    "    \"\"\"\n",
    "    Returns (ok: bool, frame_count: int, error_msg: str).\n",
    "    Tries a SEQUENTIAL read strategy: scan forward frame-by-frame.\n",
    "    This is more reliable than cap.set(POS_FRAMES) on network filesystems.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(str(path))\n",
    "        if not cap.isOpened():\n",
    "            return False, 0, 'cannot open'\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        # Try reading frame 0 to confirm file is accessible\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        if not ret or frame is None:\n",
    "            return False, total, 'read frame 0 failed'\n",
    "        return True, total, ''\n",
    "    except Exception as e:\n",
    "        return False, 0, str(e)\n",
    "\n",
    "print('Validating video files (sampling first 20 of each split)...')\n",
    "bad_videos = []\n",
    "all_to_check = TRAIN_DATA[:20] + VAL_DATA[:10] + CDF_TEST[:10]\n",
    "\n",
    "for path, label in tqdm(all_to_check, desc='Probing'):\n",
    "    ok, n, err = probe_video(path)\n",
    "    if not ok:\n",
    "        bad_videos.append((path, err))\n",
    "        print(f'  âŒ {path.name}: {err}')\n",
    "\n",
    "if bad_videos:\n",
    "    print(f'\\nâš ï¸  {len(bad_videos)} bad video(s) found. They will return black frames during training.')\n",
    "else:\n",
    "    print(f'\\nâœ… All {len(all_to_check)} sampled videos OK â€” dataset is readable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-s3",
   "metadata": {},
   "source": [
    "## Section 3 â€” Fast Parallel Frame Extraction & Dataset\n",
    "\n",
    "### Why GPU shows 0% here â€” and that's fine\n",
    "Frame extraction is a **CPU + disk I/O** task. OpenCV has no GPU decode path in this environment.\n",
    "The GPU will jump to 80-100% utilization in Section 5 once the training loop starts.\n",
    "\n",
    "### Speed problem & fix\n",
    "Single-threaded sequential scan at 3.8s/video Ã— 600 videos = **~38 min** â€” unacceptable.  \n",
    "Fix: **`ThreadPoolExecutor` with 8 threads** runs 8 videos simultaneously.  \n",
    "Threads are safe here (unlike `multiprocessing`) because:\n",
    "- `cv2` releases the GIL during decode â†’ real parallelism despite Python's GIL\n",
    "- No `fork()` = no cv2 deadlock\n",
    "- Kaggle T4 nodes have 2 CPUs with hyperthreading â†’ 8 threads is the sweet spot\n",
    "\n",
    "### Seek strategy\n",
    "Instead of a full sequential scan from frame 0, we do **one `cap.set()` seek to the first target frame** then read linearly from there. This skips buffering the first portion of the file while staying reliable on the network FS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-dataset",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T00:59:16.590843Z",
     "iopub.status.busy": "2026-02-19T00:59:16.590179Z",
     "iopub.status.idle": "2026-02-19T01:22:32.185637Z",
     "shell.execute_reply": "2026-02-19T01:22:32.184728Z",
     "shell.execute_reply.started": "2026-02-19T00:59:16.590802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Frame extraction helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Strategy: use cap.set(POS_FRAMES) for the FIRST frame only (keyframe seek),\n",
    "# then read subsequent target frames sequentially from that point.\n",
    "# This avoids full-file buffering while still being much faster than pure\n",
    "# sequential scan. Falls back to a full sequential scan if seek fails.\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "N_EXTRACT_WORKERS = 8   # Thread-based parallelism â€” safe with cv2 (no fork)\n",
    "\n",
    "def _postprocess_frame(frame_bgr, img_size):\n",
    "    \"\"\"BGR numpy â†’ center-cropped RGB resized uint8.\"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    h, w = frame_rgb.shape[:2]\n",
    "    frame_rgb = frame_rgb[int(h * 0.05):int(h * 0.95),\n",
    "                          int(w * 0.10):int(w * 0.90)]\n",
    "    return cv2.resize(frame_rgb, (img_size, img_size))\n",
    "\n",
    "\n",
    "def extract_frames_from_video(video_path, n_frames, img_size):\n",
    "    \"\"\"\n",
    "    Fast frame extraction using keyframe-anchored seek.\n",
    "\n",
    "    Algorithm:\n",
    "      1. Open video, read total frame count.\n",
    "      2. Compute N evenly-spaced target indices.\n",
    "      3. Seek to the FIRST target (one seek per video max).\n",
    "      4. Read frames sequentially from that anchor point,\n",
    "         collecting each target as we pass it.\n",
    "      5. On any failure, fall back to reading frame 0 Ã— n_frames.\n",
    "\n",
    "    This is ~5-10x faster than the pure sequential scan on Kaggle's\n",
    "    network FS because we skip the first portion of the video in one\n",
    "    seek call and then read linearly from there.\n",
    "    \"\"\"\n",
    "    black = np.zeros((img_size, img_size, 3), dtype=np.uint8)\n",
    "\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            cap.release()\n",
    "            return [black.copy() for _ in range(n_frames)]\n",
    "\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total < 1:\n",
    "            # Unknown length: just read up to 300 frames to find content\n",
    "            total = 300\n",
    "\n",
    "        targets = sorted(set(np.linspace(0, total - 1, n_frames, dtype=int).tolist()))\n",
    "        collected = {}\n",
    "\n",
    "        # Seek to just before the first target frame\n",
    "        first_target = targets[0]\n",
    "        if first_target > 0:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, first_target)\n",
    "            actual_pos = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "            # If seek overshot, reset to 0 and scan from start\n",
    "            if actual_pos > first_target:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "                current = 0\n",
    "            else:\n",
    "                current = actual_pos\n",
    "        else:\n",
    "            current = 0\n",
    "\n",
    "        target_set = set(targets)\n",
    "        last_target = targets[-1]\n",
    "\n",
    "        while current <= last_target + 5:  # +5 slack for seek imprecision\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if current in target_set:\n",
    "                collected[current] = _postprocess_frame(frame, img_size)\n",
    "                if len(collected) == n_frames:\n",
    "                    break\n",
    "            current += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # If seek was imprecise, some targets may be off-by-1; try nearest\n",
    "        frames = []\n",
    "        for t in targets:\n",
    "            if t in collected:\n",
    "                frames.append(collected[t])\n",
    "            else:\n",
    "                # Try Â±2 neighbours\n",
    "                found = None\n",
    "                for delta in [1, -1, 2, -2]:\n",
    "                    if (t + delta) in collected:\n",
    "                        found = collected[t + delta]\n",
    "                        break\n",
    "                frames.append(found if found is not None else black.copy())\n",
    "\n",
    "        return frames[:n_frames]\n",
    "\n",
    "    except Exception:\n",
    "        return [black.copy() for _ in range(n_frames)]\n",
    "\n",
    "\n",
    "def _extract_one(args):\n",
    "    \"\"\"Worker function for ThreadPoolExecutor.\"\"\"\n",
    "    path, label, n_frames, img_size = args\n",
    "    frames = extract_frames_from_video(str(path), n_frames, img_size)\n",
    "    return [(f, label) for f in frames]\n",
    "\n",
    "\n",
    "# â”€â”€ Transforms â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomGrayscale(p=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "class DeepfakeFrameDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pre-extracts all frames at __init__ time using a thread pool,\n",
    "    then caches them in RAM as uint8 numpy arrays.\n",
    "    __getitem__ only touches RAM â€” zero filesystem I/O during training.\n",
    "\n",
    "    Why threads and not processes?\n",
    "    - cv2 + fork (multiprocessing) = deadlock on Kaggle\n",
    "    - Threads share the GIL but cv2 decode releases it â†’ real parallelism\n",
    "    - ThreadPoolExecutor is safe, no zombie processes, clean shutdown\n",
    "    \"\"\"\n",
    "    def __init__(self, video_label_pairs: list, transform,\n",
    "                 n_frames: int = CFG['n_frames'],\n",
    "                 img_size: int = CFG['img_size'],\n",
    "                 augment: bool = True,\n",
    "                 desc: str = 'Extracting',\n",
    "                 n_workers: int = N_EXTRACT_WORKERS):\n",
    "        self.transform = transform\n",
    "        self.augment   = augment\n",
    "        self.items     = []  # list of (frame_np_uint8, label_int)\n",
    "\n",
    "        n_total = len(video_label_pairs) * n_frames\n",
    "        print(f'{desc}: {len(video_label_pairs)} videos Ã— {n_frames} frames '\n",
    "              f'= {n_total} items  [{n_workers} threads]')\n",
    "\n",
    "        args_list = [(p, lbl, n_frames, img_size) for p, lbl in video_label_pairs]\n",
    "\n",
    "        # Preserve original order: submit all, collect in submission order\n",
    "        results_ordered = [None] * len(args_list)\n",
    "        with ThreadPoolExecutor(max_workers=n_workers) as pool:\n",
    "            future_to_idx = {pool.submit(_extract_one, a): i\n",
    "                             for i, a in enumerate(args_list)}\n",
    "            pbar = tqdm(as_completed(future_to_idx),\n",
    "                        total=len(args_list), desc=desc, leave=True)\n",
    "            for future in pbar:\n",
    "                idx = future_to_idx[future]\n",
    "                try:\n",
    "                    results_ordered[idx] = future.result()\n",
    "                except Exception as e:\n",
    "                    black = np.zeros((img_size, img_size, 3), dtype=np.uint8)\n",
    "                    lbl   = video_label_pairs[idx][1]\n",
    "                    results_ordered[idx] = [(black, lbl)] * n_frames\n",
    "\n",
    "        for video_frames in results_ordered:\n",
    "            self.items.extend(video_frames)\n",
    "\n",
    "        print(f'  â†’ {len(self.items)} frames cached in RAM  '\n",
    "              f'({len(self.items) * img_size * img_size * 3 / 1e6:.0f} MB)')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame, label = self.items[idx]\n",
    "        return self.transform(frame), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# â”€â”€ Time the extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('Building datasets (parallel frame extraction)...')\n",
    "print(f'Using {N_EXTRACT_WORKERS} threads â€” GPU stays idle here, that is normal.')\n",
    "print('â”€' * 65)\n",
    "\n",
    "t0 = time.time()\n",
    "train_ds = DeepfakeFrameDataset(TRAIN_DATA, train_transform, augment=True,  desc='Train')\n",
    "print(f'  Train done: {(time.time()-t0):.1f}s')\n",
    "\n",
    "t1 = time.time()\n",
    "val_ds   = DeepfakeFrameDataset(VAL_DATA,   val_transform,   augment=False, desc='Val')\n",
    "print(f'  Val done:   {(time.time()-t1):.1f}s')\n",
    "\n",
    "t2 = time.time()\n",
    "cdf_ds   = DeepfakeFrameDataset(CDF_TEST,   val_transform,   augment=False, desc='CDF')\n",
    "print(f'  CDF done:   {(time.time()-t2):.1f}s')\n",
    "\n",
    "print(f'\\nTotal extraction time: {(time.time()-t0)/60:.1f} min  '\n",
    "      f'(was ~38 min sequential, now ~3-5 min parallel)')\n",
    "\n",
    "# â”€â”€ DataLoaders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG['batch_size'],\n",
    "                          shuffle=True,  num_workers=0, pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=CFG['batch_size'],\n",
    "                          shuffle=False, num_workers=0, pin_memory=False)\n",
    "cdf_loader   = DataLoader(cdf_ds,   batch_size=CFG['batch_size'],\n",
    "                          shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(f'\\nDataLoader summary:')\n",
    "print(f'  Train : {len(train_ds):6d} frames,  {len(train_loader):4d} batches')\n",
    "print(f'  Val   : {len(val_ds):6d} frames,  {len(val_loader):4d} batches')\n",
    "print(f'  CDF   : {len(cdf_ds):6d} frames,  {len(cdf_loader):4d} batches')\n",
    "\n",
    "# â”€â”€ Sanity check â€” one batch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('\\nSanity check: loading one batch from RAM...')\n",
    "x_b, y_b = next(iter(train_loader))\n",
    "print(f'  x shape : {x_b.shape}, dtype: {x_b.dtype}')\n",
    "print(f'  y shape : {y_b.shape}, label balance: {y_b.sum().item()}/{len(y_b)}')\n",
    "print('âœ… DataLoader OK â€” GPU will activate in Section 5 during model forward pass.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-s4",
   "metadata": {},
   "source": [
    "## Section 4 â€” Model: EfficientNet-B0\n",
    "\n",
    "EfficientNet-B0 pretrained on ImageNet-1K.  \n",
    "We replace the classifier head with a binary deepfake detector.\n",
    "\n",
    "**Why B0 and not B4?**  \n",
    "B0 trains in ~60 min on T4. B4 would take 3-4 hours and we just need  \n",
    "a baseline number â€” not maximum performance. We can upgrade to B4 on RunPod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-model",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T01:22:43.090252Z",
     "iopub.status.busy": "2026-02-19T01:22:43.089590Z",
     "iopub.status.idle": "2026-02-19T01:22:45.711021Z",
     "shell.execute_reply": "2026-02-19T01:22:45.710191Z",
     "shell.execute_reply.started": "2026-02-19T01:22:43.090219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeepfakeDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet-B0 pretrained on ImageNet + custom classification head.\n",
    "    The backbone provides semantic face features that survive compression.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout: float = CFG['dropout']):\n",
    "        super().__init__()\n",
    "        self.backbone = models.efficientnet_b0(\n",
    "            weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "        )\n",
    "        in_features = self.backbone.classifier[1].in_features  # 1280\n",
    "\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout * 0.5),\n",
    "            nn.Linear(256, 2),  # real=0, fake=1\n",
    "        )\n",
    "\n",
    "        # Track param groups for differential LRs\n",
    "        self.backbone_params = list(self.backbone.features.parameters())\n",
    "        self.head_params     = list(self.backbone.classifier.parameters())\n",
    "\n",
    "        print(f'Backbone params : {sum(p.numel() for p in self.backbone_params):,}')\n",
    "        print(f'Head params     : {sum(p.numel() for p in self.head_params):,}')\n",
    "        print(f'Total params    : {sum(p.numel() for p in self.parameters()):,}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def get_param_groups(self, base_lr: float):\n",
    "        \"\"\"Differential LRs: backbone 10Ã— lower than head.\"\"\"\n",
    "        return [\n",
    "            {'params': self.backbone_params, 'lr': base_lr / 10},  # 1e-5\n",
    "            {'params': self.head_params,     'lr': base_lr},        # 1e-4\n",
    "        ]\n",
    "\n",
    "\n",
    "model = DeepfakeDetector().to(DEVICE)\n",
    "print(f'\\nModel on: {DEVICE}')\n",
    "\n",
    "# Verify forward pass\n",
    "with torch.no_grad():\n",
    "    _out = model(torch.randn(2, 3, 224, 224).to(DEVICE))\n",
    "    print(f'Forward pass: (2,3,224,224) â†’ {_out.shape}  âœ…')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-s5",
   "metadata": {},
   "source": [
    "## Section 5 â€” Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-optim",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T01:22:51.682392Z",
     "iopub.status.busy": "2026-02-19T01:22:51.681796Z",
     "iopub.status.idle": "2026-02-19T01:22:51.689918Z",
     "shell.execute_reply": "2026-02-19T01:22:51.689144Z",
     "shell.execute_reply.started": "2026-02-19T01:22:51.682363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Loss, optimizer, scheduler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=CFG['label_smoothing'])\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.get_param_groups(CFG['lr']),\n",
    "    weight_decay=CFG['weight_decay']\n",
    ")\n",
    "\n",
    "def get_lr_lambda(epoch):\n",
    "    if epoch < CFG['warmup_epochs']:\n",
    "        return (epoch + 1) / CFG['warmup_epochs']\n",
    "    progress = (epoch - CFG['warmup_epochs']) / max(1, CFG['epochs'] - CFG['warmup_epochs'])\n",
    "    return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr_lambda)\n",
    "\n",
    "print('Optimizer : AdamW with differential LRs')\n",
    "print(f'  Backbone LR : {CFG[\"lr\"]/10:.2e}')\n",
    "print(f'  Head LR     : {CFG[\"lr\"]:.2e}')\n",
    "print(f'Scheduler : Linear warmup ({CFG[\"warmup_epochs\"]} epochs) + Cosine decay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-train-fns",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T01:22:54.772957Z",
     "iopub.status.busy": "2026-02-19T01:22:54.772635Z",
     "iopub.status.idle": "2026-02-19T01:22:54.785256Z",
     "shell.execute_reply": "2026-02-19T01:22:54.784232Z",
     "shell.execute_reply.started": "2026-02-19T01:22:54.772930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Evaluation function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def evaluate(model, loader, device, desc='Eval'):\n",
    "    model.eval()\n",
    "    all_labels, all_probs, all_preds = [], [], []\n",
    "    total_loss = 0.0\n",
    "    n_batches  = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=desc, leave=False, dynamic_ncols=True)\n",
    "        for x, y in pbar:\n",
    "            x, y   = x.to(device), y.to(device)\n",
    "            logits  = model(x)\n",
    "            loss    = criterion(logits, y)\n",
    "            probs   = F.softmax(logits, dim=1)[:, 1]\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches  += 1\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "\n",
    "            pbar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "\n",
    "    labels = np.array(all_labels)\n",
    "    probs  = np.array(all_probs)\n",
    "    preds  = np.array(all_preds)\n",
    "\n",
    "    auc  = roc_auc_score(labels, probs) if len(np.unique(labels)) > 1 else 0.5\n",
    "    acc  = (preds == labels).mean()\n",
    "    loss = total_loss / max(n_batches, 1)\n",
    "    return {'auc': auc, 'acc': acc, 'loss': loss, 'labels': labels, 'probs': probs}\n",
    "\n",
    "\n",
    "# â”€â”€ Training function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct    = 0\n",
    "    total      = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc='Train', leave=False, dynamic_ncols=True)\n",
    "    for batch_idx, (x, y) in enumerate(pbar):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss   = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct    += (logits.argmax(1) == y).sum().item()\n",
    "        total      += y.size(0)\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            loss=f'{loss.item():.4f}',\n",
    "            acc=f'{correct/total:.3f}',\n",
    "            batch=f'{batch_idx+1}/{len(loader)}'\n",
    "        )\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "print('âœ… Training functions ready â€” tqdm progress bars will show batch-level progress.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-train-loop",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T01:22:59.994508Z",
     "iopub.status.busy": "2026-02-19T01:22:59.994207Z",
     "iopub.status.idle": "2026-02-19T01:33:07.132233Z",
     "shell.execute_reply": "2026-02-19T01:33:07.131389Z",
     "shell.execute_reply.started": "2026-02-19T01:22:59.994484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Main training loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [],\n",
    "           'val_auc': [], 'val_acc': [], 'lr': []}\n",
    "\n",
    "best_val_auc = 0.0\n",
    "best_epoch   = 0\n",
    "start_time   = time.time()\n",
    "\n",
    "HDR = f\"{'Ep':>3} {'TrLoss':>8} {'TrAcc':>7} {'VaLoss':>8} {'VaAUC':>7} {'VaAcc':>7} {'HeadLR':>9} {'Min':>5}\"\n",
    "print('=' * len(HDR))\n",
    "print(HDR)\n",
    "print('=' * len(HDR))\n",
    "sys.stdout.flush()\n",
    "\n",
    "epoch_pbar = tqdm(range(CFG['epochs']), desc='Epochs', position=0)\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    t0 = time.time()\n",
    "\n",
    "    tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, DEVICE)\n",
    "    val_metrics     = evaluate(model, val_loader, DEVICE, desc=f'Val E{epoch+1}')\n",
    "\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[1]['lr']\n",
    "\n",
    "    history['train_loss'].append(tr_loss)\n",
    "    history['train_acc'].append(tr_acc)\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_auc'].append(val_metrics['auc'])\n",
    "    history['val_acc'].append(val_metrics['acc'])\n",
    "    history['lr'].append(current_lr)\n",
    "\n",
    "    elapsed_min = (time.time() - t0) / 60\n",
    "    is_best = val_metrics['auc'] > best_val_auc\n",
    "    flag    = ' â†' if is_best else ''\n",
    "\n",
    "    row = (f\"{epoch+1:>3} {tr_loss:>8.4f} {tr_acc:>7.3f} \"\n",
    "           f\"{val_metrics['loss']:>8.4f} {val_metrics['auc']:>7.4f} \"\n",
    "           f\"{val_metrics['acc']:>7.3f} {current_lr:>9.2e} \"\n",
    "           f\"{elapsed_min:>4.1f}m{flag}\")\n",
    "    print(row)\n",
    "    sys.stdout.flush()  # Force immediate output in Kaggle\n",
    "\n",
    "    epoch_pbar.set_postfix(val_auc=f\"{val_metrics['auc']:.4f}\", best=f\"{best_val_auc:.4f}\")\n",
    "\n",
    "    if is_best:\n",
    "        best_val_auc = val_metrics['auc']\n",
    "        best_epoch   = epoch + 1\n",
    "        torch.save({\n",
    "            'epoch':       epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'val_auc':     best_val_auc,\n",
    "            'cfg':         CFG,\n",
    "        }, CKPT_DIR / 'best.pth')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('=' * len(HDR))\n",
    "print(f'\\nBest Val AUC : {best_val_auc:.4f} at epoch {best_epoch}')\n",
    "print(f'Total time   : {total_time/60:.1f} min')\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-s6",
   "metadata": {},
   "source": [
    "## Section 6 â€” Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-eval",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T01:34:53.776846Z",
     "iopub.status.busy": "2026-02-19T01:34:53.776206Z",
     "iopub.status.idle": "2026-02-19T01:34:58.294006Z",
     "shell.execute_reply": "2026-02-19T01:34:58.293106Z",
     "shell.execute_reply.started": "2026-02-19T01:34:53.776815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Load best checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# weights_only=False required because PyTorch 2.6 changed the default to True,\n",
    "# and older checkpoints saved with numpy scalars fail the safe unpickler.\n",
    "# Safe here: we wrote this checkpoint ourselves in the cell above.\n",
    "ckpt = torch.load(CKPT_DIR / 'best.pth', map_location=DEVICE, weights_only=False)\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "print(f\"Loaded best model from epoch {ckpt['epoch']+1}  (val AUC={ckpt['val_auc']:.4f})\")\n",
    "\n",
    "# â”€â”€ Evaluate on FF++ val â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('\\nEvaluating on FF++ validation set...')\n",
    "ff_metrics = evaluate(model, val_loader, DEVICE, desc='FF++ Val')\n",
    "print(f\"  AUC={ff_metrics['auc']:.4f}  Acc={ff_metrics['acc']:.4f}\")\n",
    "\n",
    "# â”€â”€ Evaluate on Celeb-DF (cross-dataset) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('\\nEvaluating on Celeb-DF (cross-dataset test)...')\n",
    "cdf_metrics = evaluate(model, cdf_loader, DEVICE, desc='Celeb-DF')\n",
    "print(f\"  AUC={cdf_metrics['auc']:.4f}  Acc={cdf_metrics['acc']:.4f}\")\n",
    "\n",
    "gap = ff_metrics['auc'] - cdf_metrics['auc']\n",
    "print('\\n' + '=' * 52)\n",
    "print('BASELINE RESULTS SUMMARY')\n",
    "print('=' * 52)\n",
    "print(f\"  FF++ Val  AUC : {ff_metrics['auc']:.4f}\")\n",
    "print(f\"  Celeb-DF  AUC : {cdf_metrics['auc']:.4f}  â† cross-dataset generalization\")\n",
    "print(f\"  Gap           : {gap:.4f}\")\n",
    "print('=' * 52)\n",
    "\n",
    "if cdf_metrics['auc'] >= 0.75:\n",
    "    print('ğŸŸ¢ STRONG â€” Semantic signal confirmed. V8.0 temporal module will push higher.')\n",
    "elif cdf_metrics['auc'] >= 0.60:\n",
    "    print('ğŸŸ¡ MODERATE â€” Signal exists but weak. Train on all FF++ methods next.')\n",
    "else:\n",
    "    print('ğŸ”´ WEAK â€” Check training setup. Consider longer training or B4 backbone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plots-curves",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T01:35:07.767536Z",
     "iopub.status.busy": "2026-02-19T01:35:07.767205Z",
     "iopub.status.idle": "2026-02-19T01:35:09.343904Z",
     "shell.execute_reply": "2026-02-19T01:35:09.343100Z",
     "shell.execute_reply.started": "2026-02-19T01:35:07.767509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Training curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('EfficientNet-B0 Baseline â€” Training Curves', fontsize=14, fontweight='bold')\n",
    "\n",
    "x = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "axes[0].plot(x, history['train_loss'], label='Train', color='#3498db', linewidth=2)\n",
    "axes[0].plot(x, history['val_loss'],   label='Val',   color='#e74c3c', linewidth=2)\n",
    "axes[0].set_title('Loss'); axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(x, history['val_auc'], color='#2ecc71', linewidth=2.5, label='Val AUC')\n",
    "axes[1].axhline(best_val_auc,        color='gray',    linestyle='--', alpha=0.7,\n",
    "                label=f'Best={best_val_auc:.4f}')\n",
    "axes[1].axhline(cdf_metrics['auc'],  color='#e74c3c', linestyle='--', alpha=0.7,\n",
    "                label=f'CDF={cdf_metrics[\"auc\"]:.4f}')\n",
    "axes[1].set_title('Val AUC'); axes[1].set_xlabel('Epoch')\n",
    "axes[1].legend(); axes[1].grid(True, alpha=0.3); axes[1].set_ylim(0.4, 1.0)\n",
    "\n",
    "axes[2].plot(x, history['lr'], color='#9b59b6', linewidth=2)\n",
    "axes[2].set_title('Head Learning Rate'); axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_yscale('log'); axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Saved training_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plots-roc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T01:35:13.760640Z",
     "iopub.status.busy": "2026-02-19T01:35:13.760309Z",
     "iopub.status.idle": "2026-02-19T01:35:14.631842Z",
     "shell.execute_reply": "2026-02-19T01:35:14.631010Z",
     "shell.execute_reply.started": "2026-02-19T01:35:13.760612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ ROC curves: FF++ vs Celeb-DF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('ROC Curves â€” Baseline EfficientNet-B0\\n'\n",
    "             'Left: FF++ Val (in-distribution) | Right: Celeb-DF (cross-dataset)',\n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for ax, metrics, title, color in [\n",
    "    (axes[0], ff_metrics,  f\"FF++ Val\\nAUC={ff_metrics['auc']:.4f}\",  '#3498db'),\n",
    "    (axes[1], cdf_metrics, f\"Celeb-DF (cross-dataset)\\nAUC={cdf_metrics['auc']:.4f}\", '#e74c3c'),\n",
    "]:\n",
    "    fpr, tpr, _ = roc_curve(metrics['labels'], metrics['probs'])\n",
    "    ax.plot(fpr, tpr, color=color, linewidth=2.5, label=f\"AUC={metrics['auc']:.4f}\")\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.4, label='Random (0.50)')\n",
    "    ax.fill_between(fpr, tpr, alpha=0.1, color=color)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Saved roc_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-plots-dist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T01:35:18.694729Z",
     "iopub.status.busy": "2026-02-19T01:35:18.694420Z",
     "iopub.status.idle": "2026-02-19T01:35:19.883167Z",
     "shell.execute_reply": "2026-02-19T01:35:19.882467Z",
     "shell.execute_reply.started": "2026-02-19T01:35:18.694706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€ Score distributions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Prediction Score Distributions\\n'\n",
    "             'Well-separated = model is confident and correct',\n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "for ax, metrics, title in [\n",
    "    (axes[0], ff_metrics,  'FF++ Validation'),\n",
    "    (axes[1], cdf_metrics, 'Celeb-DF (Cross-Dataset)'),\n",
    "]:\n",
    "    real_scores = metrics['probs'][metrics['labels'] == 0]\n",
    "    fake_scores = metrics['probs'][metrics['labels'] == 1]\n",
    "    bins = np.linspace(0, 1, 40)\n",
    "    ax.hist(real_scores, bins=bins, alpha=0.6, color='#2ecc71',\n",
    "            label=f'Real (n={len(real_scores)})', density=True)\n",
    "    ax.hist(fake_scores, bins=bins, alpha=0.6, color='#e74c3c',\n",
    "            label=f'Fake (n={len(fake_scores)})', density=True)\n",
    "    ax.axvline(0.5, color='black', linestyle='--', linewidth=1.5, label='Threshold=0.5')\n",
    "    ax.set_xlabel('P(fake) score')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f\"{title}\\nAUC={metrics['auc']:.4f}, Acc={metrics['acc']:.3f}\",\n",
    "                 fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'score_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Saved score_distributions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-md-s7",
   "metadata": {},
   "source": [
    "## Section 7 â€” Save Results & V8.0 Roadmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-results",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T01:35:24.422363Z",
     "iopub.status.busy": "2026-02-19T01:35:24.422011Z",
     "iopub.status.idle": "2026-02-19T01:35:24.433483Z",
     "shell.execute_reply": "2026-02-19T01:35:24.432671Z",
     "shell.execute_reply.started": "2026-02-19T01:35:24.422338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    'model':              'EfficientNet-B0 ImageNet pretrained',\n",
    "    'train_on':           'FF++ Deepfakes only',\n",
    "    'best_epoch':          best_epoch,\n",
    "    'ff_val': {\n",
    "        'auc': round(ff_metrics['auc'],  4),\n",
    "        'acc': round(ff_metrics['acc'],  4),\n",
    "    },\n",
    "    'celeb_df': {\n",
    "        'auc': round(cdf_metrics['auc'], 4),\n",
    "        'acc': round(cdf_metrics['acc'], 4),\n",
    "    },\n",
    "    'generalization_gap': round(ff_metrics['auc'] - cdf_metrics['auc'], 4),\n",
    "    'training_minutes':   round(total_time / 60, 1),\n",
    "    'fixes_applied': [\n",
    "        'frame pre-extraction (no VideoCapture in DataLoader)',\n",
    "        'sequential read (no random seek on network FS)',\n",
    "        'tqdm progress bars (training cannot silently hang)',\n",
    "        'sys.stdout.flush() after every epoch row',\n",
    "        'video validation step before training',\n",
    "    ],\n",
    "}\n",
    "\n",
    "out_path = OUTPUT_DIR / 'baseline_results.json'\n",
    "with open(out_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print('=' * 62)\n",
    "print('BASELINE COMPLETE â€” V8.0 ROADMAP')\n",
    "print('=' * 62)\n",
    "print(f\"\\nFF++ Val AUC  : {results['ff_val']['auc']}\")\n",
    "print(f\"Celeb-DF AUC  : {results['celeb_df']['auc']}  â† number to beat with V8.0\")\n",
    "print(f\"Gap           : {results['generalization_gap']}\")\n",
    "print(f\"Training time : {results['training_minutes']} min\")\n",
    "\n",
    "print('\\nV8.0 Architecture on top of this baseline:')\n",
    "print('  Stage 1: EfficientNet-B0/B4 backbone (pretrained) â€” KEEP AS-IS')\n",
    "print('  Stage 2: Per-frame embeddings â†’ (B, T, 1280)')\n",
    "print('  Stage 3: Hydra Mixer / Mamba for temporal consistency')\n",
    "print('  Stage 4: Classification head')\n",
    "\n",
    "cdf_auc = results['celeb_df']['auc']\n",
    "print('\\nExpected V8.0 improvement from temporal module:')\n",
    "print(f'  Baseline : {cdf_auc:.4f}')\n",
    "print(f'  Target   : {min(cdf_auc + 0.08, 0.97):.4f}  (+5-10% from temporal consistency)')\n",
    "print(f'  SOTA     : 0.9629 (WMamba)')\n",
    "print(f'\\nResults saved â†’ {out_path}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 5454468,
     "datasetId": 3120670,
     "sourceId": 5380830,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10408999,
     "datasetId": 6248577,
     "sourceId": 10125851,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
