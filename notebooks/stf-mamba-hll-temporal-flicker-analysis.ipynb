{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66124f37-99a2-4020-ace4-f39e2a937e8d",
   "metadata": {},
   "source": [
    "# ðŸ”¬ STF-Mamba: HLL Temporal Flicker Analysis\n",
    "## Empirical Proof-of-Concept for the HLL Hypothesis\n",
    "\n",
    "**Goal:** Before building V8.0, we need empirical proof that the HLL (High-Temporal, Low-Spatial)\n",
    "wavelet sub-band actually separates real from fake videos across manipulation methods and datasets.\n",
    "\n",
    "**The Hypothesis:**  \n",
    "> Deepfake videos have elevated and patterned HLL energy compared to real videos,  \n",
    "> because face blending creates temporal inconsistencies at blend boundaries.\n",
    "\n",
    "**If the hypothesis holds:** HLL is a valid forensic signal â†’ proceed with V8.0.  \n",
    "**If it fails:** We need a different core signal before building anything.\n",
    "\n",
    "### Sections\n",
    "1. Environment Setup  \n",
    "2. Core Analysis Tools (DWT, Frame Diff)  \n",
    "3. Dataset Discovery  \n",
    "4. âœ… Baseline: Frame Difference (Sanity Check)  \n",
    "5. âœ… HLL Energy Analysis (Primary Hypothesis)  \n",
    "6. âœ… Temporal Trajectory Analysis  \n",
    "7. âœ… Spatial HLL Heatmaps  \n",
    "8. âœ… Cross-Dataset Validation (Celeb-DF)  \n",
    "9. âœ… SBI vs Real Deepfake Comparison  \n",
    "10. ðŸ“‹ Summary & V8.0 Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff60924-5125-48cb-9ea5-eda40a79c81c",
   "metadata": {},
   "source": [
    "## Section 1 â€” Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d9b17-304b-4b1a-90cb-26e4aa9c2de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:02:59.835623Z",
     "iopub.status.busy": "2026-02-18T22:02:59.834756Z",
     "iopub.status.idle": "2026-02-18T22:03:03.119871Z",
     "shell.execute_reply": "2026-02-18T22:03:03.119120Z",
     "shell.execute_reply.started": "2026-02-18T22:02:59.835571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install only what Kaggle doesn't have pre-installed\n",
    "import subprocess, sys\n",
    "\n",
    "# Only install packages not on Kaggle by default\n",
    "packages = ['scipy']  # numpy, cv2, matplotlib are pre-installed\n",
    "for pkg in packages:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', pkg], check=False)\n",
    "print(\"âœ… Dependencies ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e980e6-7f62-4d84-a896-0a5e4ce1649c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:03:03.121296Z",
     "iopub.status.busy": "2026-02-18T22:03:03.121032Z",
     "iopub.status.idle": "2026-02-18T22:03:03.128316Z",
     "shell.execute_reply": "2026-02-18T22:03:03.127450Z",
     "shell.execute_reply.started": "2026-02-18T22:03:03.121276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, json, random, warnings\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# â”€â”€ Reproducibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# â”€â”€ Output directory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "OUTPUT_DIR = Path('/kaggle/working/hll_analysis')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR = OUTPUT_DIR / 'plots'\n",
    "PLOTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ Outputs â†’ {OUTPUT_DIR}\")\n",
    "print(f\"ðŸ NumPy {np.__version__}, OpenCV {cv2.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f9683-b3a5-477b-bec3-9fa0b676d129",
   "metadata": {},
   "source": [
    "## Section 2 â€” Core Analysis Tools\n",
    "\n",
    "Pure NumPy implementations â€” no PyTorch needed.  \n",
    "Runs on Kaggle CPU or T4 GPU (same results).\n",
    "\n",
    "### The 3D-DWT Explained\n",
    "We decompose the video volume **(T, H, W)** using Symlet-2 wavelets along all three axes:\n",
    "- **H axis (temporal):** high-pass filter â†’ catches frame-to-frame changes\n",
    "- **L axes (spatial):** low-pass filter â†’ removes spatial detail, keeps smooth regions\n",
    "\n",
    "**HLL = High(T) Ã— Low(H) Ã— Low(W)** = pure temporal flicker with no spatial frequency content.\n",
    "This is exactly what face-blending artifacts look like: temporally inconsistent, spatially smooth regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97af5ec-4a5b-42ef-a440-1bd1bd083078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:03:07.624545Z",
     "iopub.status.busy": "2026-02-18T22:03:07.624244Z",
     "iopub.status.idle": "2026-02-18T22:03:07.663171Z",
     "shell.execute_reply": "2026-02-18T22:03:07.662305Z",
     "shell.execute_reply.started": "2026-02-18T22:03:07.624520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Symlet-2 (Daubechies-2) filter coefficients â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Same as wavelet_constants.py in STF-Mamba backbone\n",
    "SYM2_LO = np.array([-0.12940952255092145,  0.22414386804185735,\n",
    "                      0.83651630373746899,  0.48296291314469025])\n",
    "SYM2_HI = np.array([-0.48296291314469025,  0.83651630373746899,\n",
    "                    -0.22414386804185735, -0.12940952255092145])\n",
    "\n",
    "def _conv1d_strided(signal: np.ndarray, filt: np.ndarray, axis: int) -> np.ndarray:\n",
    "    \"\"\"Apply 1D filter along `axis` with stride=2 (decimation) and reflect padding.\"\"\"\n",
    "    n = signal.shape[axis]\n",
    "    pad = [(0, 0)] * signal.ndim\n",
    "    pad[axis] = (len(filt) - 1, len(filt) - 1)\n",
    "    padded = np.pad(signal, pad, mode='reflect')\n",
    "    \n",
    "    out_len = n // 2\n",
    "    out_shape = list(signal.shape)\n",
    "    out_shape[axis] = out_len\n",
    "    out = np.zeros(out_shape, dtype=np.float32)\n",
    "    \n",
    "    for i in range(out_len):\n",
    "        sl_in  = [slice(None)] * signal.ndim\n",
    "        sl_out = [slice(None)] * signal.ndim\n",
    "        sl_in[axis]  = slice(i * 2, i * 2 + len(filt))\n",
    "        sl_out[axis] = i\n",
    "        chunk = padded[tuple(sl_in)]\n",
    "        out[tuple(sl_out)] = np.tensordot(chunk, filt, axes=([axis], [0]))\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_hll(frames_rgb: np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute HLL sub-band from video frames.\n",
    "    \n",
    "    Args:\n",
    "        frames_rgb: (T, H, W, 3) uint8 â€” RGB video frames\n",
    "    Returns:\n",
    "        energy: scalar â€” mean squared HLL coefficient (the key metric)\n",
    "        hll:    (T//2, H//2, W//2) array â€” HLL sub-band map\n",
    "    \"\"\"\n",
    "    # Grayscale (T, H, W) in [0, 1]\n",
    "    gray = (0.299 * frames_rgb[..., 0].astype(np.float32) +\n",
    "            0.587 * frames_rgb[..., 1].astype(np.float32) +\n",
    "            0.114 * frames_rgb[..., 2].astype(np.float32)) / 255.0\n",
    "    \n",
    "    # HLL: High(T=axis0) â†’ Low(H=axis1) â†’ Low(W=axis2)\n",
    "    h   = _conv1d_strided(gray,  SYM2_HI, axis=0)   # temporal high-pass\n",
    "    hl  = _conv1d_strided(h,     SYM2_LO, axis=1)   # spatial low-pass (height)\n",
    "    hll = _conv1d_strided(hl,    SYM2_LO, axis=2)   # spatial low-pass (width)\n",
    "    \n",
    "    energy = float(np.mean(hll ** 2))\n",
    "    return energy, hll\n",
    "\n",
    "\n",
    "def compute_all_subbands(frames_rgb: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Compute all 8 DWT sub-bands for comparison.\"\"\"\n",
    "    gray = (0.299 * frames_rgb[..., 0].astype(np.float32) +\n",
    "            0.587 * frames_rgb[..., 1].astype(np.float32) +\n",
    "            0.114 * frames_rgb[..., 2].astype(np.float32)) / 255.0\n",
    "    \n",
    "    energies = {}\n",
    "    for t_filt, t_name in [(SYM2_LO, 'L'), (SYM2_HI, 'H')]:\n",
    "        t_out = _conv1d_strided(gray, t_filt, axis=0)\n",
    "        for h_filt, h_name in [(SYM2_LO, 'L'), (SYM2_HI, 'H')]:\n",
    "            h_out = _conv1d_strided(t_out, h_filt, axis=1)\n",
    "            for w_filt, w_name in [(SYM2_LO, 'L'), (SYM2_HI, 'H')]:\n",
    "                w_out = _conv1d_strided(h_out, w_filt, axis=2)\n",
    "                name = t_name + h_name + w_name\n",
    "                energies[name] = float(np.mean(w_out ** 2))\n",
    "    return energies\n",
    "\n",
    "\n",
    "def frame_diff_energy(frames_rgb: np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"Simple absolute frame difference â€” baseline temporal change measure.\"\"\"\n",
    "    gray = np.mean(frames_rgb.astype(np.float32), axis=-1) / 255.0\n",
    "    diffs = np.abs(np.diff(gray, axis=0))          # (T-1, H, W)\n",
    "    return float(np.mean(diffs)), diffs\n",
    "\n",
    "\n",
    "# â”€â”€â”€ Self-test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "_t = np.random.randint(0, 255, (32, 64, 64, 3), dtype=np.uint8)\n",
    "_e, _hll = compute_hll(_t)\n",
    "_de, _dd  = frame_diff_energy(_t)\n",
    "print(f\"âœ… compute_hll     â†’ energy={_e:.6f}, hll shape={_hll.shape}\")\n",
    "print(f\"âœ… frame_diff      â†’ energy={_de:.6f}, diff shape={_dd.shape}\")\n",
    "print(f\"âœ… all_subbands    â†’ {list(compute_all_subbands(_t).keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c380a19-8da9-4332-926e-14643c39c76e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:03:14.707263Z",
     "iopub.status.busy": "2026-02-18T22:03:14.706961Z",
     "iopub.status.idle": "2026-02-18T22:03:14.717376Z",
     "shell.execute_reply": "2026-02-18T22:03:14.716501Z",
     "shell.execute_reply.started": "2026-02-18T22:03:14.707239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Video utility: extract N evenly-spaced frames from a video â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def extract_frames(video_path: str,\n",
    "                   n_frames: int = 32,\n",
    "                   face_size: int = 224,\n",
    "                   use_face_crop: bool = False) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract n_frames evenly spaced from video.\n",
    "    \n",
    "    Args:\n",
    "        video_path:    path to .mp4 file\n",
    "        n_frames:      how many frames to extract (default 32 â€” model standard)\n",
    "        face_size:     resize to this square size\n",
    "        use_face_crop: if True, attempt center-crop to face region\n",
    "    Returns:\n",
    "        frames: (n_frames, face_size, face_size, 3) uint8 or None on failure\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "    \n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total < n_frames:\n",
    "        cap.release()\n",
    "        return None\n",
    "    \n",
    "    indices = np.linspace(0, total - 1, n_frames, dtype=int)\n",
    "    frames = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if use_face_crop:\n",
    "            h, w = frame.shape[:2]\n",
    "            margin = 0.1\n",
    "            y1 = int(h * margin)\n",
    "            y2 = int(h * (1 - margin))\n",
    "            x1 = int(w * 0.15)\n",
    "            x2 = int(w * 0.85)\n",
    "            frame = frame[y1:y2, x1:x2]\n",
    "        \n",
    "        frame = cv2.resize(frame, (face_size, face_size))\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if len(frames) < n_frames // 2:   # too many failed reads\n",
    "        return None\n",
    "    \n",
    "    # Pad with last frame if slightly short\n",
    "    while len(frames) < n_frames:\n",
    "        frames.append(frames[-1])\n",
    "    \n",
    "    return np.stack(frames[:n_frames], axis=0)   # (T, H, W, 3)\n",
    "\n",
    "\n",
    "# â”€â”€â”€ HLL trajectory: per-frame-pair HLL energy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def hll_trajectory(frames_rgb: np.ndarray, window: int = 2) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute HLL energy over sliding temporal windows.\n",
    "    Returns an energy value per time step, revealing dynamics.\n",
    "    \"\"\"\n",
    "    T = frames_rgb.shape[0]\n",
    "    energies = []\n",
    "    for start in range(0, T - window, 1):\n",
    "        chunk = frames_rgb[start : start + window + 1]  # (window+1, H, W, 3)\n",
    "        e, _ = compute_hll(chunk)\n",
    "        energies.append(e)\n",
    "    return np.array(energies)\n",
    "\n",
    "print(\"âœ… Video utilities ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b3e75-02fe-40f4-b706-a0a0b7068778",
   "metadata": {},
   "source": [
    "## Section 3 â€” Dataset Discovery\n",
    "\n",
    "Discovering available videos on Kaggle.  \n",
    "**FF++ expected:** `/kaggle/input/faceforensics/` (6000 fake + 1000 real)  \n",
    "**Celeb-DF expected:** `/kaggle/input/celeb-df-v2/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417bb44-a395-4075-9dc2-93d986a4ced7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:03:20.361177Z",
     "iopub.status.busy": "2026-02-18T22:03:20.360872Z",
     "iopub.status.idle": "2026-02-18T22:05:59.808468Z",
     "shell.execute_reply": "2026-02-18T22:05:59.807783Z",
     "shell.execute_reply.started": "2026-02-18T22:03:20.361152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Auto-discover dataset paths (handles Kaggle /datasets/ nesting) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "KAGGLE_INPUT = Path('/kaggle/input')\n",
    "\n",
    "def find_mp4s_quick(root: Path, max_depth: int = 6) -> list:\n",
    "    \"\"\"Recursively find .mp4 files up to max_depth.\"\"\"\n",
    "    results = []\n",
    "    try:\n",
    "        for p in root.rglob('*.mp4'):\n",
    "            results.append(p)\n",
    "            if len(results) >= 5:   # early-exit for scanning\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "    return results\n",
    "\n",
    "print(\"ðŸ“‚ Scanning /kaggle/input for all directories ...\", flush=True)\n",
    "for d in sorted(KAGGLE_INPUT.rglob('*')):\n",
    "    if d.is_dir() and d.stat().st_size == 0:  # skip empty\n",
    "        pass\n",
    "    # Only print directories that directly contain files\n",
    "    try:\n",
    "        files = list(d.glob('*.mp4'))[:1]\n",
    "        if files:\n",
    "            print(f\"  {str(d)}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# â”€â”€â”€ FF++ : handle both flat and nested structures â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FF_METHODS = ['Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures',\n",
    "              'FaceShifter', 'DeepFakeDetection']\n",
    "\n",
    "# Known path fragments â€” we search for any dir containing these method folders\n",
    "def locate_ff_root(base: Path) -> Path | None:\n",
    "    \"\"\"Walk the tree and find the directory that contains FF++ method folders.\"\"\"\n",
    "    # Check explicit known path first (from user-provided paths)\n",
    "    known = base / 'datasets' / 'xdxd003' / 'ff-c23' / 'FaceForensics++_C23'\n",
    "    if known.exists():\n",
    "        return known\n",
    "    # Generic: any directory that contains at least 2 known FF++ method subfolders\n",
    "    for d in sorted(base.rglob('*')):\n",
    "        if d.is_dir():\n",
    "            hits = sum(1 for m in FF_METHODS if (d / m).exists())\n",
    "            if hits >= 2:\n",
    "                return d\n",
    "    return None\n",
    "\n",
    "FF_ROOT = locate_ff_root(KAGGLE_INPUT)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FF++ root: {FF_ROOT}\")\n",
    "\n",
    "FF_VIDEOS = {}\n",
    "if FF_ROOT is not None:\n",
    "    # Real videos\n",
    "    real_paths = (list(FF_ROOT.rglob('original*/*.mp4')) +\n",
    "                  list(FF_ROOT.rglob('original/*.mp4')))\n",
    "    if not real_paths:\n",
    "        real_paths = [p for p in FF_ROOT.rglob('*.mp4')\n",
    "                      if 'original' in str(p).lower()]\n",
    "    FF_VIDEOS['real'] = sorted(real_paths)\n",
    "\n",
    "    # Fake videos by method\n",
    "    for method in FF_METHODS:\n",
    "        paths = list((FF_ROOT / method).glob('*.mp4')) if (FF_ROOT / method).exists() else []\n",
    "        if not paths:\n",
    "            paths = list(FF_ROOT.rglob(f'{method}/*.mp4'))\n",
    "        if paths:\n",
    "            FF_VIDEOS[method] = sorted(paths)\n",
    "\n",
    "    for k, v in FF_VIDEOS.items():\n",
    "        print(f\"  FF++/{k:25s}: {len(v):5d} videos\")\n",
    "else:\n",
    "    print(\"âš ï¸  FF++ video dataset not found.\")\n",
    "\n",
    "# â”€â”€â”€ Also check for the extracted-frames dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FRAMES_ROOT = None\n",
    "known_frames = KAGGLE_INPUT / 'datasets' / 'adham7elmy' / 'faceforencispp-extracted-frames'\n",
    "if known_frames.exists():\n",
    "    FRAMES_ROOT = known_frames\n",
    "    print(f\"\\nâœ… Found extracted-frames dataset: {FRAMES_ROOT}\")\n",
    "    fake_methods_frames = [d.name for d in (FRAMES_ROOT / 'fake').iterdir()\n",
    "                           if d.is_dir()] if (FRAMES_ROOT / 'fake').exists() else []\n",
    "    print(f\"   Fake methods (frames): {fake_methods_frames}\")\n",
    "else:\n",
    "    for d in KAGGLE_INPUT.rglob('*'):\n",
    "        if d.is_dir() and (d / 'fake').exists() and (d / 'real').exists():\n",
    "            FRAMES_ROOT = d\n",
    "            print(f\"\\nâœ… Found extracted-frames dataset: {FRAMES_ROOT}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a5552-eb65-4261-85e4-f4969386ac04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:06:17.410825Z",
     "iopub.status.busy": "2026-02-18T22:06:17.410071Z",
     "iopub.status.idle": "2026-02-18T22:06:17.505248Z",
     "shell.execute_reply": "2026-02-18T22:06:17.504414Z",
     "shell.execute_reply.started": "2026-02-18T22:06:17.410794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Celeb-DF paths (handles /datasets/ nesting) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def locate_celeb_root(base: Path) -> Path | None:\n",
    "    \"\"\"Find Celeb-DF root â€” looks for Celeb-real and Celeb-synthesis subfolders.\"\"\"\n",
    "    known = base / 'datasets' / 'reubensuju' / 'celeb-df-v2'\n",
    "    if known.exists():\n",
    "        return known\n",
    "    for d in sorted(base.rglob('*')):\n",
    "        if d.is_dir():\n",
    "            if (d / 'Celeb-real').exists() and (d / 'Celeb-synthesis').exists():\n",
    "                return d\n",
    "    return None\n",
    "\n",
    "CELEB_ROOT = locate_celeb_root(KAGGLE_INPUT)\n",
    "print(f\"Celeb-DF root: {CELEB_ROOT}\")\n",
    "\n",
    "CELEB_VIDEOS = {}\n",
    "if CELEB_ROOT is not None:\n",
    "    real_paths  = (list((CELEB_ROOT / 'Celeb-real').glob('*.mp4')) +\n",
    "                   list((CELEB_ROOT / 'YouTube-real').glob('*.mp4')))\n",
    "    fake_paths  = list((CELEB_ROOT / 'Celeb-synthesis').glob('*.mp4'))\n",
    "    CELEB_VIDEOS['real'] = sorted(real_paths)\n",
    "    CELEB_VIDEOS['fake'] = sorted(fake_paths)\n",
    "    for k, v in CELEB_VIDEOS.items():\n",
    "        print(f\"  Celeb-DF/{k:10s}: {len(v):5d} videos\")\n",
    "    \n",
    "    # Check test split file\n",
    "    test_list = CELEB_ROOT / 'List_of_testing_videos.txt'\n",
    "    if test_list.exists():\n",
    "        with open(test_list) as f:\n",
    "            test_names = set(line.strip().split()[-1].split('/')[-1].replace('.mp4','')\n",
    "                           for line in f if line.strip())\n",
    "        print(f\"  Test split file found: {len(test_names)} test videos\")\n",
    "        # Keep test videos separate\n",
    "        CELEB_TEST_REAL = [p for p in CELEB_VIDEOS['real'] if p.stem in test_names]\n",
    "        CELEB_TEST_FAKE = [p for p in CELEB_VIDEOS['fake'] if p.stem in test_names]\n",
    "        CELEB_TRAIN_REAL = [p for p in CELEB_VIDEOS['real'] if p.stem not in test_names]\n",
    "        CELEB_TRAIN_FAKE = [p for p in CELEB_VIDEOS['fake'] if p.stem not in test_names]\n",
    "        print(f\"  Test:  {len(CELEB_TEST_REAL)} real, {len(CELEB_TEST_FAKE)} fake\")\n",
    "        print(f\"  Train: {len(CELEB_TRAIN_REAL)} real, {len(CELEB_TRAIN_FAKE)} fake\")\n",
    "else:\n",
    "    print(\"âš ï¸  Celeb-DF not found. Add: reubensuju/celeb-df-v2\")\n",
    "    CELEB_VIDEOS = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff93151-4d62-467a-accd-fa1cd5cb1655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:06:22.105142Z",
     "iopub.status.busy": "2026-02-18T22:06:22.104859Z",
     "iopub.status.idle": "2026-02-18T22:06:25.714332Z",
     "shell.execute_reply": "2026-02-18T22:06:25.713373Z",
     "shell.execute_reply.started": "2026-02-18T22:06:22.105120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Sampling helpers + frames-dataset fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "N_SAMPLE = 20   # Videos per class/method\n",
    "\n",
    "def sample_videos(video_list: list, n: int = N_SAMPLE, seed: int = SEED) -> list:\n",
    "    rng = random.Random(seed)\n",
    "    return rng.sample(video_list, min(n, len(video_list)))\n",
    "\n",
    "# â”€â”€â”€ Frames dataset: load PNG sequences instead of videos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_frames_from_dir(frame_dir: Path, n_frames: int = 32,\n",
    "                         face_size: int = 112) -> Optional[np.ndarray]:\n",
    "    pngs = sorted(frame_dir.glob('*.png'))\n",
    "    if len(pngs) < 4:\n",
    "        return None\n",
    "    indices = np.linspace(0, len(pngs) - 1, n_frames, dtype=int)\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        img = cv2.imread(str(pngs[idx]))\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (face_size, face_size))\n",
    "        frames.append(img)\n",
    "    if len(frames) < n_frames // 2:\n",
    "        return None\n",
    "    while len(frames) < n_frames:\n",
    "        frames.append(frames[-1])\n",
    "    return np.stack(frames[:n_frames], axis=0)\n",
    "\n",
    "def get_frame_dirs(frames_root: Path, split: str, method: str = None) -> list:\n",
    "    if frames_root is None:\n",
    "        return []\n",
    "    base = frames_root / split\n",
    "    if not base.exists():\n",
    "        return []\n",
    "    if method:\n",
    "        method_dir = base / method\n",
    "        return sorted([d for d in method_dir.iterdir() if d.is_dir()]) if method_dir.exists() else []\n",
    "    else:\n",
    "        return sorted([d for d in base.iterdir() if d.is_dir()])\n",
    "\n",
    "def load_clip(source, n_frames=32, face_size=112) -> Optional[np.ndarray]:\n",
    "    \"\"\"Load a clip from either a .mp4 path or a frames directory.\"\"\"\n",
    "    if isinstance(source, Path) and source.is_dir():\n",
    "        return load_frames_from_dir(source, n_frames, face_size)\n",
    "    return extract_frames(str(source), n_frames, face_size)\n",
    "\n",
    "# â”€â”€â”€ Build unified SOURCES dict (video paths OR frame dirs) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SOURCES = {}\n",
    "\n",
    "# Priority: use video files if found by discovery\n",
    "for k, v in FF_VIDEOS.items():\n",
    "    SOURCES[k] = v\n",
    "\n",
    "# Fallback: use extracted-frames dataset if no videos found\n",
    "if not SOURCES and FRAMES_ROOT is not None:\n",
    "    print(\"INFO: No video files found â€” using extracted-frames dataset instead\")\n",
    "    SOURCES['real'] = get_frame_dirs(FRAMES_ROOT, 'real')\n",
    "    for method in ['Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures']:\n",
    "        dirs = get_frame_dirs(FRAMES_ROOT, 'fake', method)\n",
    "        if dirs:\n",
    "            SOURCES[method] = dirs\n",
    "\n",
    "print(\"Available sources:\")\n",
    "for k, v in SOURCES.items():\n",
    "    print(f\"  {k:<25}: {len(v):5d} items\")\n",
    "\n",
    "# Quick sanity test\n",
    "if SOURCES:\n",
    "    first_key = next(iter(SOURCES))\n",
    "    test_src = SOURCES[first_key][0]\n",
    "    test_frames = load_clip(test_src, n_frames=32, face_size=112)\n",
    "    if test_frames is not None:\n",
    "        e, hll = compute_hll(test_frames)\n",
    "        print(f\"\\nLoad test ({first_key}): frames={test_frames.shape}, HLL energy={e:.6f}\")\n",
    "    else:\n",
    "        print(\"\\nLoad test failed â€” check dataset structure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedafa1d-4724-468b-b283-5c7539dac6bc",
   "metadata": {},
   "source": [
    "## Section 4 â€” Baseline: Frame Difference (Sanity Check)\n",
    "\n",
    "**Before DWT:** does raw frame-to-frame difference separate real from fake?\n",
    "\n",
    "This is the absolute baseline. If frame difference doesn't separate the classes,\n",
    "we have a data problem, not a model problem. If it does, then temporal signals\n",
    "are real and the question becomes whether DWT adds anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e256b5-cea4-44d3-a98a-647e9d6b6e11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:06:33.749293Z",
     "iopub.status.busy": "2026-02-18T22:06:33.748989Z",
     "iopub.status.idle": "2026-02-18T22:20:16.669431Z",
     "shell.execute_reply": "2026-02-18T22:20:16.668607Z",
     "shell.execute_reply.started": "2026-02-18T22:06:33.749267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Collect frame difference energies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def analyze_videos(video_paths: list, label: str, n_sample: int = N_SAMPLE,\n",
    "                   verbose: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a list of videos and compute HLL + frame-diff metrics.\n",
    "    Returns dict with per-video results.\n",
    "    \"\"\"\n",
    "    sampled = sample_videos(video_paths, n_sample)\n",
    "    results = []\n",
    "    \n",
    "    for i, vpath in enumerate(sampled):\n",
    "        frames = load_clip(vpath, n_frames=32, face_size=112)\n",
    "        if frames is None:\n",
    "            if verbose: print(f\"  âš ï¸  Skip {Path(vpath).name} (read failed)\")\n",
    "            continue\n",
    "        \n",
    "        hll_e, hll_map      = compute_hll(frames)\n",
    "        fd_e, _             = frame_diff_energy(frames)\n",
    "        all_bands           = compute_all_subbands(frames)\n",
    "        traj                = hll_trajectory(frames, window=2)\n",
    "        \n",
    "        results.append({\n",
    "            'video':       Path(vpath).name,\n",
    "            'label':       label,\n",
    "            'hll_energy':  hll_e,\n",
    "            'fd_energy':   fd_e,\n",
    "            'hll_traj':    traj,\n",
    "            'hll_map':     hll_map,\n",
    "            'all_bands':   all_bands,\n",
    "        })\n",
    "        if verbose and (i + 1) % 5 == 0:\n",
    "            print(f\"  [{label}] {i+1}/{len(sampled)} â€” hll={hll_e:.6f}, fd={fd_e:.6f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"ðŸ“Š Collecting metrics... (this takes a few minutes)\")\n",
    "print(\"â”€\" * 60)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# Real videos (FF++)\n",
    "if 'real' in SOURCES and SOURCES['real']:\n",
    "    all_results['real'] = analyze_videos(SOURCES['real'], 'real')\n",
    "    print(f\"  âœ… real: {len(all_results['real'])} videos processed\\n\")\n",
    "\n",
    "# Each manipulation method\n",
    "for method in ['Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures']:\n",
    "    if method in SOURCES and SOURCES[method]:\n",
    "        all_results[method] = analyze_videos(SOURCES[method], method)\n",
    "        print(f\"  âœ… {method}: {len(all_results[method])} videos processed\\n\")\n",
    "\n",
    "print(\"â”€\" * 60)\n",
    "print(f\"âœ… Total groups collected: {list(all_results.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c24368-59f1-4e90-93e4-7befaac9dab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:20:16.671131Z",
     "iopub.status.busy": "2026-02-18T22:20:16.670878Z",
     "iopub.status.idle": "2026-02-18T22:20:17.556519Z",
     "shell.execute_reply": "2026-02-18T22:20:17.555822Z",
     "shell.execute_reply.started": "2026-02-18T22:20:16.671107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Plot: Frame Difference distributions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "COLORS = {\n",
    "    'real':           '#2ecc71',\n",
    "    'Deepfakes':      '#e74c3c',\n",
    "    'Face2Face':      '#e67e22',\n",
    "    'FaceSwap':       '#9b59b6',\n",
    "    'NeuralTextures': '#3498db',\n",
    "    'FaceShifter':    '#1abc9c',\n",
    "}\n",
    "\n",
    "def get_values(results_dict: dict, key: str) -> Dict[str, list]:\n",
    "    return {k: [r[key] for r in v] for k, v in results_dict.items()}\n",
    "\n",
    "fd_vals = get_values(all_results, 'fd_energy')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Section 4: Frame Difference Baseline\\n(Does temporal change separate real from fake?)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# â”€â”€ Boxplot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[0]\n",
    "groups  = list(fd_vals.keys())\n",
    "data    = [fd_vals[g] for g in groups]\n",
    "colors  = [COLORS.get(g, '#95a5a6') for g in groups]\n",
    "\n",
    "bp = ax.boxplot(data, patch_artist=True, notch=True,\n",
    "                medianprops=dict(color='black', linewidth=2))\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_xticklabels([g.replace('NeuralTextures', 'NeuralTex.') for g in groups], rotation=30, ha='right')\n",
    "ax.set_ylabel('Mean Absolute Frame Difference')\n",
    "ax.set_title('Frame Difference Energy by Class')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate medians\n",
    "for i, (g, vals) in enumerate(zip(groups, data)):\n",
    "    med = np.median(vals)\n",
    "    ax.text(i + 1, med * 1.05, f'{med:.4f}', ha='center', fontsize=8, color='black')\n",
    "\n",
    "# â”€â”€ Violin â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[1]\n",
    "positions = np.arange(1, len(groups) + 1)\n",
    "parts = ax.violinplot(data, positions=positions, showmedians=True, showextrema=True)\n",
    "for i, (pc, color) in enumerate(zip(parts['bodies'], colors)):\n",
    "    pc.set_facecolor(color)\n",
    "    pc.set_alpha(0.6)\n",
    "\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels([g.replace('NeuralTextures', 'NeuralTex.') for g in groups], rotation=30, ha='right')\n",
    "ax.set_ylabel('Mean Absolute Frame Difference')\n",
    "ax.set_title('Distribution Shape (Violin)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'sec4_frame_diff_baseline.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Saved sec4_frame_diff_baseline.png\")\n",
    "\n",
    "# â”€â”€ Mann-Whitney U test: each fake method vs real â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nðŸ“Š Statistical Significance (Mann-Whitney U, real vs each method):\")\n",
    "print(f\"{'Method':<20} {'Real median':>12} {'Fake median':>12} {'p-value':>12} {'Significant?':>13}\")\n",
    "print(\"â”€\" * 70)\n",
    "real_fd = fd_vals.get('real', [])\n",
    "for method, vals in fd_vals.items():\n",
    "    if method == 'real': continue\n",
    "    stat, p = stats.mannwhitneyu(real_fd, vals, alternative='two-sided')\n",
    "    sig = 'âœ… YES' if p < 0.05 else 'âŒ NO'\n",
    "    print(f\"{method:<20} {np.median(real_fd):>12.5f} {np.median(vals):>12.5f} {p:>12.4f} {sig:>13}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56e7b1-a478-4aab-a0c1-3d8c75a5a04c",
   "metadata": {},
   "source": [
    "## Section 5 â€” HLL Energy Analysis (Primary Hypothesis Test)\n",
    "\n",
    "Now we apply the 3D-DWT and measure the HLL sub-band energy directly.\n",
    "\n",
    "**Key questions:**\n",
    "1. Is HLL energy higher for fakes than real? (magnitude test)\n",
    "2. Which manipulation method shows the strongest signal?\n",
    "3. Is HLL better than raw frame difference?\n",
    "4. Are all 8 DWT sub-bands discriminative, or just HLL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c3437-18e5-46a0-bd35-b22ab2533d77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:20:17.557793Z",
     "iopub.status.busy": "2026-02-18T22:20:17.557536Z",
     "iopub.status.idle": "2026-02-18T22:20:18.430046Z",
     "shell.execute_reply": "2026-02-18T22:20:18.429333Z",
     "shell.execute_reply.started": "2026-02-18T22:20:17.557771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ HLL Energy boxplots â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "hll_vals = get_values(all_results, 'hll_energy')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Section 5: HLL Sub-Band Energy\\n(Primary Hypothesis: Deepfakes have elevated temporal flicker)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "groups = list(hll_vals.keys())\n",
    "data   = [hll_vals[g] for g in groups]\n",
    "colors = [COLORS.get(g, '#95a5a6') for g in groups]\n",
    "\n",
    "# â”€â”€ Boxplot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[0]\n",
    "bp = ax.boxplot(data, patch_artist=True, notch=False,\n",
    "                medianprops=dict(color='black', linewidth=2.5))\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.75)\n",
    "\n",
    "ax.set_xticklabels([g.replace('NeuralTextures', 'NeuralTex.') for g in groups], rotation=30, ha='right')\n",
    "ax.set_ylabel('HLL Energy (mean squared coefficient)')\n",
    "ax.set_title('HLL Energy per Class')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (g, vals) in enumerate(zip(groups, data)):\n",
    "    med = np.median(vals)\n",
    "    ax.text(i + 1, med * 1.02, f'{med:.5f}', ha='center', fontsize=8)\n",
    "\n",
    "# â”€â”€ Effect size: HLL vs Frame Diff comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[1]\n",
    "real_hll = hll_vals.get('real', [])\n",
    "real_fd  = fd_vals.get('real', [])\n",
    "\n",
    "methods_plot = [g for g in groups if g != 'real']\n",
    "x = np.arange(len(methods_plot))\n",
    "w = 0.35\n",
    "\n",
    "hll_ratios = []\n",
    "fd_ratios  = []\n",
    "for method in methods_plot:\n",
    "    fake_hll = hll_vals.get(method, [1])\n",
    "    fake_fd  = fd_vals.get(method, [1])\n",
    "    hll_ratios.append(np.median(fake_hll) / max(np.median(real_hll), 1e-10))\n",
    "    fd_ratios.append(np.median(fake_fd)   / max(np.median(real_fd),  1e-10))\n",
    "\n",
    "bars1 = ax.bar(x - w/2, hll_ratios, w, label='HLL ratio (fake/real)', color='#e74c3c', alpha=0.75)\n",
    "bars2 = ax.bar(x + w/2, fd_ratios,  w, label='FrameDiff ratio (fake/real)', color='#3498db', alpha=0.75)\n",
    "ax.axhline(1.0, color='black', linestyle='--', label='ratio=1 (no difference)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.replace('NeuralTextures', 'NeuralTex.') for m in methods_plot], rotation=30, ha='right')\n",
    "ax.set_ylabel('Median energy ratio (fake / real)')\n",
    "ax.set_title('HLL vs Frame Diff: Which is more discriminative?')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate\n",
    "for bar, val in zip(bars1, hll_ratios):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{val:.2f}x',\n",
    "            ha='center', va='bottom', fontsize=8, color='#e74c3c', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'sec5_hll_energy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Saved sec5_hll_energy.png\")\n",
    "\n",
    "# â”€â”€ Stats table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nðŸ“Š HLL Energy Statistics:\")\n",
    "print(f\"{'Class':<20} {'Mean':>10} {'Median':>10} {'Std':>10} {'vs Real':>10} {'p-value':>10}\")\n",
    "print(\"â”€\" * 72)\n",
    "for method, vals in hll_vals.items():\n",
    "    ratio = np.median(vals) / max(np.median(real_hll), 1e-10) if method != 'real' else 1.0\n",
    "    if method != 'real' and real_hll:\n",
    "        _, p = stats.mannwhitneyu(real_hll, vals, alternative='two-sided')\n",
    "        p_str = f'{p:.4f}'\n",
    "    else:\n",
    "        p_str = 'â€”'\n",
    "    print(f\"{method:<20} {np.mean(vals):>10.6f} {np.median(vals):>10.6f} {np.std(vals):>10.6f} {ratio:>10.2f}x {p_str:>10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171cb271-585a-462c-87f1-3d94b467d83e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:20:18.432146Z",
     "iopub.status.busy": "2026-02-18T22:20:18.431836Z",
     "iopub.status.idle": "2026-02-18T22:20:19.024619Z",
     "shell.execute_reply": "2026-02-18T22:20:19.023831Z",
     "shell.execute_reply.started": "2026-02-18T22:20:18.432123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ All 8 sub-bands: which are discriminative? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "subband_names = ['LLL', 'LLH', 'LHL', 'LHH', 'HLL', 'HLH', 'HHL', 'HHH']\n",
    "\n",
    "# Aggregate per sub-band per class\n",
    "subband_data = {}\n",
    "for method, results in all_results.items():\n",
    "    sb = {name: [] for name in subband_names}\n",
    "    for r in results:\n",
    "        for name in subband_names:\n",
    "            sb[name].append(r['all_bands'].get(name, 0.0))\n",
    "    subband_data[method] = {name: np.median(vals) for name, vals in sb.items()}\n",
    "\n",
    "# Compute ratio: each fake method median / real median per subband\n",
    "real_sb = subband_data.get('real', {})\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "fig.suptitle('Section 5b: All 8 DWT Sub-Bands â€” Which Discriminate Real vs Fake?',\n",
    "             fontsize=13, fontweight='bold')\n",
    "\n",
    "x = np.arange(len(subband_names))\n",
    "width = 0.8 / max(1, len(all_results) - 1)\n",
    "\n",
    "fake_methods = [m for m in all_results if m != 'real']\n",
    "offsets = np.linspace(-0.35, 0.35, len(fake_methods))\n",
    "\n",
    "for i, method in enumerate(fake_methods):\n",
    "    ratios = [subband_data[method].get(sb, 0) / max(real_sb.get(sb, 1e-10), 1e-10)\n",
    "              for sb in subband_names]\n",
    "    bars = ax.bar(x + offsets[i], ratios, width * 0.9,\n",
    "                  label=method, color=COLORS.get(method, '#95a5a6'), alpha=0.75)\n",
    "\n",
    "ax.axhline(1.0, color='black', linewidth=1.5, linestyle='--', label='no difference')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(subband_names, fontsize=12, fontweight='bold')\n",
    "\n",
    "# Highlight HLL\n",
    "ax.axvspan(4 - 0.45, 4 + 0.45, alpha=0.1, color='red', label='HLL (our target)')\n",
    "ax.text(4, ax.get_ylim()[1] * 0.95 if ax.get_ylim()[1] > 1 else 1.5,\n",
    "        'HLL â†“', ha='center', fontsize=11, color='red', fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Median Energy Ratio (fake / real)')\n",
    "ax.set_xlabel('DWT Sub-band (Temporal Ã— Height Ã— Width frequency)')\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'sec5b_all_subbands.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Saved sec5b_all_subbands.png\")\n",
    "print(\"\\nðŸ’¡ Key insight: if HLL bar is tallest â†’ HLL is uniquely discriminative\")\n",
    "print(\"   If LLL or spatial bands are equally tall â†’ spatial features matter more than temporal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0777d666-a1d3-4053-a940-7ec718ebbcc1",
   "metadata": {},
   "source": [
    "## Section 6 â€” Temporal Trajectory Analysis\n",
    "\n",
    "A scalar energy value per video is useful, but **how does HLL energy change over time** \n",
    "within a video reveals the temporal pattern.\n",
    "\n",
    "**Real videos:** HLL should fluctuate naturally with motion/lighting â€” irregular, random-looking.  \n",
    "**Deepfakes:** HLL should show more structured patterns â€” sustained elevation, periodic spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3ba69-1641-4b78-bc82-e81aacd52399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:20:19.026663Z",
     "iopub.status.busy": "2026-02-18T22:20:19.026352Z",
     "iopub.status.idle": "2026-02-18T22:20:21.424557Z",
     "shell.execute_reply": "2026-02-18T22:20:21.423525Z",
     "shell.execute_reply.started": "2026-02-18T22:20:19.026640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Plot temporal trajectories â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Section 6: Temporal HLL Trajectory\\n(How does flicker evolve across frames?)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_methods = ['real'] + [m for m in ['Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures'] \n",
    "                            if m in all_results][:4]\n",
    "\n",
    "for ax_idx, method in enumerate(plot_methods):\n",
    "    ax = axes[ax_idx]\n",
    "    results = all_results.get(method, [])[:8]   # Show max 8 videos\n",
    "    color = COLORS.get(method, '#95a5a6')\n",
    "    \n",
    "    all_trajs = []\n",
    "    for r in results:\n",
    "        traj = r['hll_traj']\n",
    "        if len(traj) > 0:\n",
    "            # Normalize each trajectory to [0,1] for shape comparison\n",
    "            traj_norm = (traj - traj.min()) / max(traj.max() - traj.min(), 1e-10)\n",
    "            all_trajs.append(traj_norm)\n",
    "            ax.plot(traj_norm, alpha=0.4, linewidth=1, color=color)\n",
    "    \n",
    "    if all_trajs:\n",
    "        min_len = min(len(t) for t in all_trajs)\n",
    "        stacked = np.stack([t[:min_len] for t in all_trajs], axis=0)\n",
    "        mean_traj = stacked.mean(axis=0)\n",
    "        std_traj  = stacked.std(axis=0)\n",
    "        x = np.arange(min_len)\n",
    "        ax.plot(mean_traj, color='black', linewidth=2.5, label='Mean trajectory')\n",
    "        ax.fill_between(x, mean_traj - std_traj, mean_traj + std_traj,\n",
    "                        alpha=0.2, color='black', label='Â±1 std')\n",
    "    \n",
    "    # Compute trajectory statistics\n",
    "    if all_trajs:\n",
    "        raw_trajs = [r['hll_traj'] for r in results if len(r['hll_traj']) > 0]\n",
    "        variances  = [np.var(t) for t in raw_trajs]\n",
    "        mean_var   = np.mean(variances)\n",
    "    else:\n",
    "        mean_var = 0\n",
    "    \n",
    "    ax.set_title(f'{method}\\n(mean variance = {mean_var:.2e})', fontweight='bold', color=color)\n",
    "    ax.set_xlabel('Time window index')\n",
    "    ax.set_ylabel('Normalized HLL energy')\n",
    "    ax.set_ylim(-0.05, 1.1)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused axes\n",
    "for ax_idx in range(len(plot_methods), len(axes)):\n",
    "    axes[ax_idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'sec6_temporal_trajectory.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Saved sec6_temporal_trajectory.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d351e-1b70-47dd-9870-118195c5fb68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:20:21.426426Z",
     "iopub.status.busy": "2026-02-18T22:20:21.425984Z",
     "iopub.status.idle": "2026-02-18T22:20:22.111906Z",
     "shell.execute_reply": "2026-02-18T22:20:22.111138Z",
     "shell.execute_reply.started": "2026-02-18T22:20:21.426394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Trajectory variance comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Higher variance = more erratic HLL = potentially more suspicious?\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Section 6b: HLL Trajectory Statistics', fontsize=13, fontweight='bold')\n",
    "\n",
    "groups = list(all_results.keys())\n",
    "\n",
    "# Compute autocorrelation at lag-1 (how predictable is the next flicker value?)\n",
    "def lag1_autocorr(x):\n",
    "    if len(x) < 3: return 0.0\n",
    "    return np.corrcoef(x[:-1], x[1:])[0, 1]\n",
    "\n",
    "variances = {g: [np.var(r['hll_traj']) for r in all_results[g] if len(r['hll_traj']) > 0]\n",
    "             for g in groups}\n",
    "autocorrs = {g: [lag1_autocorr(r['hll_traj']) for r in all_results[g] if len(r['hll_traj']) > 0]\n",
    "             for g in groups}\n",
    "\n",
    "# Variance boxplot\n",
    "ax = axes[0]\n",
    "data   = [variances.get(g, [0]) for g in groups]\n",
    "colors = [COLORS.get(g, '#95a5a6') for g in groups]\n",
    "bp = ax.boxplot(data, patch_artist=True, medianprops=dict(color='black', linewidth=2))\n",
    "for patch, c in zip(bp['boxes'], colors): patch.set_facecolor(c); patch.set_alpha(0.7)\n",
    "ax.set_xticklabels([g.replace('NeuralTextures','NeuralTex.') for g in groups], rotation=30, ha='right')\n",
    "ax.set_ylabel('HLL trajectory variance')\n",
    "ax.set_title('Variance: Higher = More Erratic Flicker')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Autocorrelation boxplot\n",
    "ax = axes[1]\n",
    "data = [autocorrs.get(g, [0]) for g in groups]\n",
    "bp = ax.boxplot(data, patch_artist=True, medianprops=dict(color='black', linewidth=2))\n",
    "for patch, c in zip(bp['boxes'], colors): patch.set_facecolor(c); patch.set_alpha(0.7)\n",
    "ax.set_xticklabels([g.replace('NeuralTextures','NeuralTex.') for g in groups], rotation=30, ha='right')\n",
    "ax.set_ylabel('Lag-1 autocorrelation of HLL trajectory')\n",
    "ax.set_title('Autocorrelation: Higher = More Structured/Predictable Flicker')\n",
    "ax.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "print(\"\\nðŸ“Š Trajectory variance (mean per class):\")\n",
    "for g in groups:\n",
    "    vals = variances.get(g, [])\n",
    "    if vals:\n",
    "        print(f\"  {g:<20}: {np.mean(vals):.2e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'sec6b_trajectory_stats.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Saved sec6b_trajectory_stats.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43480ffa-2f13-462d-87c4-ac54fe4a2dce",
   "metadata": {},
   "source": [
    "## Section 7 â€” Spatial HLL Heatmaps\n",
    "\n",
    "*Where* in the face does temporal flicker concentrate?\n",
    "\n",
    "Real: flicker should be diffuse (camera noise, natural motion).  \n",
    "Deepfakes: flicker should concentrate at **blend boundaries** â€” typically the face oval boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a77d25-5c89-4d43-964a-9440ed69993e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:20:22.113195Z",
     "iopub.status.busy": "2026-02-18T22:20:22.112889Z",
     "iopub.status.idle": "2026-02-18T22:20:24.766245Z",
     "shell.execute_reply": "2026-02-18T22:20:24.765432Z",
     "shell.execute_reply.started": "2026-02-18T22:20:22.113153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Average HLL spatial heatmap per class â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Section 7: Spatial HLL Heatmaps\\n(Where does temporal flicker concentrate?)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_methods = ['real'] + [m for m in ['Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures']\n",
    "                            if m in all_results][:4]\n",
    "\n",
    "global_vmax = 0\n",
    "avg_maps = {}\n",
    "\n",
    "for method in plot_methods:\n",
    "    results = all_results.get(method, [])\n",
    "    maps = []\n",
    "    for r in results:\n",
    "        hll_map = np.abs(r['hll_map'])              # (T//2, H//2, W//2)\n",
    "        temporal_max = hll_map.max(axis=0)           # (H//2, W//2) â€” worst-case per pixel\n",
    "        maps.append(temporal_max)\n",
    "    if maps:\n",
    "        avg_map = np.mean(maps, axis=0)              # average over videos\n",
    "        avg_maps[method] = avg_map\n",
    "        global_vmax = max(global_vmax, avg_map.max())\n",
    "\n",
    "for ax_idx, method in enumerate(plot_methods):\n",
    "    ax = axes[ax_idx]\n",
    "    if method not in avg_maps:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    \n",
    "    avg_map = avg_maps[method]\n",
    "    color = COLORS.get(method, '#95a5a6')\n",
    "    \n",
    "    im = ax.imshow(avg_map, cmap='hot', vmin=0, vmax=global_vmax, interpolation='bilinear')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    ax.set_title(f'{method}\\nMax HLL={avg_map.max():.5f}', fontweight='bold',\n",
    "                 color='white' if method != 'real' else 'black',\n",
    "                 bbox=dict(facecolor=color, alpha=0.8, boxstyle='round'))\n",
    "    ax.axis('off')\n",
    "\n",
    "for ax_idx in range(len(plot_methods), len(axes)):\n",
    "    axes[ax_idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'sec7_spatial_heatmaps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Saved sec7_spatial_heatmaps.png\")\n",
    "print(\"\\nðŸ’¡ Key: if deepfakes show ring-shaped or boundary-concentrated heatmaps â†’ HLL detects blend edges\")\n",
    "print(\"         if all heatmaps look similar â†’ HLL doesn't localize forgeries spatially\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19c57fd-8ac6-42ab-be88-31c6e4318382",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:20:24.768263Z",
     "iopub.status.busy": "2026-02-18T22:20:24.767667Z",
     "iopub.status.idle": "2026-02-18T22:20:24.775946Z",
     "shell.execute_reply": "2026-02-18T22:20:24.775302Z",
     "shell.execute_reply.started": "2026-02-18T22:20:24.768230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Center vs border HLL ratio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Blend boundaries are at the face periphery, so fake videos should have\n",
    "# higher HLL at borders vs center compared to real\n",
    "print(\"ðŸ“Š Center vs Border HLL concentration:\")\n",
    "print(f\"{'Class':<20} {'Center':>10} {'Border':>10} {'Border/Center':>15}\")\n",
    "print(\"â”€\" * 55)\n",
    "\n",
    "for method in plot_methods:\n",
    "    if method not in avg_maps:\n",
    "        continue\n",
    "    avg_map = avg_maps[method]\n",
    "    H, W    = avg_map.shape\n",
    "    \n",
    "    # Center region: middle 50%\n",
    "    cy1, cy2 = H // 4, 3 * H // 4\n",
    "    cx1, cx2 = W // 4, 3 * W // 4\n",
    "    center_val = avg_map[cy1:cy2, cx1:cx2].mean()\n",
    "    \n",
    "    # Border: everything outside center\n",
    "    border_mask = np.ones_like(avg_map, dtype=bool)\n",
    "    border_mask[cy1:cy2, cx1:cx2] = False\n",
    "    border_val  = avg_map[border_mask].mean()\n",
    "    \n",
    "    ratio = border_val / max(center_val, 1e-10)\n",
    "    print(f\"{method:<20} {center_val:>10.5f} {border_val:>10.5f} {ratio:>15.3f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fe953-8ded-4fc0-9721-ee5c44ef8457",
   "metadata": {},
   "source": [
    "## Section 8 â€” Cross-Dataset Validation (Celeb-DF)\n",
    "\n",
    "**The generalization test:** If HLL energy separates real from fake on Celeb-DF  \n",
    "(which uses *different generation methods* than FF++), then HLL is a universal signal.\n",
    "\n",
    "If the HLL distributions on Celeb-DF look similar to FF++, we're on the right track for V8.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253053ea-bd4c-4964-bfc1-15f79e960e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:20:24.777147Z",
     "iopub.status.busy": "2026-02-18T22:20:24.776895Z",
     "iopub.status.idle": "2026-02-18T22:20:44.094644Z",
     "shell.execute_reply": "2026-02-18T22:20:44.093910Z",
     "shell.execute_reply.started": "2026-02-18T22:20:24.777128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Celeb-DF HLL analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "celeb_results = {}\n",
    "\n",
    "if CELEB_VIDEOS:\n",
    "    print(\"ðŸ“Š Processing Celeb-DF videos...\")\n",
    "    for split in ['real', 'fake']:\n",
    "        if split in CELEB_VIDEOS and CELEB_VIDEOS[split]:\n",
    "            celeb_results[f'CelebDF_{split}'] = analyze_videos(\n",
    "                CELEB_VIDEOS[split], f'CelebDF_{split}', n_sample=N_SAMPLE\n",
    "            )\n",
    "            print(f\"  âœ… CelebDF/{split}: {len(celeb_results[f'CelebDF_{split}'])} videos\")\n",
    "else:\n",
    "    print(\"âš ï¸  Celeb-DF not available â€” skipping cross-dataset section\")\n",
    "    print(\"   To enable: attach 'andrewmvd/celeb-df-v2' to your Kaggle notebook\")\n",
    "\n",
    "if celeb_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Section 8: Cross-Dataset Generalization\\nDoes HLL signal transfer from FF++ to Celeb-DF?',\n",
    "                 fontsize=13, fontweight='bold')\n",
    "    \n",
    "    CELEB_COLORS = {'CelebDF_real': '#2ecc71', 'CelebDF_fake': '#e74c3c'}\n",
    "    all_for_plot  = {**hll_vals}\n",
    "    celeb_hll     = {k: [r['hll_energy'] for r in v] for k, v in celeb_results.items()}\n",
    "    all_for_plot.update(celeb_hll)\n",
    "    \n",
    "    # Boxplot: FF++ methods + Celeb-DF\n",
    "    ax = axes[0]\n",
    "    groups = list(all_for_plot.keys())\n",
    "    data   = [all_for_plot[g] for g in groups]\n",
    "    colors = [CELEB_COLORS.get(g, COLORS.get(g, '#95a5a6')) for g in groups]\n",
    "    \n",
    "    bp = ax.boxplot(data, patch_artist=True, medianprops=dict(color='black', linewidth=2))\n",
    "    for patch, c in zip(bp['boxes'], colors): patch.set_facecolor(c); patch.set_alpha(0.7)\n",
    "    ax.set_xticklabels([g.replace('NeuralTextures','NeuralTex.').replace('CelebDF_','CDF-')\n",
    "                        for g in groups], rotation=40, ha='right', fontsize=9)\n",
    "    ax.set_ylabel('HLL Energy')\n",
    "    ax.set_title('HLL Energy: FF++ vs Celeb-DF')\n",
    "    ax.axvline(len(hll_vals) + 0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "    ax.text(len(hll_vals) + 0.6, ax.get_ylim()[1] * 0.95, 'Celeb-DF â†’', fontsize=9, color='gray')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Compare: does the real/fake gap transfer?\n",
    "    ax = axes[1]\n",
    "    ff_real_med   = np.median(hll_vals.get('real', [0]))\n",
    "    ff_fake_meds  = {m: np.median(hll_vals.get(m, [0])) for m in hll_vals if m != 'real'}\n",
    "    cdf_real_med  = np.median(celeb_hll.get('CelebDF_real', [0]))\n",
    "    cdf_fake_med  = np.median(celeb_hll.get('CelebDF_fake', [0]))\n",
    "    \n",
    "    datasets = ['FF++ Real'] + [f'FF++\\n{m[:8]}' for m in ff_fake_meds] + ['CDF Real', 'CDF Fake']\n",
    "    values   = [ff_real_med] + list(ff_fake_meds.values()) + [cdf_real_med, cdf_fake_med]\n",
    "    bar_colors = ['#2ecc71'] + [COLORS.get(m, '#e74c3c') for m in ff_fake_meds] + ['#2ecc71', '#e74c3c']\n",
    "    \n",
    "    bars = ax.bar(range(len(values)), values, color=bar_colors, alpha=0.75, edgecolor='white')\n",
    "    ax.set_xticks(range(len(values)))\n",
    "    ax.set_xticklabels(datasets, rotation=40, ha='right', fontsize=8)\n",
    "    ax.set_ylabel('Median HLL Energy')\n",
    "    ax.set_title('Median HLL: Does the Gap Transfer to Celeb-DF?')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, val * 1.02, f'{val:.5f}',\n",
    "                ha='center', va='bottom', fontsize=7, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'sec8_cross_dataset.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ… Saved sec8_cross_dataset.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889baa5-c535-4943-a302-d14aed8163e0",
   "metadata": {},
   "source": [
    "## Section 9 â€” SBI vs Real Deepfake Comparison\n",
    "\n",
    "**Critical question for V8.0 training strategy:** Is SBI a good proxy for real deepfakes?\n",
    "\n",
    "We generate simple SBI examples in-notebook (no dlib required â€” basic blend only)\n",
    "and compare their HLL energy to real FF++ deepfakes.\n",
    "\n",
    "If SBI HLL â‰ˆ real deepfake HLL â†’ SBI training is valid.  \n",
    "If SBI HLL â‰« real deepfake HLL â†’ SBI creates unrealistic artifacts â†’ training fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857846b7-1505-4903-aad1-f53ac27fdbca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:20:44.097305Z",
     "iopub.status.busy": "2026-02-18T22:20:44.096975Z",
     "iopub.status.idle": "2026-02-18T22:22:04.861277Z",
     "shell.execute_reply": "2026-02-18T22:22:04.860601Z",
     "shell.execute_reply.started": "2026-02-18T22:20:44.097282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Simple SBI generator (no landmarks â€” basic alpha blend) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def generate_simple_sbi(frame: np.ndarray, blend_alpha: float = 0.4) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simplest possible SBI: blend face with a slightly shifted copy.\n",
    "    No landmarks needed â€” just center region blending.\n",
    "    This is a crude proxy but enough for HLL comparison.\n",
    "    \"\"\"\n",
    "    H, W = frame.shape[:2]\n",
    "    fake = frame.copy()\n",
    "    \n",
    "    # Center crop = 'face region'\n",
    "    margin = 0.15\n",
    "    y1, y2 = int(H * margin), int(H * (1 - margin))\n",
    "    x1, x2 = int(W * margin), int(W * (1 - margin))\n",
    "    \n",
    "    # Perturb: slight elastic warp on source\n",
    "    dx = np.random.randint(3, 8)\n",
    "    dy = np.random.randint(3, 8)\n",
    "    source_region = frame[y1+dy:y2+dy, x1+dx:x2+dx]\n",
    "    target_region = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    # Resize source to target if needed\n",
    "    if source_region.shape != target_region.shape:\n",
    "        source_region = cv2.resize(source_region, (target_region.shape[1], target_region.shape[0]))\n",
    "    \n",
    "    # Blend with smooth mask\n",
    "    blended = (blend_alpha * source_region + (1 - blend_alpha) * target_region).astype(np.uint8)\n",
    "    \n",
    "    # Feather the edges\n",
    "    mask = np.zeros((y2 - y1, x2 - x1), dtype=np.float32)\n",
    "    inner_margin = 10\n",
    "    mask[inner_margin:-inner_margin, inner_margin:-inner_margin] = 1.0\n",
    "    mask = cv2.GaussianBlur(mask, (21, 21), 0)\n",
    "    mask = mask[:, :, np.newaxis]\n",
    "    \n",
    "    fake[y1:y2, x1:x2] = (mask * blended + (1 - mask) * target_region).astype(np.uint8)\n",
    "    return fake\n",
    "\n",
    "\n",
    "def generate_sbi_video(real_frames: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Generate SBI fake version of a real video clip.\"\"\"\n",
    "    sbi_frames = []\n",
    "    for frame in real_frames:\n",
    "        # Small Brownian drift: Â±2px per frame\n",
    "        fake = generate_simple_sbi(frame, blend_alpha=np.random.uniform(0.3, 0.6))\n",
    "        sbi_frames.append(fake)\n",
    "    return np.stack(sbi_frames, axis=0)\n",
    "\n",
    "\n",
    "# â”€â”€â”€ Compare SBI vs real deepfake HLL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sbi_results = []\n",
    "real_source_results = []\n",
    "\n",
    "if 'real' in SOURCES and SOURCES['real']:\n",
    "    real_sample = sample_videos(SOURCES['real'], 10)\n",
    "    print(\"ðŸ“Š Generating SBI fakes from real videos and comparing HLL...\")\n",
    "    \n",
    "    for vpath in real_sample:\n",
    "        frames = load_clip(vpath, n_frames=32, face_size=112)\n",
    "        if frames is None: continue\n",
    "        \n",
    "        # Real HLL\n",
    "        real_e, _ = compute_hll(frames)\n",
    "        real_source_results.append(real_e)\n",
    "        \n",
    "        # SBI HLL\n",
    "        sbi_frames = generate_sbi_video(frames)\n",
    "        sbi_e, _   = compute_hll(sbi_frames)\n",
    "        sbi_results.append(sbi_e)\n",
    "        \n",
    "        print(f\"  Real: {real_e:.6f} â†’ SBI: {sbi_e:.6f} (ratio: {sbi_e/max(real_e,1e-10):.2f}x)\")\n",
    "\n",
    "print(f\"\\nReal HLL median:  {np.median(real_source_results):.6f}\")\n",
    "print(f\"SBI HLL median:   {np.median(sbi_results):.6f}\")\n",
    "print(f\"SBI/Real ratio:   {np.median(sbi_results)/max(np.median(real_source_results),1e-10):.2f}x\")\n",
    "\n",
    "ff_df_hll   = hll_vals.get('Deepfakes', [])\n",
    "ff_df_med   = np.median(ff_df_hll) if ff_df_hll else None\n",
    "print(f\"FF++ Deepfakes HLL median: {ff_df_med:.6f}\" if ff_df_med else \"FF++ Deepfakes not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0944d557-9b88-4ff2-91dc-75784f524a33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:22:04.862485Z",
     "iopub.status.busy": "2026-02-18T22:22:04.862266Z",
     "iopub.status.idle": "2026-02-18T22:22:05.695727Z",
     "shell.execute_reply": "2026-02-18T22:22:05.694949Z",
     "shell.execute_reply.started": "2026-02-18T22:22:04.862465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ SBI comparison plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if sbi_results and real_source_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Section 9: SBI vs Real Deepfakes â€” Is SBI a Good Training Proxy?',\n",
    "                 fontsize=13, fontweight='bold')\n",
    "    \n",
    "    ax = axes[0]\n",
    "    comparison_groups  = {'Real':       real_source_results,\n",
    "                          'SBI Fake':   sbi_results}\n",
    "    if ff_df_hll:\n",
    "        comparison_groups['FF++ Deepfakes'] = ff_df_hll\n",
    "    if 'Face2Face' in hll_vals:\n",
    "        comparison_groups['FF++ Face2Face'] = hll_vals['Face2Face']\n",
    "    \n",
    "    comp_colors = {'Real':'#2ecc71', 'SBI Fake':'#f39c12',\n",
    "                   'FF++ Deepfakes':'#e74c3c', 'FF++ Face2Face':'#e67e22'}\n",
    "    \n",
    "    data   = list(comparison_groups.values())\n",
    "    labels = list(comparison_groups.keys())\n",
    "    colors = [comp_colors.get(l, '#95a5a6') for l in labels]\n",
    "    \n",
    "    bp = ax.boxplot(data, patch_artist=True, medianprops=dict(color='black', linewidth=2.5))\n",
    "    for patch, c in zip(bp['boxes'], colors): patch.set_facecolor(c); patch.set_alpha(0.75)\n",
    "    ax.set_xticklabels(labels, rotation=20, ha='right')\n",
    "    ax.set_ylabel('HLL Energy')\n",
    "    ax.set_title('SBI HLL vs Real Deepfake HLL')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    # Paired comparison: same source video, real vs SBI\n",
    "    x = np.arange(len(real_source_results))\n",
    "    ax.scatter(x, real_source_results, color='#2ecc71', s=80, label='Real', zorder=3)\n",
    "    ax.scatter(x, sbi_results,         color='#f39c12', s=80, label='SBI Fake', zorder=3, marker='^')\n",
    "    for i in range(len(real_source_results)):\n",
    "        ax.plot([i, i], [real_source_results[i], sbi_results[i]],\n",
    "                color='gray', alpha=0.4, linewidth=1)\n",
    "    ax.axhline(np.median(real_source_results), color='#2ecc71', linestyle='--', alpha=0.7, label='Real median')\n",
    "    ax.axhline(np.median(sbi_results),         color='#f39c12', linestyle='--', alpha=0.7, label='SBI median')\n",
    "    if ff_df_med:\n",
    "        ax.axhline(ff_df_med, color='#e74c3c', linestyle=':', alpha=0.7, label='FF++ Deepfakes median')\n",
    "    ax.set_xlabel('Video index')\n",
    "    ax.set_ylabel('HLL Energy')\n",
    "    ax.set_title('Paired: Same Source Video â€” Real vs SBI')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'sec9_sbi_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ… Saved sec9_sbi_comparison.png\")\n",
    "    \n",
    "    # Key diagnostic\n",
    "    sbi_med  = np.median(sbi_results)\n",
    "    real_med = np.median(real_source_results)\n",
    "    print(\"\\nðŸ” SBI Proxy Quality Diagnosis:\")\n",
    "    if ff_df_med:\n",
    "        sbi_diff  = abs(sbi_med  - ff_df_med) / max(ff_df_med, 1e-10)\n",
    "        real_diff = abs(real_med - ff_df_med) / max(ff_df_med, 1e-10)\n",
    "        print(f\"  SBI  distance from FF++ Deepfakes: {sbi_diff:.1%}\")\n",
    "        print(f\"  Real distance from FF++ Deepfakes: {real_diff:.1%}\")\n",
    "        if sbi_diff < real_diff:\n",
    "            print(\"  âœ… SBI is a reasonable proxy for real deepfakes on HLL\")\n",
    "        else:\n",
    "            print(\"  âš ï¸  SBI is farther from real deepfakes than real videos are!\")\n",
    "            print(\"     This explains why SBI-trained models fail on real deepfakes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a1e0c-d677-4513-86e9-d4c33a3587cf",
   "metadata": {},
   "source": [
    "## Section 10 â€” Summary & V8.0 Recommendations\n",
    "\n",
    "Based on the analysis above, fill in the findings table below.  \n",
    "The answers directly determine the V8.0 architecture choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed73c0d3-27ef-44a5-966a-3f728fe6f4b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:22:05.696999Z",
     "iopub.status.busy": "2026-02-18T22:22:05.696742Z",
     "iopub.status.idle": "2026-02-18T22:22:05.711747Z",
     "shell.execute_reply": "2026-02-18T22:22:05.710837Z",
     "shell.execute_reply.started": "2026-02-18T22:22:05.696978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Auto-generate findings report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“‹ STF-MAMBA V8.0 DATA ANALYSIS REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "real_hll_med = np.median(hll_vals.get('real', [0]))\n",
    "print(f\"\\n1. HLL ENERGY SEPARATION\")\n",
    "print(f\"   Real videos median HLL: {real_hll_med:.6f}\")\n",
    "for method in ['Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures']:\n",
    "    if method in hll_vals:\n",
    "        med = np.median(hll_vals[method])\n",
    "        ratio = med / max(real_hll_med, 1e-10)\n",
    "        sep = 'âœ… SEPARATES' if ratio > 1.1 else 'âš ï¸  WEAK' if ratio > 0.9 else 'âŒ INVERTED'\n",
    "        print(f\"   {method:<20}: {med:.6f} ({ratio:.2f}x real) â†’ {sep}\")\n",
    "\n",
    "print(f\"\\n2. FRAME DIFF vs HLL\")\n",
    "real_fd_med = np.median(fd_vals.get('real', [0]))\n",
    "for method in ['Deepfakes', 'Face2Face']:\n",
    "    if method in hll_vals and method in fd_vals:\n",
    "        hll_ratio = np.median(hll_vals[method]) / max(real_hll_med, 1e-10)\n",
    "        fd_ratio  = np.median(fd_vals[method])  / max(real_fd_med,  1e-10)\n",
    "        winner = 'HLL' if hll_ratio > fd_ratio else 'FrameDiff'\n",
    "        print(f\"   {method}: HLL={hll_ratio:.2f}x | FrameDiff={fd_ratio:.2f}x | Winner={winner}\")\n",
    "\n",
    "print(f\"\\n3. MOST DISCRIMINATIVE DWT SUB-BAND\")\n",
    "if all_results and 'real' in all_results:\n",
    "    real_sb = {name: np.mean([r['all_bands'].get(name, 0) for r in all_results['real']])\n",
    "               for name in ['LLL', 'LLH', 'LHL', 'LHH', 'HLL', 'HLH', 'HHL', 'HHH']}\n",
    "    for method in ['Deepfakes', 'Face2Face']:\n",
    "        if method in all_results:\n",
    "            fake_sb = {name: np.mean([r['all_bands'].get(name, 0) for r in all_results[method]])\n",
    "                       for name in real_sb}\n",
    "            ratios = {name: fake_sb[name] / max(real_sb[name], 1e-10) for name in real_sb}\n",
    "            best = max(ratios, key=ratios.get)\n",
    "            print(f\"   {method}: most discriminative sub-band = {best} ({ratios[best]:.2f}x)\")\n",
    "            \n",
    "print(f\"\\n4. SBI PROXY QUALITY\")\n",
    "if sbi_results and real_source_results and ff_df_hll:\n",
    "    sbi_med    = np.median(sbi_results)\n",
    "    ff_df_med  = np.median(ff_df_hll)\n",
    "    quality    = 'GOOD PROXY' if abs(sbi_med - ff_df_med) / max(ff_df_med, 1e-10) < 0.3 else 'POOR PROXY'\n",
    "    print(f\"   SBI HLL median:   {sbi_med:.6f}\")\n",
    "    print(f\"   FF++ DFakes HLL:  {ff_df_med:.6f}\")\n",
    "    print(f\"   Assessment:       {quality}\")\n",
    "\n",
    "print(f\"\\n5. V8.0 DESIGN RECOMMENDATIONS\")\n",
    "print(\"   Based on the data above:\")\n",
    "print(\"   [ ] Keep HLL if separation ratio > 1.1x for majority of methods\")\n",
    "print(\"   [ ] Add frame-difference branch if FrameDiff > HLL on any method\")\n",
    "print(\"   [ ] Use contrastive HLL loss (not absolute MSE(HLL, 0))\")\n",
    "print(\"   [ ] Train on REAL FF++ deepfakes, not just SBI, if SBI is poor proxy\")\n",
    "print(\"   [ ] Focus augmentation on border/boundary regions if heatmaps show boundary concentration\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(f\"ðŸ“ All plots saved to: {PLOTS_DIR}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4639e0a-166a-40ca-a90c-100b9b621b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T22:22:05.713531Z",
     "iopub.status.busy": "2026-02-18T22:22:05.712949Z",
     "iopub.status.idle": "2026-02-18T22:22:05.729850Z",
     "shell.execute_reply": "2026-02-18T22:22:05.729076Z",
     "shell.execute_reply.started": "2026-02-18T22:22:05.713508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€ Save all numerical results to JSON for reference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def to_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray): return obj.tolist()\n",
    "    if isinstance(obj, (np.float32, np.float64)): return float(obj)\n",
    "    if isinstance(obj, (np.int32, np.int64)): return int(obj)\n",
    "    return obj\n",
    "\n",
    "summary = {\n",
    "    'hll_energy_stats': {\n",
    "        method: {\n",
    "            'mean':   float(np.mean(vals)),\n",
    "            'median': float(np.median(vals)),\n",
    "            'std':    float(np.std(vals)),\n",
    "            'count':  len(vals)\n",
    "        }\n",
    "        for method, vals in hll_vals.items()\n",
    "    },\n",
    "    'frame_diff_stats': {\n",
    "        method: {\n",
    "            'mean':   float(np.mean(vals)),\n",
    "            'median': float(np.median(vals)),\n",
    "        }\n",
    "        for method, vals in fd_vals.items()\n",
    "    },\n",
    "    'sbi_proxy': {\n",
    "        'sbi_median':          float(np.median(sbi_results)) if sbi_results else None,\n",
    "        'real_source_median':  float(np.median(real_source_results)) if real_source_results else None,\n",
    "        'ff_deepfakes_median': float(np.median(hll_vals.get('Deepfakes', [0])))\n",
    "    }\n",
    "}\n",
    "\n",
    "out_path = OUTPUT_DIR / 'analysis_results.json'\n",
    "with open(out_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=to_serializable)\n",
    "print(f\"âœ… Numerical results saved â†’ {out_path}\")\n",
    "print(\"\\nðŸ Analysis complete! Review plots and report above before designing V8.0.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 5454468,
     "datasetId": 3120670,
     "sourceId": 5380830,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10408999,
     "datasetId": 6248577,
     "sourceId": 10125851,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
