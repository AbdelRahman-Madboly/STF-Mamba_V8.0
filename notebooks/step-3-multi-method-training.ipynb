{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6dc5bd9-1f91-45f3-86a7-97381b71fc92",
   "metadata": {},
   "source": [
    "# Step 3: Multi-Method Training\n",
    "## Fixing the Two Diagnosed Weaknesses from Step 2\n",
    "\n",
    "**Step 2 result:** EfficientNet-B0 â†’ FF++ Val AUC=0.7241, Celeb-DF AUC=0.6659\n",
    "\n",
    "**Two diagnosed problems:**\n",
    "1. Trained on Deepfakes only â†’ learned one method's fingerprint, not general semantics\n",
    "2. Only 300 videos â†’ overfitting (train loss 0.25, val loss 0.85 by epoch 20)\n",
    "\n",
    "**This notebook fixes both:**\n",
    "- All 4 FF++ methods: Deepfakes + Face2Face + FaceSwap + NeuralTextures\n",
    "- 600 real + 600 fake videos (150 per method)\n",
    "- Fixed double forward-pass bug from Step 2 train loop\n",
    "- Everything else identical to Step 2\n",
    "\n",
    "**Expected improvement:** +8-13% Celeb-DF AUC (0.6659 â†’ 0.75-0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c5becb-9bbd-4612-83c6-8c76f3d7b17f",
   "metadata": {},
   "source": [
    "## Section 1 â€” Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b540b33-8c54-42b6-b08a-00761a74921f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T02:24:57.779641Z",
     "iopub.status.busy": "2026-02-19T02:24:57.779426Z",
     "iopub.status.idle": "2026-02-19T02:25:13.455117Z",
     "shell.execute_reply": "2026-02-19T02:25:13.454397Z",
     "shell.execute_reply.started": "2026-02-19T02:24:57.779621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, json, random, time, warnings, sys\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device : {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU    : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM   : {torch.cuda.get_device_properties(0).total_memory/1e9:.1f} GB\")\n",
    "\n",
    "OUTPUT_DIR = Path('/kaggle/working/step3')\n",
    "CKPT_DIR   = OUTPUT_DIR / 'checkpoints'\n",
    "PLOTS_DIR  = OUTPUT_DIR / 'plots'\n",
    "for d in [OUTPUT_DIR, CKPT_DIR, PLOTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Outputs â†’ {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8737f2f-5621-4cd5-be40-52579440722d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T02:25:13.456743Z",
     "iopub.status.busy": "2026-02-19T02:25:13.456389Z",
     "iopub.status.idle": "2026-02-19T02:25:13.462230Z",
     "shell.execute_reply": "2026-02-19T02:25:13.461355Z",
     "shell.execute_reply.started": "2026-02-19T02:25:13.456720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'img_size':        224,\n",
    "    'n_frames':        4,       # frames per video\n",
    "    'n_train_real':    600,     # â†‘ was 300\n",
    "    'n_train_fake':    600,     # â†‘ was 300 (150 per method Ã— 4 methods)\n",
    "    'n_val_each':      50,      # per class for validation\n",
    "    'epochs':          20,\n",
    "    'batch_size':      32,\n",
    "    'lr':              1e-4,\n",
    "    'weight_decay':    1e-4,\n",
    "    'warmup_epochs':   3,\n",
    "    'dropout':         0.3,\n",
    "    'label_smoothing': 0.0,     # CONFIRMED: must be 0.0\n",
    "}\n",
    "\n",
    "TRAIN_METHODS = ['Deepfakes', 'Face2Face', 'FaceSwap', 'NeuralTextures']\n",
    "\n",
    "print(\"Config:\")\n",
    "for k, v in CFG.items():\n",
    "    print(f\"  {k:22s}: {v}\")\n",
    "print(f\"Train methods: {TRAIN_METHODS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985255d-fc3d-45da-b132-a0e178a65def",
   "metadata": {},
   "source": [
    "## Section 2 â€” Dataset Paths & Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d92deac-6bf4-49a2-9e2c-653659a9f207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T02:25:13.467488Z",
     "iopub.status.busy": "2026-02-19T02:25:13.467031Z",
     "iopub.status.idle": "2026-02-19T02:25:42.489896Z",
     "shell.execute_reply": "2026-02-19T02:25:42.489029Z",
     "shell.execute_reply.started": "2026-02-19T02:25:13.467467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "KAGGLE_INPUT = Path('/kaggle/input')\n",
    "\n",
    "def locate_ff_root(base):\n",
    "    known = base / 'datasets' / 'xdxd003' / 'ff-c23' / 'FaceForensics++_C23'\n",
    "    if known.exists(): return known\n",
    "    for d in sorted(base.rglob('*')):\n",
    "        if d.is_dir():\n",
    "            if sum(1 for m in ['Deepfakes','Face2Face','FaceSwap'] if (d/m).exists()) >= 2:\n",
    "                return d\n",
    "    return None\n",
    "\n",
    "def locate_celeb_root(base):\n",
    "    known = base / 'datasets' / 'reubensuju' / 'celeb-df-v2'\n",
    "    if known.exists(): return known\n",
    "    for d in sorted(base.rglob('*')):\n",
    "        if d.is_dir() and (d/'Celeb-real').exists(): return d\n",
    "    return None\n",
    "\n",
    "FF_ROOT    = locate_ff_root(KAGGLE_INPUT)\n",
    "CELEB_ROOT = locate_celeb_root(KAGGLE_INPUT)\n",
    "print(f\"FF++    : {FF_ROOT}\")\n",
    "print(f\"Celeb-DF: {CELEB_ROOT}\")\n",
    "\n",
    "FF_REAL = sorted(FF_ROOT.rglob('original*/*.mp4')) if FF_ROOT else []\n",
    "if not FF_REAL and FF_ROOT:\n",
    "    FF_REAL = sorted(p for p in FF_ROOT.rglob('*.mp4') if 'original' in str(p).lower())\n",
    "\n",
    "FF_FAKE_BY_METHOD = {}\n",
    "for method in TRAIN_METHODS:\n",
    "    paths = sorted((FF_ROOT/method).glob('*.mp4')) if FF_ROOT and (FF_ROOT/method).exists() else []\n",
    "    FF_FAKE_BY_METHOD[method] = paths\n",
    "    print(f\"  FF++/{method:20s}: {len(paths)} videos\")\n",
    "print(f\"  FF++/{'real':20s}: {len(FF_REAL)} videos\")\n",
    "\n",
    "CDF_REAL, CDF_FAKE = [], []\n",
    "if CELEB_ROOT:\n",
    "    CDF_REAL = (sorted((CELEB_ROOT/'Celeb-real').glob('*.mp4')) +\n",
    "                sorted((CELEB_ROOT/'YouTube-real').glob('*.mp4')))\n",
    "    CDF_FAKE = sorted((CELEB_ROOT/'Celeb-synthesis').glob('*.mp4'))\n",
    "    print(f\"  Celeb-DF real: {len(CDF_REAL)} | fake: {len(CDF_FAKE)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386ac05-4e52-454a-b3b9-13cc30a07140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:07:58.725655Z",
     "iopub.status.busy": "2026-02-19T03:07:58.725027Z",
     "iopub.status.idle": "2026-02-19T03:07:58.805424Z",
     "shell.execute_reply": "2026-02-19T03:07:58.804870Z",
     "shell.execute_reply.started": "2026-02-19T03:07:58.725628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rng = random.Random(SEED)\n",
    "\n",
    "# â”€â”€ Extract video ID from filename â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# FF++ filenames: \"000_003.mp4\" â†’ ID is \"000\" (source video)\n",
    "def get_video_id(path):\n",
    "    return Path(path).stem.split('_')[0]\n",
    "\n",
    "# â”€â”€ Build ID-level splits FIRST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_ids = sorted(set(get_video_id(p) for p in FF_REAL))\n",
    "rng.shuffle(all_ids)\n",
    "\n",
    "n_train_ids = int(len(all_ids) * 0.75)   # 75% train, 25% val\n",
    "train_ids   = set(all_ids[:n_train_ids])\n",
    "val_ids     = set(all_ids[n_train_ids:])\n",
    "\n",
    "print(f\"Total video IDs: {len(all_ids)}\")\n",
    "print(f\"Train IDs: {len(train_ids)} | Val IDs: {len(val_ids)}\")\n",
    "print(f\"(No ID appears in both â€” guaranteed no content leakage)\")\n",
    "\n",
    "# â”€â”€ Training set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_real = [p for p in FF_REAL if get_video_id(p) in train_ids]\n",
    "train_real = rng.sample(train_real, min(CFG['n_train_real'], len(train_real)))\n",
    "TRAIN_DATA = [(p, 0) for p in train_real]\n",
    "\n",
    "n_per_method = CFG['n_train_fake'] // len(TRAIN_METHODS)\n",
    "for method in TRAIN_METHODS:\n",
    "    pool   = [p for p in FF_FAKE_BY_METHOD[method] if get_video_id(p) in train_ids]\n",
    "    picked = rng.sample(pool, min(n_per_method, len(pool)))\n",
    "    TRAIN_DATA += [(p, 1) for p in picked]\n",
    "rng.shuffle(TRAIN_DATA)\n",
    "\n",
    "# â”€â”€ Validation set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_real = [p for p in FF_REAL if get_video_id(p) in val_ids]\n",
    "val_real = rng.sample(val_real, min(CFG['n_val_each'], len(val_real)))\n",
    "VAL_DATA = [(p, 0) for p in val_real]\n",
    "\n",
    "for method in TRAIN_METHODS:\n",
    "    pool   = [p for p in FF_FAKE_BY_METHOD[method] if get_video_id(p) in val_ids]\n",
    "    picked = rng.sample(pool, min(CFG['n_val_each'] // len(TRAIN_METHODS), len(pool)))\n",
    "    VAL_DATA += [(p, 1) for p in picked]\n",
    "rng.shuffle(VAL_DATA)\n",
    "\n",
    "# â”€â”€ Celeb-DF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "n_cdf    = min(200, len(CDF_REAL), len(CDF_FAKE))\n",
    "CDF_TEST = ([(p, 0) for p in rng.sample(CDF_REAL, n_cdf)] +\n",
    "            [(p, 1) for p in rng.sample(CDF_FAKE, n_cdf)])\n",
    "\n",
    "print(f\"\\nTrain: {sum(1 for _,l in TRAIN_DATA if l==0)} real + \"\n",
    "      f\"{sum(1 for _,l in TRAIN_DATA if l==1)} fake = {len(TRAIN_DATA)}\")\n",
    "print(f\"Val  : {sum(1 for _,l in VAL_DATA if l==0)} real + \"\n",
    "      f\"{sum(1 for _,l in VAL_DATA if l==1)} fake = {len(VAL_DATA)}\")\n",
    "print(f\"CDF  : {n_cdf} real + {n_cdf} fake = {len(CDF_TEST)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcbfff-bf18-45c7-9781-3530fe480f62",
   "metadata": {},
   "source": [
    "## Section 3 â€” Dataset (Frame Pre-Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01921e81-c081-43dc-a42c-1368c88994ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:09:09.968323Z",
     "iopub.status.busy": "2026-02-19T03:09:09.967613Z",
     "iopub.status.idle": "2026-02-19T03:25:51.862678Z",
     "shell.execute_reply": "2026-02-19T03:25:51.862018Z",
     "shell.execute_reply.started": "2026-02-19T03:09:09.968296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomGrayscale(p=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "def load_frames(video_path, n_frames, img_size):\n",
    "    \"\"\"Extract n evenly-spaced frames from video. Returns list of uint8 arrays or None.\"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened(): return None\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total < 1:\n",
    "        cap.release(); return None\n",
    "    positions = np.linspace(0, total - 1, n_frames, dtype=int)\n",
    "    frames = []\n",
    "    for pos in positions:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(pos))\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: continue\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w = frame.shape[:2]\n",
    "        frame = frame[int(h*0.05):int(h*0.95), int(w*0.10):int(w*0.90)]\n",
    "        frame = cv2.resize(frame, (img_size, img_size))\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    if not frames: return None\n",
    "    while len(frames) < n_frames: frames.append(frames[-1])\n",
    "    return frames[:n_frames]\n",
    "\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "    \"\"\"Pre-extracts all frames at construction. DataLoader does only transforms.\"\"\"\n",
    "    def __init__(self, video_label_pairs, transform, n_frames, img_size):\n",
    "        self.transform = transform\n",
    "        self.items = []\n",
    "        failed = 0\n",
    "        for path, label in tqdm(video_label_pairs, ncols=80, desc='Loading'):\n",
    "            frames = load_frames(str(path), n_frames, img_size)\n",
    "            if frames is None:\n",
    "                failed += 1\n",
    "                continue\n",
    "            for f in frames:\n",
    "                self.items.append((f, label))\n",
    "        print(f\"  {len(self.items)} frames ready, {failed} videos failed\")\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame, label = self.items[idx]\n",
    "        return self.transform(frame), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "print(\"Pre-extracting frames (this takes ~5 min)...\")\n",
    "t0 = time.time()\n",
    "train_ds = DeepfakeDataset(TRAIN_DATA, train_tf, CFG['n_frames'], CFG['img_size'])\n",
    "val_ds   = DeepfakeDataset(VAL_DATA,   val_tf,   CFG['n_frames'], CFG['img_size'])\n",
    "cdf_ds   = DeepfakeDataset(CDF_TEST,   val_tf,   CFG['n_frames'], CFG['img_size'])\n",
    "print(f\"Done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG['batch_size'],\n",
    "                          shuffle=True,  num_workers=0, pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=CFG['batch_size'],\n",
    "                          shuffle=False, num_workers=0, pin_memory=False)\n",
    "cdf_loader   = DataLoader(cdf_ds,   batch_size=CFG['batch_size'],\n",
    "                          shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(f\"Train frames: {len(train_ds)} | Val: {len(val_ds)} | CDF: {len(cdf_ds)}\")\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"Batch: x={x.shape}, labels={y.unique().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabbcabd-ec15-49fa-b027-793a4808f42e",
   "metadata": {},
   "source": [
    "## Section 4 â€” Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4e7c1-dba7-469d-b363-978b469c786c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:36:01.122769Z",
     "iopub.status.busy": "2026-02-19T03:36:01.122434Z",
     "iopub.status.idle": "2026-02-19T03:36:01.282346Z",
     "shell.execute_reply": "2026-02-19T03:36:01.281749Z",
     "shell.execute_reply.started": "2026-02-19T03:36:01.122741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self, dropout=CFG['dropout']):\n",
    "        super().__init__()\n",
    "        self.backbone = models.efficientnet_b0(\n",
    "            weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        in_feat = self.backbone.classifier[1].in_features  # 1280\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_feat, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout * 0.5),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "        self.backbone_params = list(self.backbone.features.parameters())\n",
    "        self.head_params     = list(self.backbone.classifier.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def get_param_groups(self, base_lr):\n",
    "        return [\n",
    "            {'params': self.backbone_params, 'lr': base_lr / 10},  # 1e-5\n",
    "            {'params': self.head_params,     'lr': base_lr},        # 1e-4\n",
    "        ]\n",
    "\n",
    "model = DeepfakeDetector().to(DEVICE)\n",
    "print(f\"EfficientNet-B0: {sum(p.numel() for p in model.parameters())/1e6:.2f}M params\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(torch.randn(2, 3, 224, 224).to(DEVICE))\n",
    "    print(f\"Forward pass: (2,3,224,224) â†’ {out.shape} âœ“\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72814886-e5c9-4ab9-abec-5028d757a431",
   "metadata": {},
   "source": [
    "## Section 5 â€” Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e90669-c737-4f94-a1f5-5042c8c1e5cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:36:08.879532Z",
     "iopub.status.busy": "2026-02-19T03:36:08.879027Z",
     "iopub.status.idle": "2026-02-19T03:36:08.893438Z",
     "shell.execute_reply": "2026-02-19T03:36:08.892646Z",
     "shell.execute_reply.started": "2026-02-19T03:36:08.879508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=CFG['label_smoothing'])\n",
    "optimizer = torch.optim.AdamW(model.get_param_groups(CFG['lr']),\n",
    "                               weight_decay=CFG['weight_decay'])\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < CFG['warmup_epochs']:\n",
    "        return (epoch + 1) / CFG['warmup_epochs']\n",
    "    progress = (epoch - CFG['warmup_epochs']) / max(1, CFG['epochs'] - CFG['warmup_epochs'])\n",
    "    return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)               # ONE forward pass â€” fixed double-call bug\n",
    "        loss   = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct    += (logits.detach().argmax(1) == y).sum().item()\n",
    "        total      += y.size(0)\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_labels, all_probs = [], []\n",
    "    total_loss, n_batches = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y    = x.to(DEVICE), y.to(DEVICE)\n",
    "            logits  = model(x)\n",
    "            total_loss += criterion(logits, y).item()\n",
    "            probs   = F.softmax(logits, dim=1)[:, 1]\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            n_batches += 1\n",
    "    labels = np.array(all_labels)\n",
    "    probs  = np.array(all_probs)\n",
    "    auc    = roc_auc_score(labels, probs) if len(np.unique(labels)) > 1 else 0.5\n",
    "    acc    = ((probs > 0.5).astype(int) == labels).mean()\n",
    "    return {'auc': auc, 'acc': acc, 'loss': total_loss / max(n_batches, 1),\n",
    "            'labels': labels, 'probs': probs}\n",
    "\n",
    "print(\"âœ… train_epoch and evaluate ready\")\n",
    "print(f\"   Steps per epoch: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c904ecf1-1445-4df7-aebf-eaccea3e3c86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:36:14.418802Z",
     "iopub.status.busy": "2026-02-19T03:36:14.418308Z",
     "iopub.status.idle": "2026-02-19T03:53:23.847255Z",
     "shell.execute_reply": "2026-02-19T03:53:23.846586Z",
     "shell.execute_reply.started": "2026-02-19T03:36:14.418776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_auc':[], 'lr':[]}\n",
    "best_val_auc, best_epoch = 0.0, 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"=\" * 68)\n",
    "print(f\"{'Ep':>3} {'TrLoss':>8} {'TrAcc':>7} {'VaLoss':>8} \"\n",
    "      f\"{'VaAUC':>7} {'VaAcc':>7} {'LR':>9} {'t':>5}\")\n",
    "print(\"=\" * 68)\n",
    "\n",
    "for epoch in range(CFG['epochs']):\n",
    "    t0 = time.time()\n",
    "\n",
    "    tr_loss, tr_acc = train_epoch(model, train_loader)\n",
    "    val_m           = evaluate(model, val_loader)\n",
    "    scheduler.step()\n",
    "    lr = optimizer.param_groups[1]['lr']   # head LR\n",
    "\n",
    "    history['train_loss'].append(tr_loss)\n",
    "    history['train_acc'].append(tr_acc)\n",
    "    history['val_loss'].append(val_m['loss'])\n",
    "    history['val_auc'].append(val_m['auc'])\n",
    "    history['lr'].append(lr)\n",
    "\n",
    "    flag = ' âœ“' if val_m['auc'] > best_val_auc else ''\n",
    "    print(f\"{epoch+1:>3} {tr_loss:>8.4f} {tr_acc:>7.3f} {val_m['loss']:>8.4f} \"\n",
    "          f\"{val_m['auc']:>7.4f} {val_m['acc']:>7.3f} {lr:>9.2e} \"\n",
    "          f\"{time.time()-t0:>4.0f}s{flag}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if val_m['auc'] > best_val_auc:\n",
    "        best_val_auc = val_m['auc']\n",
    "        best_epoch   = epoch + 1\n",
    "        torch.save({'epoch': epoch, 'model_state': model.state_dict(),\n",
    "                    'val_auc': best_val_auc, 'cfg': CFG},\n",
    "                   CKPT_DIR / 'best.pth')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"=\" * 68)\n",
    "print(f\"Best val AUC : {best_val_auc:.4f} at epoch {best_epoch}\")\n",
    "print(f\"Total time   : {total_time/60:.1f} min\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7ff1b-2d91-4132-9513-b68348e07f18",
   "metadata": {},
   "source": [
    "## Section 6 â€” Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a10e2f-95df-4d3e-bc91-d6db98f47e62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:53:23.849211Z",
     "iopub.status.busy": "2026-02-19T03:53:23.848740Z",
     "iopub.status.idle": "2026-02-19T03:53:27.625756Z",
     "shell.execute_reply": "2026-02-19T03:53:27.624939Z",
     "shell.execute_reply.started": "2026-02-19T03:53:23.849187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load(CKPT_DIR / 'best.pth', map_location=DEVICE, weights_only=False)\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "print(f\"Loaded best model â€” epoch {ckpt['epoch']+1}, val AUC={ckpt['val_auc']:.4f}\")\n",
    "\n",
    "ff_m  = evaluate(model, val_loader)\n",
    "cdf_m = evaluate(model, cdf_loader)\n",
    "\n",
    "STEP2 = {'ff_auc': 0.7241, 'cdf_auc': 0.6659}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 58)\n",
    "print(\"STEP 2 vs STEP 3 COMPARISON\")\n",
    "print(\"=\" * 58)\n",
    "print(f\"{'Metric':<32} {'Step 2':>9} {'Step 3':>9} {'Delta':>7}\")\n",
    "print(\"-\" * 58)\n",
    "print(f\"{'FF++ Val AUC':<32} {STEP2['ff_auc']:>9.4f} {ff_m['auc']:>9.4f} \"\n",
    "      f\"{ff_m['auc']-STEP2['ff_auc']:>+7.4f}\")\n",
    "print(f\"{'Celeb-DF AUC (cross-dataset)':<32} {STEP2['cdf_auc']:>9.4f} {cdf_m['auc']:>9.4f} \"\n",
    "      f\"{cdf_m['auc']-STEP2['cdf_auc']:>+7.4f}\")\n",
    "gap2 = STEP2['ff_auc'] - STEP2['cdf_auc']\n",
    "gap3 = ff_m['auc'] - cdf_m['auc']\n",
    "print(f\"{'Generalization gap':<32} {gap2:>9.4f} {gap3:>9.4f} {gap3-gap2:>+7.4f}\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "if cdf_m['auc'] >= 0.75:\n",
    "    verdict = \"ðŸŸ¢ STRONG â€” Ready to build V8.0 temporal module on RunPod\"\n",
    "elif cdf_m['auc'] >= 0.68:\n",
    "    verdict = \"ðŸŸ¡ GOOD â€” Clear improvement. B4 backbone will push further\"\n",
    "else:\n",
    "    verdict = \"ðŸŸ¡ MODERATE â€” Improvement but weaker than expected\"\n",
    "print(f\"\\n{verdict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a37af-9c1f-4c13-a06d-bcc381c00628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:53:27.627172Z",
     "iopub.status.busy": "2026-02-19T03:53:27.626921Z",
     "iopub.status.idle": "2026-02-19T03:53:28.608807Z",
     "shell.execute_reply": "2026-02-19T03:53:28.608080Z",
     "shell.execute_reply.started": "2026-02-19T03:53:27.627151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Step 3: Multi-Method Training â€” Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "x = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(x, history['train_loss'], label='Train', color='#3498db', linewidth=2)\n",
    "axes[0].plot(x, history['val_loss'],   label='Val',   color='#e74c3c', linewidth=2)\n",
    "axes[0].axhline(0.693, color='gray', linestyle=':', alpha=0.6, label='Random (0.693)')\n",
    "axes[0].set_title('Loss'); axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC with Step 2 reference lines\n",
    "axes[1].plot(x, history['val_auc'], color='#2ecc71', linewidth=2.5, label='Step 3 Val AUC')\n",
    "axes[1].axhline(best_val_auc,     color='#2ecc71', linestyle='--', alpha=0.6,\n",
    "                label=f'Step3 best={best_val_auc:.4f}')\n",
    "axes[1].axhline(STEP2['ff_auc'], color='gray',    linestyle='--', alpha=0.5,\n",
    "                label=f'Step2 val={STEP2[\"ff_auc\"]:.4f}')\n",
    "axes[1].axhline(cdf_m['auc'],   color='#e74c3c', linestyle='--', alpha=0.8,\n",
    "                label=f'CDF AUC={cdf_m[\"auc\"]:.4f}')\n",
    "axes[1].axhline(STEP2['cdf_auc'], color='#e74c3c', linestyle=':', alpha=0.5,\n",
    "                label=f'Step2 CDF={STEP2[\"cdf_auc\"]:.4f}')\n",
    "axes[1].set_title('Val AUC'); axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylim(0.40, 1.0); axes[1].legend(fontsize=8); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# ROC curves\n",
    "for color, m, label in [\n",
    "    ('#3498db', ff_m,  f\"FF++ Val  (AUC={ff_m['auc']:.4f})\"),\n",
    "    ('#e74c3c', cdf_m, f\"Celeb-DF  (AUC={cdf_m['auc']:.4f})\"),\n",
    "]:\n",
    "    fpr, tpr, _ = roc_curve(m['labels'], m['probs'])\n",
    "    axes[2].plot(fpr, tpr, color=color, linewidth=2, label=label)\n",
    "axes[2].plot([0,1],[0,1],'k--', alpha=0.4, label='Random')\n",
    "axes[2].set_title('ROC Curves'); axes[2].set_xlabel('FPR'); axes[2].set_ylabel('TPR')\n",
    "axes[2].legend(fontsize=9); axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'step3_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… step3_results.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15c582-8d50-4083-9351-247be17d08fb",
   "metadata": {},
   "source": [
    "## Section 7 â€” Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf8a1d3-f868-41d8-82fe-ba42a374d675",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:53:28.610411Z",
     "iopub.status.busy": "2026-02-19T03:53:28.610181Z",
     "iopub.status.idle": "2026-02-19T03:53:28.618078Z",
     "shell.execute_reply": "2026-02-19T03:53:28.617527Z",
     "shell.execute_reply.started": "2026-02-19T03:53:28.610390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    'model':               'EfficientNet-B0 ImageNet pretrained',\n",
    "    'train_methods':       TRAIN_METHODS,\n",
    "    'n_train_videos':      len(TRAIN_DATA),\n",
    "    'best_epoch':          best_epoch,\n",
    "    'ff_val':              {'auc': round(ff_m['auc'],  4), 'acc': round(ff_m['acc'],  4)},\n",
    "    'celeb_df':            {'auc': round(cdf_m['auc'], 4), 'acc': round(cdf_m['acc'], 4)},\n",
    "    'gap':                 round(ff_m['auc'] - cdf_m['auc'], 4),\n",
    "    'step2_cdf_auc':       STEP2['cdf_auc'],\n",
    "    'improvement':         round(cdf_m['auc'] - STEP2['cdf_auc'], 4),\n",
    "    'training_minutes':    round(total_time / 60, 1),\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'step3_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"=\" * 58)\n",
    "print(\"STEP 3 COMPLETE â€” V8.0 ROADMAP\")\n",
    "print(\"=\" * 58)\n",
    "print(f\"  Step 2 (1 method, 300 vids) : CDF AUC = {STEP2['cdf_auc']:.4f}\")\n",
    "print(f\"  Step 3 (4 methods, 600 vids): CDF AUC = {cdf_m['auc']:.4f}\")\n",
    "print(f\"  Improvement                 : {results['improvement']:+.4f}\")\n",
    "print()\n",
    "print(\"Remaining improvements for V8.0 on RunPod A100:\")\n",
    "print(\"  [ ] EfficientNet-B4 backbone   (+3-5% expected)\")\n",
    "print(\"  [ ] Full FF++ training set     (+2-4% expected)\")\n",
    "print(\"  [ ] Temporal Mamba module      (+5-10% expected, main contribution)\")\n",
    "print(f\"\\n  Current floor : {cdf_m['auc']:.4f}\")\n",
    "print(f\"  V8.0 target   : 0.90+\")\n",
    "print(f\"  SOTA          : 0.9629 (WMamba)\")\n",
    "print(f\"\\nâœ… Results â†’ {OUTPUT_DIR / 'step3_results.json'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 5454468,
     "datasetId": 3120670,
     "sourceId": 5380830,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10408999,
     "datasetId": 6248577,
     "sourceId": 10125851,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
